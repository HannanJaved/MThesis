{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKUtR8NboAGh"
      },
      "source": [
        "# Fairness in GCNs\n",
        "In this python notebook, we have explored the use of Graph Convolutional Networks(GCNs) for the [Alibaba](https://tianchi.aliyun.com/dataset/56) dataset.\n",
        "\n",
        "The dataset was pre-processed using the code from Erasmo Purificato's [CatGCN notebook](https://colab.research.google.com/drive/1zsx4an6BKYhJ_UT-mSl1_qPB-zyjTmrA#scrollTo=xxzSlLj3LDIu).  \n",
        "This pre-processing provided us with various .csv files, which are used to form graph data.  \n",
        "\n",
        "The nodes represent the user ids, with the node features being attributes such as buy, gender, student, etc. The edges between the nodes have been created through various relations between the users such as items bought, items clicked on, etc.  \n",
        "\n",
        "In this notebook, we have only focused on GCNs and used fairness methods from the [AIF360](https://github.com/Trusted-AI/AIF360) framework."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_AV3JhlZf00"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nlx4iq4S0auM"
      },
      "source": [
        "### Import necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHSmhafSZMnf",
        "outputId": "7652507e-e752-47c8-82d4-2140018a3988"
      },
      "outputs": [],
      "source": [
        "# Basic data processing libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "\n",
        "# Graph data processing libraries\n",
        "import networkx as nx\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.utils import from_networkx\n",
        "\n",
        "# Libraries for (G)NNs\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oTGYnLz0nqq"
      },
      "source": [
        "### Initialise paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-9cqZLujZzno"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('c:\\\\Users\\\\Hanna\\\\iCloudDrive\\\\Desktop\\\\Uni\\\\Master Thesis\\\\Trials',\n",
              " 'c:\\\\Users\\\\Hanna\\\\iCloudDrive\\\\Desktop\\\\Uni\\\\Master Thesis\\\\Trials\\\\input_ali_data')"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "base_path = os.getcwd()\n",
        "input_ali_data_path = os.path.join(base_path, \"input_ali_data\")\n",
        "\n",
        "base_path, input_ali_data_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-cz1bZUbyqg"
      },
      "source": [
        "---\n",
        "# Data Stuff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71oC_W9UZiND"
      },
      "source": [
        "### Load the data from .csv files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZkX7IkpVbMC-"
      },
      "outputs": [],
      "source": [
        "# Load the data files\n",
        "user_labels_path = os.path.join(input_ali_data_path, \"user_labels.csv\")\n",
        "user_edges_path = os.path.join(input_ali_data_path, \"user_edge.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ejJgu5rdw89n"
      },
      "outputs": [],
      "source": [
        "# Create dataframes to store the information from the .csv files\n",
        "user_labels = pd.read_csv(user_labels_path)\n",
        "user_edges = pd.read_csv(user_edges_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVZzUZlfbSui"
      },
      "source": [
        "### Pre-processing the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jS3qr14B2t5d"
      },
      "outputs": [],
      "source": [
        "# Prepare the data for GNNs\n",
        "node_features = torch.tensor(user_labels.iloc[:, 1:].values, dtype=torch.float)\n",
        "edge_index = torch.tensor(user_edges.values, dtype=torch.long).t().contiguous()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KbmyUI5QcyKx"
      },
      "outputs": [],
      "source": [
        "def show_df_info(df):\n",
        "    print(df.info())\n",
        "    print('####### Repeat ####### \\n', df.duplicated().any())\n",
        "    print('####### Count ####### \\n', df.nunique())\n",
        "    print('####### Example ####### \\n',df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDtgaVODdFit",
        "outputId": "329b730d-7780-4287-a0da-45fe6b1def0e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 1., 1.,  ..., 0., 1., 1.],\n",
              "        [0., 2., 1.,  ..., 0., 1., 1.],\n",
              "        ...,\n",
              "        [0., 0., 1.,  ..., 2., 0., 1.],\n",
              "        [0., 4., 1.,  ..., 3., 0., 1.],\n",
              "        [0., 0., 1.,  ..., 0., 0., 1.]])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "node_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThCC75tjcN3j",
        "outputId": "8008e917-9156-42d7-a415-95de9cadf893"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 166958 entries, 0 to 166957\n",
            "Data columns (total 8 columns):\n",
            " #   Column   Non-Null Count   Dtype\n",
            "---  ------   --------------   -----\n",
            " 0   uid      166958 non-null  int64\n",
            " 1   gender   166958 non-null  int64\n",
            " 2   age      166958 non-null  int64\n",
            " 3   buy      166958 non-null  int64\n",
            " 4   student  166958 non-null  int64\n",
            " 5   city     166958 non-null  int64\n",
            " 6   bin_age  166958 non-null  int64\n",
            " 7   bin_buy  166958 non-null  int64\n",
            "dtypes: int64(8)\n",
            "memory usage: 10.2 MB\n",
            "None\n",
            "####### Repeat ####### \n",
            " False\n",
            "####### Count ####### \n",
            " uid        166958\n",
            "gender          2\n",
            "age             7\n",
            "buy             3\n",
            "student         2\n",
            "city            4\n",
            "bin_age         2\n",
            "bin_buy         2\n",
            "dtype: int64\n",
            "####### Example ####### \n",
            "    uid  gender  age  buy  student  city  bin_age  bin_buy\n",
            "0    0       0    0    0        0     0        0        0\n",
            "1    1       0    1    1        1     0        1        1\n",
            "2    2       0    2    1        1     0        1        1\n",
            "3    3       0    2    1        1     1        1        1\n",
            "4    4       0    0    1        0     0        0        1\n"
          ]
        }
      ],
      "source": [
        "show_df_info(user_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mO8xfud6xbvG"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Data(x=[166958, 7], edge_index=[2, 29061406])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create torch-geometric data\n",
        "data = Data(x=node_features, edge_index=edge_index)\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "iTSWiW3lzDPc"
      },
      "outputs": [],
      "source": [
        "num_nodes = node_features.size(0)\n",
        "num_classes = 2 # Binarised gender values from the data\n",
        "num_node_features = data.num_node_features\n",
        "\n",
        "# Create masks for training, and testing\n",
        "train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "\n",
        "# 80 - 20 Train and Test data split\n",
        "num_train = int(num_nodes * 0.8)\n",
        "train_mask[:num_train] = True\n",
        "test_mask[num_train:] = True\n",
        "\n",
        "data.train_mask = train_mask\n",
        "data.test_mask = test_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_emUdZ-WdB8S",
        "outputId": "b667972f-1402-4d03-9ec4-f85633568ec4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "166958"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "IBliHF665WBC"
      },
      "outputs": [],
      "source": [
        "# Labels from the data (in this case: Gender Classification)\n",
        "data.y = torch.tensor(user_labels['gender'].values, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYc3PvX715v3"
      },
      "source": [
        "---\n",
        "# Utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIXj8cPkMa_g"
      },
      "source": [
        "### Function to clone the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "ZtwOmxUSWU53"
      },
      "outputs": [],
      "source": [
        "def clone(data):\n",
        "    \"\"\"\n",
        "    Create a new cloned torch-geometric data object.\n",
        "\n",
        "    Args:\n",
        "    data: Actual data to be cloned.\n",
        "\n",
        "    Returns:\n",
        "    A torch-geometric data object.\n",
        "    \"\"\"\n",
        "    clone_data = Data()\n",
        "\n",
        "    # Copy the data's features and edges\n",
        "    clone_data.x = data.x.clone()\n",
        "    clone_data.edge_index = data.edge_index.clone()\n",
        "\n",
        "    # Mask the data similar to the original train-test split\n",
        "    clone_data.train_mask = data.train_mask.clone()\n",
        "    clone_data.test_mask = data.test_mask.clone()\n",
        "\n",
        "    # Copy the labels\n",
        "    clone_data.y = data.y.clone()\n",
        "\n",
        "    return clone_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98ygJtxSMfLr"
      },
      "source": [
        "### Custom Loss Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ANsMDbvnV0tY"
      },
      "outputs": [],
      "source": [
        "def weighted_cross_entropy(output, data):\n",
        "    \"\"\"\n",
        "    A custom loss function to calculate a weighted-cross entropy loss.\n",
        "\n",
        "    Args:\n",
        "    output: Outputs from the model.\n",
        "    data: The torch-geometric data object used for the model.\n",
        "\n",
        "    Returns:\n",
        "    A weighted cross-entropy loss.\n",
        "    \"\"\"\n",
        "    target = data.y[data.train_mask]\n",
        "    weights = data.instance_weights[data.train_mask]\n",
        "\n",
        "    loss = F.cross_entropy(output, target, reduction='none')\n",
        "    weighted_loss = loss * weights\n",
        "\n",
        "    return weighted_loss.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def dual_objective_loss_with_lagrangian_relaxation(output, data, sensitive_attrs, lambda_param, lambda_entropy_coeff):\n",
        "    \"\"\"\n",
        "    Compute the dual-objective loss with Lagrangian relaxation and entropy regularization.\n",
        "    \n",
        "    :param outputs: Model predictions.\n",
        "    :param labels: Ground truth labels.\n",
        "    :param sensitive_attrs: Sensitive attributes for each instance.\n",
        "    :param lambda_param: Lagrange multiplier for balancing accuracy and fairness.\n",
        "    :param lambda_entropy_coeff: Coefficient for the entropy regularization.\n",
        "    :return: Total loss, prediction loss, fairness loss, entropy regularization term.\n",
        "    \"\"\"\n",
        "    # Prediction loss (e.g., Cross-Entropy for classification)\n",
        "    target = data.y[data.train_mask]\n",
        "    pred_loss = F.cross_entropy(output, target)\n",
        "\n",
        "    labels = data.y[train_mask]\n",
        "    pos_prob = torch.sigmoid(output[:, 1])\n",
        "\n",
        "    # Fairness loss (e.g., absolute difference in positive prediction rates between groups)\n",
        "    # Statistical Parity Regularization\n",
        "    sp_reg = torch.abs(pos_prob[sensitive_attrs == 1].mean() - pos_prob[sensitive_attrs == 0].mean())\n",
        "\n",
        "    fairness_loss = torch.abs(sp_reg)\n",
        "    \n",
        "    # Entropy regularization for the Lagrange multiplier\n",
        "    entropy_reg = -lambda_entropy_coeff * (lambda_param * torch.log(lambda_param))\n",
        "\n",
        "    # Total loss\n",
        "    total_loss = pred_loss + lambda_param * fairness_loss + entropy_reg\n",
        "\n",
        "    return total_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "tcBBcsJo8CMs"
      },
      "outputs": [],
      "source": [
        "def fairness_aware_loss(output, data, sensitive_attr, weighted=False, alpha=0.01, beta=0.01, gamma=0.01, delta=0.01):\n",
        "    \"\"\"\n",
        "    Custom loss function to calculate a fairness-aware loss.\n",
        "    This includes measures for statistical parity, treatment equality, equal opportunity difference, and overall accuracy equality difference.\n",
        "\n",
        "    Args:\n",
        "    output: Outputs from the model.\n",
        "    data: The torch-geometric data object used for the model.\n",
        "    sensitive_attr: The sensitive attribute in the data (e.g., bin_age).\n",
        "    weighted: Boolean value indicating re-weighing done to the data or not.\n",
        "    alpha: Parameter for statistical parity regularizer strength.\n",
        "    beta: Parameter for treatment equality regularizer strength.\n",
        "    gamma: Parameter for equal opportunity difference regularizer strength.\n",
        "    delta: Parameter for overall accuracy equality difference regularizer strength.\n",
        "\n",
        "    Returns:\n",
        "    A fairness-aware combined loss.\n",
        "    \"\"\"\n",
        "    if weighted:\n",
        "        # Weighted cross-entropy loss\n",
        "        standard_loss = weighted_cross_entropy(output, data)\n",
        "    else:\n",
        "        # Standard cross-entropy loss\n",
        "        target = data.y[data.train_mask]\n",
        "        standard_loss = F.cross_entropy(output, target)\n",
        "\n",
        "    labels = data.y[train_mask]\n",
        "    pos_prob = torch.sigmoid(output[:, 1])\n",
        "    neg_prob = 1 - pos_prob\n",
        "    predictions = output.argmax(dim=1)\n",
        "\n",
        "    # Statistical Parity Regularization\n",
        "    sp_reg = torch.abs(pos_prob[sensitive_attr == 1].mean() - pos_prob[sensitive_attr == 0].mean())\n",
        "\n",
        "    # Treatment Equality Regularization\n",
        "    fp_diff = (neg_prob * (labels == 0) * (sensitive_attr == 1)).float().mean() - \\\n",
        "              (neg_prob * (labels == 0) * (sensitive_attr == 0)).float().mean()\n",
        "    fn_diff = (pos_prob * (labels == 1) * (sensitive_attr == 1)).float().mean() - \\\n",
        "              (pos_prob * (labels == 1) * (sensitive_attr == 0)).float().mean()\n",
        "    treatment_reg = torch.abs(fp_diff) + torch.abs(fn_diff)\n",
        "\n",
        "    # Equal Opportunity Difference Regularization\n",
        "    eod_reg = torch.abs((pos_prob * (labels == 1) * (sensitive_attr == 1)).float().mean() - \\\n",
        "                        (pos_prob * (labels == 1) * (sensitive_attr == 0)).float().mean())\n",
        "\n",
        "    # Overall Accuracy Equality Difference Regularization\n",
        "    oaed_reg = torch.abs((pos_prob * (sensitive_attr == 1)).float().mean() - \\\n",
        "                         (pos_prob * (sensitive_attr == 0)).float().mean())\n",
        "\n",
        "    # Combine losses\n",
        "    combined_loss = standard_loss + alpha * sp_reg + beta * treatment_reg + gamma * eod_reg + delta * oaed_reg\n",
        "\n",
        "    return combined_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fairness_aware_loss(output, data, sensitive_attr, weighted=False, alpha=0.01, beta=0.02):\n",
        "    \"\"\"\n",
        "    Custom loss function to calculate a fairness-aware loss.\n",
        "    This includes measures for statistical parity, treatment equality, equal opportunity difference, and overall accuracy equality difference.\n",
        "\n",
        "    Args:\n",
        "    output: Outputs from the model.\n",
        "    data: The torch-geometric data object used for the model.\n",
        "    sensitive_attr: The sensitive attribute in the data (e.g., bin_age).\n",
        "    weighted: Boolean value indicating re-weighing done to the data or not.\n",
        "    alpha: Parameter for statistical parity regularizer strength.\n",
        "    beta: Parameter for treatment equality regularizer strength.\n",
        "    gamma: Parameter for equal opportunity difference regularizer strength.\n",
        "    delta: Parameter for overall accuracy equality difference regularizer strength.\n",
        "\n",
        "    Returns:\n",
        "    A fairness-aware combined loss.\n",
        "    \"\"\"\n",
        "    if weighted:\n",
        "        # Weighted cross-entropy loss\n",
        "        standard_loss = weighted_cross_entropy(output, data)\n",
        "    else:\n",
        "        # Standard cross-entropy loss\n",
        "        target = data.y[data.train_mask]\n",
        "        standard_loss = F.cross_entropy(output, target)\n",
        "\n",
        "    labels = data.y[train_mask]\n",
        "    pos_prob = torch.sigmoid(output[:, 1])\n",
        "    neg_prob = 1 - pos_prob\n",
        "    predictions = output.argmax(dim=1)\n",
        "\n",
        "    # Statistical Parity Regularization\n",
        "    sp_reg = torch.abs(pos_prob[sensitive_attr == 1].mean() - pos_prob[sensitive_attr == 0].mean())\n",
        "\n",
        "    # Calculating FPR and TPR for each group\n",
        "    fpr_group1 = ((predictions == 1) & (labels == 0) & (sensitive_attr == 1)).float().mean()\n",
        "    fpr_group0 = ((predictions == 1) & (labels == 0) & (sensitive_attr == 0)).float().mean()\n",
        "    tpr_group1 = ((predictions == 1) & (labels == 1) & (sensitive_attr == 1)).float().mean()\n",
        "    tpr_group0 = ((predictions == 1) & (labels == 1) & (sensitive_attr == 0)).float().mean()\n",
        "\n",
        "    # Difference in FPR and TPR between the two groups for Equalized Odds\n",
        "    fpr_diff = torch.abs(fpr_group1 - fpr_group0)\n",
        "    tpr_diff = torch.abs(tpr_group1 - tpr_group0)\n",
        "\n",
        "    # Combine FPR and TPR differences for Equalized Odds Regularization\n",
        "    equalized_odds_reg = fpr_diff + tpr_diff\n",
        "\n",
        "    # Treatment Equality Regularization\n",
        "    fp_diff = (neg_prob * (labels == 0) * (sensitive_attr == 1)).float().mean() - \\\n",
        "              (neg_prob * (labels == 0) * (sensitive_attr == 0)).float().mean()\n",
        "    fn_diff = (pos_prob * (labels == 1) * (sensitive_attr == 1)).float().mean() - \\\n",
        "              (pos_prob * (labels == 1) * (sensitive_attr == 0)).float().mean()\n",
        "    # treatment_reg = torch.abs(fp_diff) + torch.abs(fn_diff)\n",
        "    treatment_reg = torch.abs(fn_diff)\n",
        "\n",
        "    # Equal Opportunity Difference Regularization\n",
        "    eod_reg = torch.abs((pos_prob * (labels == 1) * (sensitive_attr == 1)).float().mean() - \\\n",
        "                        (pos_prob * (labels == 1) * (sensitive_attr == 0)).float().mean())\n",
        "\n",
        "    # Overall Accuracy Equality Difference Regularization\n",
        "    oaed_reg = torch.abs((pos_prob * (sensitive_attr == 1)).float().mean() - \\\n",
        "                         (pos_prob * (sensitive_attr == 0)).float().mean())\n",
        "\n",
        "    penalty = alpha + beta\n",
        "    \n",
        "    # Combine losses\n",
        "    combined_loss = (1-penalty)*standard_loss + alpha * equalized_odds_reg + beta * treatment_reg \n",
        "    return combined_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJQ7eKEtMj8y"
      },
      "source": [
        "### Fairness Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_fairness(label, predictions, sens_attr='bin_age'):\n",
        "    \"\"\"\n",
        "    Calculate various fairness metrics.\n",
        "\n",
        "    Args:\n",
        "    label: Actual labels (binary).\n",
        "    predictions: Model predictions (binary).\n",
        "    sens_attr: Binary sensitive attribute for fairness evaluation.\n",
        "\n",
        "    Returns:\n",
        "    A dictionary containing SPD, EOD, OAED, and TED values.\n",
        "    \"\"\"\n",
        "\n",
        "    labels = torch.tensor(user_labels[label].values, dtype=torch.long)\n",
        "    sensitive_attribute = torch.tensor(user_labels[sens_attr].values, dtype=torch.long)\n",
        "\n",
        "    predictions = predictions.float()\n",
        "    labels = labels.float()\n",
        "    sensitive_attribute = sensitive_attribute.float()\n",
        "\n",
        "    def statistical_parity_difference():\n",
        "        prob_group_1 = predictions[sensitive_attribute == 1].mean()\n",
        "        prob_group_0 = predictions[sensitive_attribute == 0].mean()\n",
        "        return abs(prob_group_1 - prob_group_0), prob_group_0, prob_group_1\n",
        "\n",
        "    def equal_opportunity_difference():\n",
        "        tpr_group_1 = predictions[(labels == 1) & (sensitive_attribute == 1)].mean()\n",
        "        tpr_group_0 = predictions[(labels == 1) & (sensitive_attribute == 0)].mean()\n",
        "        return abs(tpr_group_1 - tpr_group_0)\n",
        "\n",
        "    def overall_accuracy_equality_difference():\n",
        "        acc_group_1 = (predictions[sensitive_attribute == 1] == labels[sensitive_attribute == 1]).float().mean()\n",
        "        acc_group_0 = (predictions[sensitive_attribute == 0] == labels[sensitive_attribute == 0]).float().mean()\n",
        "        return abs(acc_group_1 - acc_group_0)\n",
        "\n",
        "    def treatment_equality_difference():\n",
        "        fn_group_1 = ((predictions == 0) & (labels == 1) & (sensitive_attribute == 1)).sum()\n",
        "        fp_group_1 = ((predictions == 1) & (labels == 0) & (sensitive_attribute == 1)).sum()\n",
        "\n",
        "        fn_group_0 = ((predictions == 0) & (labels == 1) & (sensitive_attribute == 0)).sum()\n",
        "        fp_group_0 = ((predictions == 1) & (labels == 0) & (sensitive_attribute == 0)).sum()\n",
        "\n",
        "        ratio_group_1 = fn_group_1 / fp_group_1 if fp_group_1 != 0 else float('inf')\n",
        "        ratio_group_0 = fn_group_0 / fp_group_0 if fp_group_0 != 0 else float('inf')\n",
        "\n",
        "        return abs(ratio_group_1 - ratio_group_0)\n",
        "\n",
        "    # Calculating each fairness metric\n",
        "    spd, sp_g0, sp_g1 = statistical_parity_difference()\n",
        "    eod = equal_opportunity_difference()\n",
        "    oaed = overall_accuracy_equality_difference()\n",
        "    ted = treatment_equality_difference()\n",
        "\n",
        "    return {\n",
        "        'Statistical Parity Difference': spd,\n",
        "        'Statistical Parity Group with S=0': sp_g0,\n",
        "        'Statistical Parity Group S=1': sp_g1,\n",
        "        'Equal Opportunity Difference': eod,\n",
        "        'Overall Accuracy Equality Difference': oaed,\n",
        "        'Treatment Equality Difference': ted\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1BLAdefMmlB"
      },
      "source": [
        "### Train and Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "bOHyBybux08x"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "def training(model, data, optimizer, epochs, weighted=False, fairness=False, alpha=0.01, beta=0.01, gamma=0.01, delta=0.01):\n",
        "    \"\"\"\n",
        "    Helper function to train the GNN model.\n",
        "\n",
        "    Args:\n",
        "    model: Initialized GNN model.\n",
        "    data: The torch_geometric data used to train the model.\n",
        "    optimizer: Optimizer used to train the model.\n",
        "    weighted: Boolean value indicating re-weighing done to the data or not.\n",
        "    fairness: Boolean value indicating whether to use fairness-aware loss or not.\n",
        "    alpha: Parameter for statistical parity regularizer strength.\n",
        "    beta: Parameter for treatment equality regularizer strength.\n",
        "    gamma: Parameter for equal opportunity difference regularizer strength.\n",
        "    delta: Parameter for overall accuracy equality difference regularizer strength.\n",
        "\n",
        "    Returns:\n",
        "    -\n",
        "    \"\"\"\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data)\n",
        "        \n",
        "        if fairness:\n",
        "            # loss = fairness_aware_loss(out[data.train_mask], data, data.x[data.train_mask, -1],\n",
        "                                    #    weighted=weighted, alpha=alpha, beta=beta, gamma=gamma, delta=delta)\n",
        "            loss = fairness_aware_loss(out[data.train_mask], data, data.x[data.train_mask, -1],\n",
        "                                       weighted=weighted, alpha=alpha, beta=beta)\n",
        "        \n",
        "        elif weighted:\n",
        "            loss = weighted_cross_entropy(out[data.train_mask], data)\n",
        "        else:\n",
        "            criterion = torch.nn.CrossEntropyLoss()\n",
        "            loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f'Epoch {epoch} | Loss: {loss.item()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "C-9-0Bbi2Q5l"
      },
      "outputs": [],
      "source": [
        "# Test the model\n",
        "def test(model, data):\n",
        "    \"\"\"\n",
        "    Helper function to test the trained GNN model.\n",
        "    Prints the Accuracy, as well as various fairness metrics values.\n",
        "    For fairness metrics used: Check the calculate_fairness method\n",
        "\n",
        "    Args:\n",
        "    model: Trained GNN model.\n",
        "    data: The torch_geometric data used to train the model.\n",
        "\n",
        "    Returns:\n",
        "    -\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "      out = model(data)\n",
        "\n",
        "    _, pred = model(data).max(dim=1)\n",
        "    correct = int(pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
        "    accuracy = correct / int(data.test_mask.sum())\n",
        "    print(f'Accuracy: {accuracy}')\n",
        "\n",
        "    # Convert model outputs to binary predictions\n",
        "    predictions = out.argmax(dim=1)\n",
        "\n",
        "    # Fairness calculated for gender-classification task with bin_age as the sensitive attribute\n",
        "    fairness_metrics = calculate_fairness(label='gender', predictions=predictions, sens_attr='bin_age')\n",
        "\n",
        "    # Print the fairness metrics\n",
        "    for metric, value in fairness_metrics.items():\n",
        "        print(f\"{metric}: {value}\")\n",
        "\n",
        "    return accuracy, fairness_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJJv17nM1xsH"
      },
      "source": [
        "---\n",
        "# GCN Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "xcv3FZoGxoQn"
      },
      "outputs": [],
      "source": [
        "# GCN class that takes in the data as an input for dimensions of the convolutions\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, data):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GCNConv(data.num_node_features, 16)\n",
        "        self.conv2 = GCNConv(16, 2) # 2 output classes for gender\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8DljQFY98sL"
      },
      "source": [
        "### Base Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "from set_device import set_device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "nzu1iGeBxume"
      },
      "outputs": [],
      "source": [
        "# Instantiate the model, define loss function and optimizer\n",
        "gcn_model = GCN(data)\n",
        "gcn_optimizer = torch.optim.Adam(gcn_model.parameters(), lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGXTHZpMdeiV",
        "outputId": "0c7eb32a-1958-45e8-e34e-ff68906412cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 | Loss: 0.6290466785430908\n",
            "Epoch 10 | Loss: 0.5474098324775696\n",
            "Epoch 20 | Loss: 0.48671963810920715\n",
            "Epoch 30 | Loss: 0.4376523792743683\n",
            "Epoch 40 | Loss: 0.403105765581131\n"
          ]
        }
      ],
      "source": [
        "# Train the first model: GCN, standard data, cross-entropy loss\n",
        "training(model=gcn_model, data=data, optimizer=gcn_optimizer, epochs=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "As_kuy6TdoNc",
        "outputId": "9508dea6-41f4-4716-a0a9-73f7f5240902"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here are the values for the GCN model with the standard dataset and cross-entropy loss: \n",
            "\n",
            "Accuracy: 0.8549053665548635\n",
            "Statistical Parity Difference: 0.04585549235343933\n",
            "Equal Opportunity Difference: 0.08249932527542114\n",
            "Overall Accuracy Equality Difference: 0.0841907262802124\n",
            "Treatment Equality Difference: 0.5477824211120605\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test the first model: GCN, standard data, cross-entropy loss\n",
        "print(\"Here are the values for the GCN model with the standard dataset and cross-entropy loss: \")\n",
        "print()\n",
        "test(gcn_model, data)\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instantiate the model, define loss function and optimizer\n",
        "gcn_model = GCN(data)\n",
        "gcn_optimizer = torch.optim.Adam(gcn_model.parameters(), lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x15f692310>"
            ]
          },
          "execution_count": 140,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.autograd.set_detect_anomaly(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### FAMP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch_geometric.nn import MessagePassing\n",
        "from torch_geometric.utils import add_self_loops, degree\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FairnessAwareMessagePassingLayer(MessagePassing):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(FairnessAwareMessagePassingLayer, self).__init__(aggr='mean')  \n",
        "        self.lin = nn.Linear(in_channels, out_channels)\n",
        "        self.a_fair = nn.Parameter(torch.rand(out_channels)) \n",
        "        self.sensitive_attr = torch.tensor(user_labels['bin_age'].values, dtype=torch.float) \n",
        "        self.bias_correction = nn.Parameter(torch.rand(1))\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        # Add self-loops \n",
        "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
        "\n",
        "        x = self.lin(x)\n",
        "\n",
        "        return self.propagate(edge_index, size=(x.size(0), x.size(0)), x=x)\n",
        "    \n",
        "    def message(self, x_j, edge_index, size):\n",
        "        row, col = edge_index\n",
        "        deg = degree(row, size[0], dtype=x_j.dtype)\n",
        "        deg_inv_sqrt = deg.pow(-0.5)\n",
        "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
        "\n",
        "        # Compute statistical parity difference for each edge\n",
        "        group_difference = self.sensitive_attr[row] - self.sensitive_attr[col]\n",
        "        \n",
        "        # Adjust messages based on statistical parity\n",
        "        fairness_adjustment = (1 + self.bias_correction * group_difference.view(-1, 1))\n",
        "\n",
        "        return fairness_adjustment * norm.view(-1, 1) * x_j\n",
        "\n",
        "    def update(self, aggr_out):\n",
        "        return aggr_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GCN class that takes in the data as an input for dimensions of the convolutions\n",
        "class custom_GCN(torch.nn.Module):\n",
        "    def __init__(self, data):\n",
        "        super(custom_GCN, self).__init__()\n",
        "        self.conv1 = FairnessAwareMessagePassingLayer(data.num_node_features, 16)\n",
        "        self.conv2 = FairnessAwareMessagePassingLayer(16, 2) # 2 output classes for gender\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 | Loss: 0.6280730962753296\n",
            "Epoch 10 | Loss: 0.5667222142219543\n",
            "Epoch 20 | Loss: 0.5123195052146912\n",
            "Epoch 30 | Loss: 0.4567728638648987\n",
            "Epoch 40 | Loss: 0.4058452844619751\n",
            "\n",
            "Here are the values for the GCN model with the standard dataset and cross-entropy loss: \n",
            "Accuracy: 0.8528988979396263\n",
            "Statistical Parity Difference: 0.04322892427444458\n",
            "Statistical Parity Group with S=0: 0.131281778216362\n",
            "Statistical Parity Group S=1: 0.17451070249080658\n",
            "Equal Opportunity Difference: 0.08256551623344421\n",
            "Overall Accuracy Equality Difference: 0.08447939157485962\n",
            "Treatment Equality Difference: 0.6746697425842285\n",
            "Treatment Equality Group with S=0: 6.108710289001465\n",
            "Treatment Equality Group S=1: 5.434040546417236\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.8528988979396263,\n",
              " {'Statistical Parity Difference': tensor(0.0432),\n",
              "  'Statistical Parity Group with S=0': tensor(0.1313),\n",
              "  'Statistical Parity Group S=1': tensor(0.1745),\n",
              "  'Equal Opportunity Difference': tensor(0.0826),\n",
              "  'Overall Accuracy Equality Difference': tensor(0.0845),\n",
              "  'Treatment Equality Difference': tensor(0.6747),\n",
              "  'Treatment Equality Group with S=0': tensor(6.1087),\n",
              "  'Treatment Equality Group S=1': tensor(5.4340)})"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Instantiate the model, define loss function and optimizer\n",
        "gcn_model = GCN(data)\n",
        "gcn_optimizer = torch.optim.Adam(gcn_model.parameters(), lr=0.01)\n",
        "\n",
        "training(model=gcn_model, data=data, optimizer=gcn_optimizer, epochs=50)\n",
        "\n",
        "# Test the model\n",
        "print(\"\\nHere are the values for the GCN model with the standard dataset and cross-entropy loss: \")\n",
        "test(gcn_model, data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 | Loss: 0.6909812092781067\n",
            "Epoch 10 | Loss: 0.688873291015625\n",
            "Epoch 20 | Loss: 0.685428261756897\n",
            "Epoch 30 | Loss: 0.6810439825057983\n",
            "Epoch 40 | Loss: 0.6754062175750732\n",
            "\n",
            "Here are the values for the GCN model with the standard dataset and cross-entropy loss: \n",
            "Accuracy: 0.7761140392908481\n",
            "Statistical Parity Difference: 0.011807771399617195\n",
            "Statistical Parity Group with S=0: 0.02442988194525242\n",
            "Statistical Parity Group S=1: 0.036237653344869614\n",
            "Equal Opportunity Difference: 0.00039946287870407104\n",
            "Overall Accuracy Equality Difference: 0.0871589183807373\n",
            "Treatment Equality Difference: 470.03131103515625\n",
            "Treatment Equality Group with S=0: 935.5789184570312\n",
            "Treatment Equality Group S=1: 465.547607421875\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.7761140392908481,\n",
              " {'Statistical Parity Difference': tensor(0.0118),\n",
              "  'Statistical Parity Group with S=0': tensor(0.0244),\n",
              "  'Statistical Parity Group S=1': tensor(0.0362),\n",
              "  'Equal Opportunity Difference': tensor(0.0004),\n",
              "  'Overall Accuracy Equality Difference': tensor(0.0872),\n",
              "  'Treatment Equality Difference': tensor(470.0313),\n",
              "  'Treatment Equality Group with S=0': tensor(935.5789),\n",
              "  'Treatment Equality Group S=1': tensor(465.5476)})"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model2 = custom_GCN(data)\n",
        "optimizer2 = torch.optim.Adam(model2.parameters(), lr=0.01)\n",
        "\n",
        "# Train the model: Custom MP GNN, cross-entropy loss\n",
        "training(model=model2, data=data, optimizer=optimizer2, epochs=50)\n",
        "\n",
        "# Test the model\n",
        "print(\"\\nHere are the values for the GCN model with the standard dataset and cross-entropy loss: \")\n",
        "test(model2, data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 | Loss: 0.643775999546051\n",
            "Epoch 10 | Loss: 0.6410472393035889\n",
            "Epoch 20 | Loss: 0.6383004784584045\n",
            "Epoch 30 | Loss: 0.634784996509552\n",
            "Epoch 40 | Loss: 0.6302443742752075\n",
            "\n",
            "Here are the values for the GCN model with the standard dataset and cross-entropy loss: \n",
            "Accuracy: 0.7771322472448491\n",
            "Statistical Parity Difference: 0.00629039853811264\n",
            "Statistical Parity Group with S=0: 0.028298821300268173\n",
            "Statistical Parity Group S=1: 0.034589219838380814\n",
            "Equal Opportunity Difference: 0.021982602775096893\n",
            "Overall Accuracy Equality Difference: 0.09217983484268188\n",
            "Treatment Equality Difference: 66.20721435546875\n",
            "Treatment Equality Group with S=0: 335.3846130371094\n",
            "Treatment Equality Group S=1: 401.5918273925781\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.7771322472448491,\n",
              " {'Statistical Parity Difference': tensor(0.0063),\n",
              "  'Statistical Parity Group with S=0': tensor(0.0283),\n",
              "  'Statistical Parity Group S=1': tensor(0.0346),\n",
              "  'Equal Opportunity Difference': tensor(0.0220),\n",
              "  'Overall Accuracy Equality Difference': tensor(0.0922),\n",
              "  'Treatment Equality Difference': tensor(66.2072),\n",
              "  'Treatment Equality Group with S=0': tensor(335.3846),\n",
              "  'Treatment Equality Group S=1': tensor(401.5918)})"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model2_FairLoss = custom_GCN(data)\n",
        "optimizer2_FairLoss = torch.optim.Adam(model2_FairLoss.parameters(), lr=0.01)\n",
        "\n",
        "# Train the second model: Fair loss\n",
        "training(model=model2_FairLoss, data=data, optimizer=optimizer2_FairLoss, fairness=True, alpha=0.1, epochs=50)\n",
        "\n",
        "# Test the second model\n",
        "print(\"\\nHere are the values for the GCN model with the standard dataset and Fairness loss: \")\n",
        "test(model2_FairLoss, data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 | Loss: 0.5933056473731995\n",
            "Epoch 10 | Loss: 0.5912958979606628\n",
            "Epoch 20 | Loss: 0.588941216468811\n",
            "Epoch 30 | Loss: 0.5858114957809448\n",
            "Epoch 40 | Loss: 0.5818194150924683\n",
            "Epoch 50 | Loss: 0.5769309401512146\n",
            "Epoch 60 | Loss: 0.571225643157959\n",
            "Epoch 70 | Loss: 0.5649163126945496\n",
            "Epoch 80 | Loss: 0.5582668781280518\n",
            "Epoch 90 | Loss: 0.5515241622924805\n",
            "\n",
            "Here are the values for the GCN model with the standard dataset and Fairness loss: \n",
            "Accuracy: 0.8258864398658361\n",
            "Statistical Parity Difference: 0.015432022511959076\n",
            "Statistical Parity Group with S=0: 0.07883617281913757\n",
            "Statistical Parity Group S=1: 0.09426819533109665\n",
            "Equal Opportunity Difference: 0.054860204458236694\n",
            "Overall Accuracy Equality Difference: 0.07815015316009521\n",
            "Treatment Equality Difference: 70.14901733398438\n",
            "Treatment Equality Group with S=0: 30.57176399230957\n",
            "Treatment Equality Group S=1: 100.72077941894531\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.8258864398658361,\n",
              " {'Statistical Parity Difference': tensor(0.0154),\n",
              "  'Statistical Parity Group with S=0': tensor(0.0788),\n",
              "  'Statistical Parity Group S=1': tensor(0.0943),\n",
              "  'Equal Opportunity Difference': tensor(0.0549),\n",
              "  'Overall Accuracy Equality Difference': tensor(0.0782),\n",
              "  'Treatment Equality Difference': tensor(70.1490),\n",
              "  'Treatment Equality Group with S=0': tensor(30.5718),\n",
              "  'Treatment Equality Group S=1': tensor(100.7208)})"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model2_FairLoss = custom_GCN(data)\n",
        "optimizer2_FairLoss = torch.optim.Adam(model2_FairLoss.parameters(), lr=0.01)\n",
        "\n",
        "# Train the second model: Fair loss\n",
        "training(model=model2_FairLoss, data=data, optimizer=optimizer2_FairLoss, fairness=True, alpha=0.2, epochs=100)\n",
        "\n",
        "# Test the second model\n",
        "print(\"\\nHere are the values for the GCN model with the standard dataset and Fairness loss: \")\n",
        "test(model2_FairLoss, data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "'Statistical Parity Difference': tensor(0.0432),\n",
        "\n",
        "\n",
        "  'Statistical Parity Group with S=0': tensor(0.1313),\n",
        "\n",
        "\n",
        "  'Statistical Parity Group S=1': tensor(0.1745),\n",
        "\n",
        "  'Equal Opportunity Difference': tensor(0.0826),\n",
        "\n",
        "  'Overall Accuracy Equality Difference': tensor(0.0845),\n",
        "\n",
        "  'Treatment Equality Difference': tensor(0.6747),\n",
        "\n",
        "  'Treatment Equality Group with S=0': tensor(6.1087),\n",
        "\n",
        "  'Treatment Equality Group S=1': tensor(5.4340)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 | Loss: 0.6824272871017456\n",
            "Epoch 10 | Loss: 0.6796886324882507\n",
            "Epoch 20 | Loss: 0.6766286492347717\n",
            "Epoch 30 | Loss: 0.6725429892539978\n",
            "Epoch 40 | Loss: 0.6672966480255127\n",
            "\n",
            "Here are the values for the GCN model with the standard dataset and Fairness loss: \n",
            "Accuracy: 0.7783600862482032\n",
            "Statistical Parity Difference: 0.009171690791845322\n",
            "Statistical Parity Group with S=0: 0.02826736494898796\n",
            "Statistical Parity Group S=1: 0.03743905574083328\n",
            "Equal Opportunity Difference: 0.013812802731990814\n",
            "Overall Accuracy Equality Difference: 0.08987855911254883\n",
            "Treatment Equality Difference: 156.09625244140625\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.7783600862482032,\n",
              " {'Statistical Parity Difference': tensor(0.0092),\n",
              "  'Statistical Parity Group with S=0': tensor(0.0283),\n",
              "  'Statistical Parity Group S=1': tensor(0.0374),\n",
              "  'Equal Opportunity Difference': tensor(0.0138),\n",
              "  'Overall Accuracy Equality Difference': tensor(0.0899),\n",
              "  'Treatment Equality Difference': tensor(156.0963)})"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model2_FairLoss = custom_GCN(data)\n",
        "optimizer2_FairLoss = torch.optim.Adam(model2_FairLoss.parameters(), lr=0.01)\n",
        "\n",
        "# Train the second model: Fair loss\n",
        "training(model=model2_FairLoss, data=data, optimizer=optimizer2_FairLoss, fairness=True, alpha=0, beta=0.2, epochs=50)\n",
        "\n",
        "# Test the second model\n",
        "print(\"\\nHere are the values for the GCN model with the standard dataset and Fairness loss: \")\n",
        "test(model2_FairLoss, data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 | Loss: 0.6480738520622253\n",
            "Epoch 10 | Loss: 0.6397125124931335\n",
            "Epoch 20 | Loss: 0.6073761582374573\n",
            "Epoch 30 | Loss: 0.6045373678207397\n",
            "Epoch 40 | Loss: 0.6016223430633545\n",
            "\n",
            "Here are the values for the GCN model with the standard dataset and Fairness loss: \n",
            "Accuracy: 0.7732690464781984\n",
            "Statistical Parity Difference: 0.003445802256464958\n",
            "Statistical Parity Group with S=0: 0.02587680146098137\n",
            "Statistical Parity Group S=1: 0.029322603717446327\n",
            "Equal Opportunity Difference: 0.026854470372200012\n",
            "Overall Accuracy Equality Difference: 0.09461230039596558\n",
            "Treatment Equality Difference: 239.166015625\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.7732690464781984,\n",
              " {'Statistical Parity Difference': tensor(0.0034),\n",
              "  'Statistical Parity Group with S=0': tensor(0.0259),\n",
              "  'Statistical Parity Group S=1': tensor(0.0293),\n",
              "  'Equal Opportunity Difference': tensor(0.0269),\n",
              "  'Overall Accuracy Equality Difference': tensor(0.0946),\n",
              "  'Treatment Equality Difference': tensor(239.1660)})"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model2_FairLoss = custom_GCN(data)\n",
        "optimizer2_FairLoss = torch.optim.Adam(model2_FairLoss.parameters(), lr=0.01)\n",
        "\n",
        "# Train the second model: Fair loss\n",
        "training(model=model2_FairLoss, data=data, optimizer=optimizer2_FairLoss, fairness=True, alpha=0.1, beta=0.1, epochs=50)\n",
        "\n",
        "# Test the second model\n",
        "print(\"\\nHere are the values for the GCN model with the standard dataset and Fairness loss: \")\n",
        "test(model2_FairLoss, data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 | Loss: 0.6303269863128662\n",
            "Epoch 10 | Loss: 0.6086326241493225\n",
            "Epoch 20 | Loss: 0.606320321559906\n",
            "Epoch 30 | Loss: 0.6030330061912537\n",
            "Epoch 40 | Loss: 0.5989081263542175\n",
            "\n",
            "Here are the values for the GCN model with the standard dataset and Fairness loss: \n",
            "Accuracy: 0.7688667944417824\n",
            "Statistical Parity Difference: 0.010660281404852867\n",
            "Statistical Parity Group with S=0: 0.017488859593868256\n",
            "Statistical Parity Group S=1: 0.028149140998721123\n",
            "Equal Opportunity Difference: 0.008008614182472229\n",
            "Overall Accuracy Equality Difference: 0.08765697479248047\n",
            "Treatment Equality Difference: 837.611083984375\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.7688667944417824,\n",
              " {'Statistical Parity Difference': tensor(0.0107),\n",
              "  'Statistical Parity Group with S=0': tensor(0.0175),\n",
              "  'Statistical Parity Group S=1': tensor(0.0281),\n",
              "  'Equal Opportunity Difference': tensor(0.0080),\n",
              "  'Overall Accuracy Equality Difference': tensor(0.0877),\n",
              "  'Treatment Equality Difference': tensor(837.6111)})"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model2_FairLoss = custom_GCN(data)\n",
        "optimizer2_FairLoss = torch.optim.Adam(model2_FairLoss.parameters(), lr=0.01)\n",
        "\n",
        "# Train the second model: Fair loss\n",
        "training(model=model2_FairLoss, data=data, optimizer=optimizer2_FairLoss, fairness=True, alpha=0.1, beta=0.1, epochs=50)\n",
        "\n",
        "# Test the second model\n",
        "print(\"\\nHere are the values for the GCN model with the standard dataset and Fairness loss: \")\n",
        "test(model2_FairLoss, data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 | Loss: 0.6472604274749756\n",
            "Epoch 10 | Loss: 0.6342494487762451\n",
            "Epoch 20 | Loss: 0.6073105931282043\n",
            "Epoch 30 | Loss: 0.6047623753547668\n",
            "Epoch 40 | Loss: 0.6016395688056946\n",
            "\n",
            "Here are the values for the GCN model with the standard dataset and Fairness loss: \n",
            "Accuracy: 0.7702443699089603\n",
            "Statistical Parity Difference: 0.002040307968854904\n",
            "Statistical Parity Group with S=0: 0.023161206394433975\n",
            "Statistical Parity Group S=1: 0.02520151436328888\n",
            "Equal Opportunity Difference: 0.026998378336429596\n",
            "Overall Accuracy Equality Difference: 0.09580826759338379\n",
            "Treatment Equality Difference: 430.2586669921875\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.7702443699089603,\n",
              " {'Statistical Parity Difference': tensor(0.0020),\n",
              "  'Statistical Parity Group with S=0': tensor(0.0232),\n",
              "  'Statistical Parity Group S=1': tensor(0.0252),\n",
              "  'Equal Opportunity Difference': tensor(0.0270),\n",
              "  'Overall Accuracy Equality Difference': tensor(0.0958),\n",
              "  'Treatment Equality Difference': tensor(430.2587)})"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model2_FairLoss = custom_GCN(data)\n",
        "optimizer2_FairLoss = torch.optim.Adam(model2_FairLoss.parameters(), lr=0.01)\n",
        "\n",
        "# Train the second model: Fair loss\n",
        "training(model=model2_FairLoss, data=data, optimizer=optimizer2_FairLoss, fairness=True, alpha=0.1, beta=0.1, epochs=50)\n",
        "\n",
        "# Test the second model\n",
        "print(\"\\nHere are the values for the GCN model with the standard dataset and Fairness loss: \")\n",
        "test(model2_FairLoss, data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 | Loss: 0.6112976670265198\n",
            "Epoch 10 | Loss: 0.6072863340377808\n",
            "Epoch 20 | Loss: 0.6045666933059692\n",
            "Epoch 30 | Loss: 0.6013597846031189\n",
            "Epoch 40 | Loss: 0.5975881814956665\n",
            "\n",
            "Here are the values for the GCN model with the standard dataset and Fairness loss: \n",
            "Accuracy: 0.7739578342117872\n",
            "Statistical Parity Difference: 0.005039209499955177\n",
            "Statistical Parity Group with S=0: 0.024702489376068115\n",
            "Statistical Parity Group S=1: 0.029741698876023293\n",
            "Equal Opportunity Difference: 0.02041982114315033\n",
            "Overall Accuracy Equality Difference: 0.0930333137512207\n",
            "Treatment Equality Difference: 765.7023315429688\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.7739578342117872,\n",
              " {'Statistical Parity Difference': tensor(0.0050),\n",
              "  'Statistical Parity Group with S=0': tensor(0.0247),\n",
              "  'Statistical Parity Group S=1': tensor(0.0297),\n",
              "  'Equal Opportunity Difference': tensor(0.0204),\n",
              "  'Overall Accuracy Equality Difference': tensor(0.0930),\n",
              "  'Treatment Equality Difference': tensor(765.7023)})"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model2_FairLoss = custom_GCN(data)\n",
        "optimizer2_FairLoss = torch.optim.Adam(model2_FairLoss.parameters(), lr=0.01)\n",
        "\n",
        "# Train the second model: Fair loss\n",
        "training(model=model2_FairLoss, data=data, optimizer=optimizer2_FairLoss, fairness=True, alpha=0.1, beta=0.1, epochs=50)\n",
        "\n",
        "# Test the second model\n",
        "print(\"\\nHere are the values for the GCN model with the standard dataset and Fairness loss: \")\n",
        "test(model2_FairLoss, data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GCN class that takes in the data as an input for dimensions of the convolutions\n",
        "class custom_GCN2(torch.nn.Module):\n",
        "    def __init__(self, data):\n",
        "        super(custom_GCN2, self).__init__()\n",
        "        self.conv1 = FairnessAwareMessagePassingLayer(data.num_node_features, 16)\n",
        "        self.conv2 = GCNConv(16, 2) \n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 | Loss: 0.6949846744537354\n",
            "Epoch 10 | Loss: 0.6916375756263733\n",
            "Epoch 20 | Loss: 0.6884897947311401\n",
            "Epoch 30 | Loss: 0.6849395036697388\n",
            "Epoch 40 | Loss: 0.6803786158561707\n",
            "Epoch 50 | Loss: 0.6746808886528015\n",
            "Epoch 60 | Loss: 0.6680154204368591\n",
            "Epoch 70 | Loss: 0.6604992151260376\n",
            "Epoch 80 | Loss: 0.6523556709289551\n",
            "Epoch 90 | Loss: 0.6438772082328796\n",
            "\n",
            "Here are the values for the GCN model with the standard dataset and cross-entropy loss: \n",
            "Accuracy: 0.8221130809774796\n",
            "Statistical Parity Difference: 0.03101031482219696\n",
            "Equal Opportunity Difference: 0.013422399759292603\n",
            "Overall Accuracy Equality Difference: 0.07621175050735474\n",
            "Treatment Equality Difference: 158.93136596679688\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.8221130809774796,\n",
              " {'Statistical Parity Difference': tensor(0.0310),\n",
              "  'Equal Opportunity Difference': tensor(0.0134),\n",
              "  'Overall Accuracy Equality Difference': tensor(0.0762),\n",
              "  'Treatment Equality Difference': tensor(158.9314)})"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model2 = custom_GCN(data)\n",
        "optimizer2 = torch.optim.Adam(model2.parameters(), lr=0.01)\n",
        "\n",
        "# Train the second model: GCN, standard data, cross-entropy loss\n",
        "training(model=model2, data=data, optimizer=optimizer2, epochs=100)\n",
        "\n",
        "# Test the second model: GCN, standard data, cross-entropy loss\n",
        "print(\"\\nHere are the values for the GCN model with the standard dataset and cross-entropy loss: \")\n",
        "test(model2, data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 | Loss: 0.6959884762763977\n",
            "Epoch 10 | Loss: 0.6938111782073975\n",
            "Epoch 20 | Loss: 0.6908217668533325\n",
            "Epoch 30 | Loss: 0.687012791633606\n",
            "Epoch 40 | Loss: 0.6820933818817139\n",
            "\n",
            "Here are the values for the GCN model with the standard dataset and cross-entropy loss: \n",
            "Accuracy: 0.7715620507906086\n",
            "Statistical Parity Difference: 0.006483552977442741\n",
            "Equal Opportunity Difference: 0.011388957500457764\n",
            "Overall Accuracy Equality Difference: 0.09158873558044434\n",
            "Treatment Equality Difference: 341.39794921875\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.7715620507906086,\n",
              " {'Statistical Parity Difference': tensor(0.0065),\n",
              "  'Equal Opportunity Difference': tensor(0.0114),\n",
              "  'Overall Accuracy Equality Difference': tensor(0.0916),\n",
              "  'Treatment Equality Difference': tensor(341.3979)})"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model2 = custom_GCN(data)\n",
        "optimizer2 = torch.optim.Adam(model2.parameters(), lr=0.01)\n",
        "\n",
        "# Train the second model: GCN, standard data, cross-entropy loss\n",
        "training(model=model2, data=data, optimizer=optimizer2, epochs=50, fairness=True)\n",
        "\n",
        "# Test the second model: GCN, standard data, cross-entropy loss\n",
        "print(\"\\nHere are the values for the GCN model with the standard dataset and cross-entropy loss: \")\n",
        "test(model2, data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 | Loss: 0.6934686899185181\n",
            "Epoch 10 | Loss: 0.6908966302871704\n",
            "Epoch 20 | Loss: 0.6874423027038574\n",
            "Epoch 30 | Loss: 0.6832926273345947\n",
            "Epoch 40 | Loss: 0.6780106425285339\n",
            "Here are the values for the GCN model with the standard dataset and cross-entropy loss: \n",
            "\n",
            "Accuracy: 0.7773418782942022\n",
            "Statistical Parity Difference: 0.004843294620513916\n",
            "Equal Opportunity Difference: 0.027115054428577423\n",
            "Overall Accuracy Equality Difference: 0.09278804063796997\n",
            "Treatment Equality Difference: 184.3036346435547\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.7773418782942022,\n",
              " {'Statistical Parity Difference': tensor(0.0048),\n",
              "  'Equal Opportunity Difference': tensor(0.0271),\n",
              "  'Overall Accuracy Equality Difference': tensor(0.0928),\n",
              "  'Treatment Equality Difference': tensor(184.3036)})"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model2 = custom_GCN(data)\n",
        "optimizer2 = torch.optim.Adam(model2.parameters(), lr=0.01)\n",
        "\n",
        "# Train the second model: GCN, standard data, cross-entropy loss\n",
        "training(model=model2, data=data, optimizer=optimizer2, epochs=50)\n",
        "\n",
        "# Test the second model: GCN, standard data, cross-entropy loss\n",
        "print(\"\\nHere are the values for the GCN model with the standard dataset and cross-entropy loss: \")\n",
        "test(model2, data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Values from the original model:\n",
        "\n",
        "Accuracy: 0.8549053665548635\n",
        "\n",
        "\n",
        "Statistical Parity Difference: 0.04585549235343933\n",
        "\n",
        "\n",
        "Equal Opportunity Difference: 0.08249932527542114\n",
        "\n",
        "Overall Accuracy Equality Difference: 0.0841907262802124\n",
        "\n",
        "Treatment Equality Difference: 0.5477824211120605"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 | Loss: 0.690571665763855\n",
            "Epoch 10 | Loss: 0.690571665763855\n",
            "Epoch 20 | Loss: 0.690571665763855\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[95], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m optimizer3 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model2\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Train the second model: GCN, standard data, cross-entropy loss\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m training(model\u001b[38;5;241m=\u001b[39mmodel3, data\u001b[38;5;241m=\u001b[39mdata, optimizer\u001b[38;5;241m=\u001b[39moptimizer3, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Test the second model: GCN, standard data, cross-entropy loss\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHere are the values for the GCN model with the standard dataset and cross-entropy loss: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[0;32mIn[88], line 34\u001b[0m, in \u001b[0;36mtraining\u001b[0;34m(model, data, optimizer, epochs, weighted, fairness, alpha, beta, gamma, delta)\u001b[0m\n\u001b[1;32m     31\u001b[0m     criterion \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m     32\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(out[data\u001b[38;5;241m.\u001b[39mtrain_mask], data\u001b[38;5;241m.\u001b[39my[data\u001b[38;5;241m.\u001b[39mtrain_mask])\n\u001b[0;32m---> 34\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     35\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    524\u001b[0m )\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    267\u001b[0m     tensors,\n\u001b[1;32m    268\u001b[0m     grad_tensors_,\n\u001b[1;32m    269\u001b[0m     retain_graph,\n\u001b[1;32m    270\u001b[0m     create_graph,\n\u001b[1;32m    271\u001b[0m     inputs,\n\u001b[1;32m    272\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    273\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    274\u001b[0m )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "model3 = custom_GCN2(data)\n",
        "optimizer3 = torch.optim.Adam(model2.parameters(), lr=0.01)\n",
        "\n",
        "# Train the second model: GCN, standard data, cross-entropy loss\n",
        "training(model=model3, data=data, optimizer=optimizer3, epochs=50)\n",
        "\n",
        "# Test the second model: GCN, standard data, cross-entropy loss\n",
        "print(\"Here are the values for the GCN model with the standard dataset and cross-entropy loss: \")\n",
        "test(model3, data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_statistical_parity_difference(label, predictions, sens_attr='bin_age'):\n",
        "    \"\"\"\n",
        "    Calculate various fairness metrics.\n",
        "\n",
        "    Args:\n",
        "    label: Actual labels (binary).\n",
        "    predictions: Model predictions (binary).\n",
        "    sens_attr: Binary sensitive attribute for fairness evaluation.\n",
        "\n",
        "    Returns:\n",
        "    A dictionary containing SPD, EOD, OAED, and TED values.\n",
        "    \"\"\"\n",
        "    labels = label.detach().clone()\n",
        "    predictions = predictions.detach().clone()\n",
        "    labels = torch.tensor(user_labels[labels].values, dtype=torch.long)\n",
        "    sensitive_attribute = torch.tensor(user_labels[sens_attr].values, dtype=torch.long)\n",
        "\n",
        "    predictions = predictions.float()\n",
        "    labels = labels.float()\n",
        "    sensitive_attribute = sensitive_attribute.float()\n",
        "\n",
        "    prob_group_1 = predictions[sensitive_attribute == 1].mean()\n",
        "    prob_group_0 = predictions[sensitive_attribute == 0].mean()\n",
        "    return abs(prob_group_1 - prob_group_0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def dual_objective_loss_with_lagrangian_relaxation(outputs, labels, sensitive_attrs, lambda_param, lambda_entropy_coeff):\n",
        "    \"\"\"\n",
        "    Compute the dual-objective loss with Lagrangian relaxation and entropy regularization.\n",
        "    \n",
        "    :param outputs: Model predictions.\n",
        "    :param labels: Ground truth labels.\n",
        "    :param sensitive_attrs: Sensitive attributes for each instance.\n",
        "    :param lambda_param: Lagrange multiplier for balancing accuracy and fairness.\n",
        "    :param lambda_entropy_coeff: Coefficient for the entropy regularization.\n",
        "    :return: Total loss, prediction loss, fairness loss, entropy regularization term.\n",
        "    \"\"\"\n",
        "    sensitive_attribute = torch.tensor(user_labels[sensitive_attrs].values, dtype=torch.long)\n",
        "    sensitive_attribute = sensitive_attribute.float()\n",
        "\n",
        "    # Prediction loss (e.g., Cross-Entropy for classification)\n",
        "    pred_loss = F.cross_entropy(outputs, labels)\n",
        "\n",
        "    # Fairness loss (e.g., absolute difference in positive prediction rates between groups)\n",
        "    positive_pred_rates_diff = compute_statistical_parity_difference(outputs, sensitive_attribute)\n",
        "    fairness_loss = torch.abs(positive_pred_rates_diff)\n",
        "    \n",
        "    # Entropy regularization for the Lagrange multiplier\n",
        "    entropy_reg = -lambda_entropy_coeff * (lambda_param * torch.log(lambda_param))\n",
        "\n",
        "    # Total loss\n",
        "    total_loss = pred_loss + lambda_param * fairness_loss + entropy_reg\n",
        "\n",
        "    return total_loss, pred_loss, fairness_loss, entropy_reg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "\"None of [Index([                       (tensor(-0.0012), tensor(0.0047), tensor(0.0035), tensor(0.0003), tensor(0.0003), tensor(-0.0034), tensor(-0.0015), tensor(-0.0014), tensor(-0.0017), tensor(-0.0006), tensor(0.0004), tensor(-0.0033), tensor(0.0016), tensor(0.0003), tensor(-0.0012), tensor(0.0022), tensor(-5.6961e-05), tensor(0.0014), tensor(-0.0006), tensor(-0.0015), tensor(0.0025), tensor(-0.0049), tensor(0.0018), tensor(0.0015), tensor(-0.0035), tensor(0.0004), tensor(-0.0003), tensor(-0.0007), tensor(0.0012), tensor(0.0008), tensor(0.0033), tensor(-0.0014), tensor(0.0045), tensor(-0.0013), tensor(0.0023), tensor(-0.0036), tensor(0.0007), tensor(-0.0018), tensor(-0.0010), tensor(0.0040), tensor(-0.0004), tensor(0.0018), tensor(-0.0028), tensor(-0.0024), tensor(-0.0028), tensor(-0.0011), tensor(-0.0002), tensor(0.0017), tensor(-0.0003), tensor(0.0022), tensor(0.0017), tensor(0.0007), tensor(0.0024), tensor(-0.0026), tensor(0.0032), tensor(-0.0029), tensor(0.0027), tensor(0.0060), tensor(-0.0012), tensor(-0.0013), tensor(0.0004), tensor(0.0032), tensor(-0.0038), tensor(-0.0023)),\\n                                  (tensor(-0.0774), tensor(0.1434), tensor(0.1096), tensor(0.0091), tensor(0.0061), tensor(-0.0759), tensor(-0.0347), tensor(-0.0209), tensor(-0.0423), tensor(-0.0175), tensor(-0.0089), tensor(-0.1165), tensor(0.0724), tensor(0.0022), tensor(-0.0300), tensor(0.0860), tensor(0.0149), tensor(0.0808), tensor(-0.0163), tensor(-0.0586), tensor(0.0450), tensor(-0.1565), tensor(0.0182), tensor(0.0348), tensor(-0.1062), tensor(0.0011), tensor(-0.0135), tensor(-0.0369), tensor(0.0144), tensor(0.0418), tensor(0.1259), tensor(-0.0706), tensor(0.1613), tensor(-0.0508), tensor(0.0676), tensor(-0.1075), tensor(0.0128), tensor(-0.0652), tensor(-0.0319), tensor(0.1262), tensor(-0.0093), tensor(0.0323), tensor(-0.1078), tensor(-0.0937), tensor(-0.0734), tensor(-0.0502), tensor(0.0126), tensor(0.0415), tensor(-0.0115), tensor(0.0917), tensor(0.0490), tensor(-0.0029), tensor(0.0865), tensor(-0.0932), tensor(0.0773), tensor(-0.0719), tensor(0.1001), tensor(0.1790), tensor(-0.0049), tensor(-0.0280), tensor(0.0205), tensor(0.1011), tensor(-0.0935), tensor(-0.0778)),\\n                              (tensor(-0.0027), tensor(0.0058), tensor(0.0044), tensor(0.0005), tensor(0.0004), tensor(-0.0029), tensor(-0.0014), tensor(-0.0014), tensor(-0.0018), tensor(-0.0006), tensor(-0.0005), tensor(-0.0041), tensor(0.0024), tensor(0.0002), tensor(-0.0009), tensor(0.0034), tensor(0.0004), tensor(0.0024), tensor(-0.0007), tensor(-0.0023), tensor(0.0022), tensor(-0.0061), tensor(0.0008), tensor(0.0017), tensor(-0.0044), tensor(-0.0001), tensor(-0.0003), tensor(-0.0015), tensor(0.0013), tensor(0.0013), tensor(0.0046), tensor(-0.0023), tensor(0.0061), tensor(-0.0020), tensor(0.0027), tensor(-0.0044), tensor(0.0004), tensor(-0.0022), tensor(-0.0006), tensor(0.0047), tensor(-0.0002), tensor(0.0018), tensor(-0.0040), tensor(-0.0036), tensor(-0.0030), tensor(-0.0016), tensor(1.4068e-05), tensor(0.0017), tensor(-0.0006), tensor(0.0032), tensor(0.0022), tensor(0.0005), tensor(0.0037), tensor(-0.0039), tensor(0.0032), tensor(-0.0030), tensor(0.0040), tensor(0.0067), tensor(-0.0004), tensor(-0.0015), tensor(0.0012), tensor(0.0039), tensor(-0.0042), tensor(-0.0029)),\\n                                 (tensor(-0.1088), tensor(0.2267), tensor(0.1610), tensor(0.0227), tensor(0.0182), tensor(-0.0920), tensor(-0.0446), tensor(-0.0385), tensor(-0.0585), tensor(-0.0174), tensor(-0.0290), tensor(-0.1551), tensor(0.0981), tensor(0.0053), tensor(-0.0316), tensor(0.1345), tensor(0.0046), tensor(0.0917), tensor(-0.0352), tensor(-0.0896), tensor(0.0752), tensor(-0.2362), tensor(0.0185), tensor(0.0649), tensor(-0.1672), tensor(-0.0229), tensor(-0.0036), tensor(-0.0498), tensor(0.0380), tensor(0.0440), tensor(0.1737), tensor(-0.0956), tensor(0.2241), tensor(-0.0710), tensor(0.0849), tensor(-0.1709), tensor(0.0098), tensor(-0.0894), tensor(-0.0175), tensor(0.1629), tensor(-0.0265), tensor(0.0554), tensor(-0.1545), tensor(-0.1304), tensor(-0.1148), tensor(-0.0569), tensor(0.0051), tensor(0.0765), tensor(-0.0172), tensor(0.1281), tensor(0.0803), tensor(-0.0070), tensor(0.1361), tensor(-0.1537), tensor(0.1283), tensor(-0.1016), tensor(0.1610), tensor(0.2450), tensor(-0.0081), tensor(-0.0529), tensor(0.0618), tensor(0.1482), tensor(-0.1656), tensor(-0.0880)),\\n                                  (tensor(-0.0062), tensor(0.0172), tensor(0.0128), tensor(0.0030), tensor(-0.0019), tensor(-0.0101), tensor(-0.0040), tensor(-0.0031), tensor(-0.0073), tensor(-0.0025), tensor(0.0022), tensor(-0.0116), tensor(0.0046), tensor(-0.0012), tensor(-0.0030), tensor(0.0086), tensor(-0.0013), tensor(0.0062), tensor(-0.0019), tensor(-0.0041), tensor(0.0076), tensor(-0.0164), tensor(0.0032), tensor(0.0024), tensor(-0.0103), tensor(0.0007), tensor(0.0023), tensor(-0.0049), tensor(0.0030), tensor(0.0012), tensor(0.0128), tensor(-0.0049), tensor(0.0172), tensor(-0.0024), tensor(0.0073), tensor(-0.0113), tensor(0.0030), tensor(-0.0079), tensor(-0.0026), tensor(0.0111), tensor(-0.0030), tensor(0.0057), tensor(-0.0102), tensor(-0.0073), tensor(-0.0095), tensor(-0.0017), tensor(0.0028), tensor(0.0064), tensor(0.0008), tensor(0.0093), tensor(0.0048), tensor(-0.0007), tensor(0.0067), tensor(-0.0073), tensor(0.0115), tensor(-0.0094), tensor(0.0091), tensor(0.0217), tensor(-0.0004), tensor(-0.0059), tensor(0.0019), tensor(0.0104), tensor(-0.0145), tensor(-0.0070)),\\n                                   (tensor(-0.0224), tensor(0.0599), tensor(0.0458), tensor(0.0095), tensor(0.0026), tensor(-0.0286), tensor(-0.0216), tensor(-0.0220), tensor(-0.0242), tensor(-0.0054), tensor(-0.0049), tensor(-0.0378), tensor(0.0165), tensor(0.0001), tensor(-0.0054), tensor(0.0374), tensor(0.0051), tensor(0.0142), tensor(-0.0064), tensor(-0.0204), tensor(0.0270), tensor(-0.0577), tensor(0.0098), tensor(0.0203), tensor(-0.0447), tensor(-0.0018), tensor(-0.0013), tensor(-0.0183), tensor(0.0240), tensor(0.0098), tensor(0.0399), tensor(-0.0183), tensor(0.0598), tensor(-0.0202), tensor(0.0318), tensor(-0.0424), tensor(0.0021), tensor(-0.0170), tensor(0.0018), tensor(0.0487), tensor(0.0035), tensor(0.0303), tensor(-0.0344), tensor(-0.0336), tensor(-0.0267), tensor(-0.0112), tensor(-0.0051), tensor(0.0126), tensor(-0.0093), tensor(0.0263), tensor(0.0275), tensor(0.0140), tensor(0.0392), tensor(-0.0381), tensor(0.0314), tensor(-0.0367), tensor(0.0413), tensor(0.0648), tensor(-0.0042), tensor(-0.0246), tensor(0.0146), tensor(0.0381), tensor(-0.0493), tensor(-0.0320)),\\n                         (tensor(-0.0022), tensor(0.0069), tensor(0.0051), tensor(0.0008), tensor(0.0007), tensor(-0.0034), tensor(-0.0021), tensor(-0.0025), tensor(-0.0025), tensor(-0.0006), tensor(-0.0006), tensor(-0.0043), tensor(0.0020), tensor(0.0005), tensor(-0.0008), tensor(0.0039), tensor(0.0002), tensor(0.0014), tensor(-0.0010), tensor(-0.0024), tensor(0.0032), tensor(-0.0068), tensor(0.0014), tensor(0.0026), tensor(-0.0052), tensor(-0.0003), tensor(-8.6641e-05), tensor(-0.0016), tensor(0.0025), tensor(0.0008), tensor(0.0043), tensor(-0.0020), tensor(0.0064), tensor(-0.0021), tensor(0.0033), tensor(-0.0051), tensor(0.0004), tensor(-0.0019), tensor(-2.7812e-05), tensor(0.0053), tensor(-0.0002), tensor(0.0031), tensor(-0.0040), tensor(-0.0037), tensor(-0.0035), tensor(-0.0013), tensor(-0.0008), tensor(0.0020), tensor(-0.0010), tensor(0.0030), tensor(0.0032), tensor(0.0013), tensor(0.0043), tensor(-0.0045), tensor(0.0041), tensor(-0.0040), tensor(0.0046), tensor(0.0074), tensor(-0.0010), tensor(-0.0025), tensor(0.0017), tensor(0.0045), tensor(-0.0057), tensor(-0.0032)),\\n                                  (tensor(-0.0055), tensor(0.0192), tensor(0.0141), tensor(0.0017), tensor(0.0021), tensor(-0.0116), tensor(-0.0066), tensor(-0.0071), tensor(-0.0070), tensor(-0.0020), tensor(-0.0008), tensor(-0.0128), tensor(0.0058), tensor(0.0019), tensor(-0.0032), tensor(0.0102), tensor(0.0009), tensor(0.0042), tensor(-0.0028), tensor(-0.0063), tensor(0.0095), tensor(-0.0193), tensor(0.0054), tensor(0.0076), tensor(-0.0145), tensor(0.0001), tensor(-0.0011), tensor(-0.0037), tensor(0.0067), tensor(0.0026), tensor(0.0121), tensor(-0.0058), tensor(0.0179), tensor(-0.0061), tensor(0.0099), tensor(-0.0142), tensor(0.0018), tensor(-0.0055), tensor(-0.0016), tensor(0.0161), tensor(-0.0004), tensor(0.0090), tensor(-0.0113), tensor(-0.0104), tensor(-0.0103), tensor(-0.0043), tensor(-0.0025), tensor(0.0056), tensor(-0.0030), tensor(0.0083), tensor(0.0088), tensor(0.0040), tensor(0.0116), tensor(-0.0122), tensor(0.0118), tensor(-0.0120), tensor(0.0122), tensor(0.0222), tensor(-0.0042), tensor(-0.0064), tensor(0.0034), tensor(0.0130), tensor(-0.0157), tensor(-0.0098)),\\n                                  (tensor(-0.0111), tensor(0.0438), tensor(0.0327), tensor(0.0055), tensor(0.0029), tensor(-0.0272), tensor(-0.0142), tensor(-0.0158), tensor(-0.0171), tensor(-0.0052), tensor(0.0009), tensor(-0.0278), tensor(0.0118), tensor(0.0024), tensor(-0.0075), tensor(0.0225), tensor(-0.0002), tensor(0.0087), tensor(-0.0057), tensor(-0.0139), tensor(0.0231), tensor(-0.0436), tensor(0.0131), tensor(0.0147), tensor(-0.0326), tensor(0.0017), tensor(-0.0005), tensor(-0.0091), tensor(0.0161), tensor(0.0046), tensor(0.0282), tensor(-0.0115), tensor(0.0411), tensor(-0.0123), tensor(0.0217), tensor(-0.0326), tensor(0.0049), tensor(-0.0143), tensor(-0.0033), tensor(0.0344), tensor(-0.0019), tensor(0.0208), tensor(-0.0244), tensor(-0.0222), tensor(-0.0238), tensor(-0.0076), tensor(-0.0039), tensor(0.0143), tensor(-0.0049), tensor(0.0188), tensor(0.0187), tensor(0.0092), tensor(0.0244), tensor(-0.0258), tensor(0.0283), tensor(-0.0272), tensor(0.0267), tensor(0.0517), tensor(-0.0089), tensor(-0.0161), tensor(0.0078), tensor(0.0287), tensor(-0.0375), tensor(-0.0213)),\\n                                 (tensor(-0.0060), tensor(0.0213), tensor(0.0152), tensor(0.0040), tensor(0.0002), tensor(-0.0100), tensor(-0.0056), tensor(-0.0067), tensor(-0.0089), tensor(-0.0023), tensor(-0.0002), tensor(-0.0120), tensor(0.0041), tensor(0.0003), tensor(-0.0018), tensor(0.0114), tensor(-0.0012), tensor(0.0029), tensor(-0.0034), tensor(-0.0058), tensor(0.0100), tensor(-0.0195), tensor(0.0034), tensor(0.0059), tensor(-0.0139), tensor(-0.0010), tensor(0.0026), tensor(-0.0057), tensor(0.0079), tensor(-0.0004), tensor(0.0124), tensor(-0.0050), tensor(0.0188), tensor(-0.0040), tensor(0.0093), tensor(-0.0145), tensor(0.0022), tensor(-0.0066), tensor(0.0006), tensor(0.0128), tensor(-0.0024), tensor(0.0100), tensor(-0.0111), tensor(-0.0093), tensor(-0.0107), tensor(-0.0013), tensor(-0.0009), tensor(0.0073), tensor(-0.0018), tensor(0.0094), tensor(0.0089), tensor(0.0026), tensor(0.0107), tensor(-0.0117), tensor(0.0139), tensor(-0.0120), tensor(0.0124), tensor(0.0228), tensor(-0.0021), tensor(-0.0089), tensor(0.0053), tensor(0.0131), tensor(-0.0191), tensor(-0.0081)),\\n       ...\\n                          (tensor(-0.0040), tensor(0.0119), tensor(0.0089), tensor(0.0013), tensor(0.0010), tensor(-0.0063), tensor(-0.0031), tensor(-0.0039), tensor(-0.0040), tensor(-0.0011), tensor(-0.0007), tensor(-0.0075), tensor(0.0036), tensor(0.0008), tensor(-0.0017), tensor(0.0064), tensor(5.0072e-05), tensor(0.0029), tensor(-0.0017), tensor(-0.0042), tensor(0.0055), tensor(-0.0120), tensor(0.0026), tensor(0.0040), tensor(-0.0091), tensor(-0.0005), tensor(6.5253e-05), tensor(-0.0025), tensor(0.0037), tensor(0.0016), tensor(0.0080), tensor(-0.0035), tensor(0.0112), tensor(-0.0034), tensor(0.0053), tensor(-0.0089), tensor(0.0009), tensor(-0.0038), tensor(-0.0006), tensor(0.0088), tensor(-0.0008), tensor(0.0047), tensor(-0.0072), tensor(-0.0063), tensor(-0.0065), tensor(-0.0022), tensor(-0.0008), tensor(0.0039), tensor(-0.0013), tensor(0.0054), tensor(0.0049), tensor(0.0016), tensor(0.0071), tensor(-0.0077), tensor(0.0074), tensor(-0.0065), tensor(0.0077), tensor(0.0133), tensor(-0.0017), tensor(-0.0038), tensor(0.0026), tensor(0.0077), tensor(-0.0098), tensor(-0.0052)),\\n                                  (tensor(-0.0581), tensor(0.2934), tensor(0.2196), tensor(0.0327), tensor(0.0399), tensor(-0.1607), tensor(-0.0516), tensor(-0.1195), tensor(-0.0881), tensor(-0.0207), tensor(-0.0152), tensor(-0.1494), tensor(0.0630), tensor(0.0413), tensor(-0.0373), tensor(0.1300), tensor(-0.0383), tensor(0.0231), tensor(-0.0538), tensor(-0.1040), tensor(0.1675), tensor(-0.2942), tensor(0.0877), tensor(0.1094), tensor(-0.2382), tensor(-0.0233), tensor(0.0303), tensor(-0.0338), tensor(0.1050), tensor(0.0065), tensor(0.1688), tensor(-0.0508), tensor(0.2372), tensor(-0.0627), tensor(0.1026), tensor(-0.2350), tensor(0.0300), tensor(-0.0843), tensor(0.0054), tensor(0.1699), tensor(-0.0494), tensor(0.1159), tensor(-0.1633), tensor(-0.1302), tensor(-0.1935), tensor(-0.0205), tensor(-0.0382), tensor(0.1306), tensor(-0.0211), tensor(0.1049), tensor(0.1220), tensor(0.0434), tensor(0.1668), tensor(-0.1943), tensor(0.2209), tensor(-0.1475), tensor(0.1763), tensor(0.3134), tensor(-0.0725), tensor(-0.0953), tensor(0.0754), tensor(0.1823), tensor(-0.2747), tensor(-0.0830)),\\n                                    (tensor(-0.1099), tensor(0.2329), tensor(0.2028), tensor(0.0450), tensor(0.0037), tensor(-0.0823), tensor(-0.0056), tensor(-0.0656), tensor(-0.0490), tensor(-0.0088), tensor(-0.0257), tensor(-0.1134), tensor(0.0800), tensor(-0.0152), tensor(-0.0192), tensor(0.1334), tensor(-0.0337), tensor(0.0821), tensor(-0.0157), tensor(-0.1191), tensor(0.1127), tensor(-0.2548), tensor(0.0259), tensor(0.0374), tensor(-0.2172), tensor(-0.0314), tensor(0.0369), tensor(-0.0569), tensor(0.0543), tensor(0.0555), tensor(0.2086), tensor(-0.0580), tensor(0.2441), tensor(-0.0655), tensor(0.0478), tensor(-0.2050), tensor(0.0015), tensor(-0.1097), tensor(0.0269), tensor(0.1153), tensor(-0.0319), tensor(0.0415), tensor(-0.1548), tensor(-0.1271), tensor(-0.1494), tensor(-0.0141), tensor(0.0273), tensor(0.1029), tensor(0.0169), tensor(0.1060), tensor(0.0570), tensor(0.0011), tensor(0.1588), tensor(-0.1678), tensor(0.1515), tensor(-0.0725), tensor(0.1794), tensor(0.2451), tensor(0.0175), tensor(-0.0685), tensor(0.0884), tensor(0.1234), tensor(-0.2091), tensor(-0.0518)),\\n                   (tensor(-0.0030), tensor(0.0088), tensor(0.0062), tensor(0.0017), tensor(-0.0002), tensor(-0.0037), tensor(-0.0017), tensor(-0.0019), tensor(-0.0034), tensor(-0.0010), tensor(-1.4659e-05), tensor(-0.0051), tensor(0.0022), tensor(-0.0002), tensor(-0.0008), tensor(0.0047), tensor(-0.0008), tensor(0.0019), tensor(-0.0014), tensor(-0.0025), tensor(0.0037), tensor(-0.0082), tensor(0.0009), tensor(0.0018), tensor(-0.0055), tensor(-0.0005), tensor(0.0013), tensor(-0.0024), tensor(0.0025), tensor(-0.0001), tensor(0.0057), tensor(-0.0024), tensor(0.0080), tensor(-0.0014), tensor(0.0033), tensor(-0.0060), tensor(0.0009), tensor(-0.0034), tensor(-3.4379e-05), tensor(0.0047), tensor(-0.0016), tensor(0.0032), tensor(-0.0049), tensor(-0.0038), tensor(-0.0044), tensor(-0.0006), tensor(0.0004), tensor(0.0035), tensor(-0.0003), tensor(0.0045), tensor(0.0031), tensor(5.5291e-05), tensor(0.0040), tensor(-0.0047), tensor(0.0058), tensor(-0.0043), tensor(0.0051), tensor(0.0095), tensor(-0.0004), tensor(-0.0033), tensor(0.0023), tensor(0.0054), tensor(-0.0076), tensor(-0.0029)),\\n                                  (tensor(-0.0056), tensor(0.0177), tensor(0.0128), tensor(0.0028), tensor(-0.0002), tensor(-0.0092), tensor(-0.0042), tensor(-0.0049), tensor(-0.0068), tensor(-0.0017), tensor(0.0003), tensor(-0.0106), tensor(0.0040), tensor(0.0002), tensor(-0.0021), tensor(0.0089), tensor(-0.0012), tensor(0.0041), tensor(-0.0026), tensor(-0.0046), tensor(0.0081), tensor(-0.0165), tensor(0.0033), tensor(0.0046), tensor(-0.0115), tensor(-0.0008), tensor(0.0022), tensor(-0.0042), tensor(0.0045), tensor(0.0007), tensor(0.0112), tensor(-0.0044), tensor(0.0159), tensor(-0.0030), tensor(0.0073), tensor(-0.0120), tensor(0.0022), tensor(-0.0062), tensor(-0.0008), tensor(0.0110), tensor(-0.0027), tensor(0.0068), tensor(-0.0101), tensor(-0.0075), tensor(-0.0099), tensor(-0.0015), tensor(0.0005), tensor(0.0063), tensor(-0.0005), tensor(0.0081), tensor(0.0063), tensor(0.0006), tensor(0.0083), tensor(-0.0092), tensor(0.0117), tensor(-0.0095), tensor(0.0100), tensor(0.0200), tensor(-0.0015), tensor(-0.0063), tensor(0.0032), tensor(0.0107), tensor(-0.0154), tensor(-0.0063)),\\n                                  (tensor(-0.0250), tensor(0.0599), tensor(0.0472), tensor(0.0088), tensor(0.0038), tensor(-0.0245), tensor(-0.0104), tensor(-0.0179), tensor(-0.0176), tensor(-0.0043), tensor(-0.0071), tensor(-0.0349), tensor(0.0195), tensor(0.0013), tensor(-0.0055), tensor(0.0348), tensor(-0.0016), tensor(0.0172), tensor(-0.0076), tensor(-0.0252), tensor(0.0260), tensor(-0.0616), tensor(0.0073), tensor(0.0163), tensor(-0.0485), tensor(-0.0058), tensor(0.0038), tensor(-0.0150), tensor(0.0174), tensor(0.0094), tensor(0.0446), tensor(-0.0183), tensor(0.0592), tensor(-0.0176), tensor(0.0221), tensor(-0.0471), tensor(0.0023), tensor(-0.0217), tensor(0.0022), tensor(0.0383), tensor(-0.0048), tensor(0.0193), tensor(-0.0381), tensor(-0.0333), tensor(-0.0328), tensor(-0.0087), tensor(-0.0005), tensor(0.0208), tensor(-0.0041), tensor(0.0285), tensor(0.0226), tensor(0.0051), tensor(0.0388), tensor(-0.0415), tensor(0.0365), tensor(-0.0271), tensor(0.0423), tensor(0.0635), tensor(-0.0023), tensor(-0.0189), tensor(0.0180), tensor(0.0370), tensor(-0.0499), tensor(-0.0221)),\\n                          (tensor(-0.0048), tensor(0.0128), tensor(0.0089), tensor(0.0025), tensor(-0.0013), tensor(-0.0065), tensor(-0.0038), tensor(-0.0022), tensor(-0.0058), tensor(-0.0017), tensor(0.0011), tensor(-0.0086), tensor(0.0032), tensor(-0.0011), tensor(-0.0017), tensor(0.0071), tensor(-0.0003), tensor(0.0041), tensor(-0.0016), tensor(-0.0027), tensor(0.0050), tensor(-0.0115), tensor(0.0015), tensor(0.0022), tensor(-0.0069), tensor(0.0002), tensor(0.0014), tensor(-0.0041), tensor(0.0029), tensor(0.0005), tensor(0.0087), tensor(-0.0040), tensor(0.0124), tensor(-0.0020), tensor(0.0060), tensor(-0.0078), tensor(0.0017), tensor(-0.0052), tensor(-0.0012), tensor(0.0085), tensor(-0.0016), tensor(0.0051), tensor(-0.0071), tensor(-0.0054), tensor(-0.0058), tensor(-0.0014), tensor(0.0015), tensor(0.0039), tensor(-0.0002), tensor(0.0069), tensor(0.0042), tensor(-9.2372e-05), tensor(0.0051), tensor(-0.0054), tensor(0.0077), tensor(-0.0074), tensor(0.0070), tensor(0.0151), tensor(2.9559e-05), tensor(-0.0050), tensor(0.0020), tensor(0.0078), tensor(-0.0105), tensor(-0.0056)),\\n                                 (tensor(-0.1317), tensor(0.2251), tensor(0.1706), tensor(0.0437), tensor(-0.0533), tensor(-0.0942), tensor(0.0081), tensor(0.0163), tensor(-0.0598), tensor(-0.0174), tensor(0.0263), tensor(-0.1442), tensor(0.0816), tensor(-0.0449), tensor(-0.0363), tensor(0.1085), tensor(-0.0526), tensor(0.1454), tensor(-0.0176), tensor(-0.0589), tensor(0.0683), tensor(-0.2209), tensor(0.0040), tensor(-0.0127), tensor(-0.1307), tensor(-0.0239), tensor(0.0712), tensor(-0.0587), tensor(-0.0452), tensor(0.0362), tensor(0.2160), tensor(-0.0743), tensor(0.2345), tensor(-0.0034), tensor(0.0303), tensor(-0.1492), tensor(0.0365), tensor(-0.1474), tensor(-0.0392), tensor(0.0861), tensor(-0.0937), tensor(-0.0158), tensor(-0.1640), tensor(-0.0784), tensor(-0.1516), tensor(-0.0052), tensor(0.1130), tensor(0.1136), tensor(0.0705), tensor(0.1462), tensor(0.0014), tensor(-0.1097), tensor(0.0650), tensor(-0.0828), tensor(0.1562), tensor(-0.0590), tensor(0.1242), tensor(0.2799), tensor(0.0574), tensor(-0.0398), tensor(0.0317), tensor(0.1081), tensor(-0.1825), tensor(-0.0229)),\\n       (tensor(-0.0004), tensor(0.0014), tensor(0.0010), tensor(0.0001), tensor(7.7193e-05), tensor(-0.0008), tensor(-0.0004), tensor(-0.0004), tensor(-0.0005), tensor(-0.0002), tensor(3.6554e-05), tensor(-0.0009), tensor(0.0004), tensor(8.2265e-05), tensor(-0.0003), tensor(0.0007), tensor(-3.4714e-05), tensor(0.0003), tensor(-0.0002), tensor(-0.0004), tensor(0.0007), tensor(-0.0014), tensor(0.0004), tensor(0.0004), tensor(-0.0010), tensor(3.1380e-05), tensor(1.5369e-05), tensor(-0.0003), tensor(0.0004), tensor(0.0001), tensor(0.0009), tensor(-0.0004), tensor(0.0013), tensor(-0.0003), tensor(0.0006), tensor(-0.0010), tensor(0.0002), tensor(-0.0005), tensor(-0.0002), tensor(0.0010), tensor(-0.0001), tensor(0.0005), tensor(-0.0008), tensor(-0.0007), tensor(-0.0008), tensor(-0.0002), tensor(-5.1753e-05), tensor(0.0005), tensor(-0.0001), tensor(0.0006), tensor(0.0005), tensor(0.0002), tensor(0.0007), tensor(-0.0008), tensor(0.0009), tensor(-0.0008), tensor(0.0008), tensor(0.0016), tensor(-0.0003), tensor(-0.0004), tensor(0.0002), tensor(0.0009), tensor(-0.0011), tensor(-0.0006)),\\n                                (tensor(-0.0027), tensor(0.0092), tensor(0.0068), tensor(0.0011), tensor(6.0358e-05), tensor(-0.0057), tensor(-0.0022), tensor(-0.0025), tensor(-0.0034), tensor(-0.0011), tensor(0.0006), tensor(-0.0060), tensor(0.0025), tensor(0.0003), tensor(-0.0017), tensor(0.0044), tensor(-0.0006), tensor(0.0026), tensor(-0.0013), tensor(-0.0026), tensor(0.0045), tensor(-0.0091), tensor(0.0024), tensor(0.0024), tensor(-0.0063), tensor(0.0002), tensor(0.0006), tensor(-0.0018), tensor(0.0021), tensor(0.0008), tensor(0.0063), tensor(-0.0025), tensor(0.0086), tensor(-0.0018), tensor(0.0040), tensor(-0.0066), tensor(0.0014), tensor(-0.0036), tensor(-0.0012), tensor(0.0064), tensor(-0.0014), tensor(0.0033), tensor(-0.0055), tensor(-0.0042), tensor(-0.0055), tensor(-0.0013), tensor(0.0003), tensor(0.0035), tensor(-0.0002), tensor(0.0044), tensor(0.0032), tensor(0.0004), tensor(0.0043), tensor(-0.0048), tensor(0.0063), tensor(-0.0051), tensor(0.0052), tensor(0.0113), tensor(-0.0013), tensor(-0.0029), tensor(0.0012), tensor(0.0058), tensor(-0.0078), tensor(-0.0037))],\\n      dtype='object', length=166958)] are in the [columns]\"",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[51], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m lambda_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     17\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model2(data\u001b[38;5;241m.\u001b[39mx, data\u001b[38;5;241m.\u001b[39medge_index)\n\u001b[0;32m---> 18\u001b[0m total_loss, pred_loss, fairness_loss, entropy_reg \u001b[38;5;241m=\u001b[39m dual_objective_loss_with_lagrangian_relaxation(\n\u001b[1;32m     19\u001b[0m     outputs, data\u001b[38;5;241m.\u001b[39my, sens_attr, lambda_param, lambda_entropy_coeff\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Update model parameters\u001b[39;00m\n\u001b[1;32m     23\u001b[0m total_loss\u001b[38;5;241m.\u001b[39mbackward()\n",
            "Cell \u001b[0;32mIn[48], line 22\u001b[0m, in \u001b[0;36mdual_objective_loss_with_lagrangian_relaxation\u001b[0;34m(outputs, labels, sensitive_attrs, lambda_param, lambda_entropy_coeff)\u001b[0m\n\u001b[1;32m     19\u001b[0m pred_loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(outputs, labels)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Fairness loss (e.g., absolute difference in positive prediction rates between groups)\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m positive_pred_rates_diff \u001b[38;5;241m=\u001b[39m compute_statistical_parity_difference(outputs, sensitive_attribute)\n\u001b[1;32m     23\u001b[0m fairness_loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mabs(positive_pred_rates_diff)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Entropy regularization for the Lagrange multiplier\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[50], line 15\u001b[0m, in \u001b[0;36mcompute_statistical_parity_difference\u001b[0;34m(label, predictions, sens_attr)\u001b[0m\n\u001b[1;32m     13\u001b[0m labels \u001b[38;5;241m=\u001b[39m label\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m     14\u001b[0m predictions \u001b[38;5;241m=\u001b[39m predictions\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mclone()\n\u001b[0;32m---> 15\u001b[0m labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(user_labels[labels]\u001b[38;5;241m.\u001b[39mvalues, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m     16\u001b[0m sensitive_attribute \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(user_labels[sens_attr]\u001b[38;5;241m.\u001b[39mvalues, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m     18\u001b[0m predictions \u001b[38;5;241m=\u001b[39m predictions\u001b[38;5;241m.\u001b[39mfloat()\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:3767\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3766\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 3767\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39m_get_indexer_strict(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   3769\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3770\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:5877\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   5874\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5875\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 5877\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[1;32m   5879\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   5880\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   5881\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:5938\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   5936\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_interval_msg:\n\u001b[1;32m   5937\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 5938\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   5940\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m   5941\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mKeyError\u001b[0m: \"None of [Index([                       (tensor(-0.0012), tensor(0.0047), tensor(0.0035), tensor(0.0003), tensor(0.0003), tensor(-0.0034), tensor(-0.0015), tensor(-0.0014), tensor(-0.0017), tensor(-0.0006), tensor(0.0004), tensor(-0.0033), tensor(0.0016), tensor(0.0003), tensor(-0.0012), tensor(0.0022), tensor(-5.6961e-05), tensor(0.0014), tensor(-0.0006), tensor(-0.0015), tensor(0.0025), tensor(-0.0049), tensor(0.0018), tensor(0.0015), tensor(-0.0035), tensor(0.0004), tensor(-0.0003), tensor(-0.0007), tensor(0.0012), tensor(0.0008), tensor(0.0033), tensor(-0.0014), tensor(0.0045), tensor(-0.0013), tensor(0.0023), tensor(-0.0036), tensor(0.0007), tensor(-0.0018), tensor(-0.0010), tensor(0.0040), tensor(-0.0004), tensor(0.0018), tensor(-0.0028), tensor(-0.0024), tensor(-0.0028), tensor(-0.0011), tensor(-0.0002), tensor(0.0017), tensor(-0.0003), tensor(0.0022), tensor(0.0017), tensor(0.0007), tensor(0.0024), tensor(-0.0026), tensor(0.0032), tensor(-0.0029), tensor(0.0027), tensor(0.0060), tensor(-0.0012), tensor(-0.0013), tensor(0.0004), tensor(0.0032), tensor(-0.0038), tensor(-0.0023)),\\n                                  (tensor(-0.0774), tensor(0.1434), tensor(0.1096), tensor(0.0091), tensor(0.0061), tensor(-0.0759), tensor(-0.0347), tensor(-0.0209), tensor(-0.0423), tensor(-0.0175), tensor(-0.0089), tensor(-0.1165), tensor(0.0724), tensor(0.0022), tensor(-0.0300), tensor(0.0860), tensor(0.0149), tensor(0.0808), tensor(-0.0163), tensor(-0.0586), tensor(0.0450), tensor(-0.1565), tensor(0.0182), tensor(0.0348), tensor(-0.1062), tensor(0.0011), tensor(-0.0135), tensor(-0.0369), tensor(0.0144), tensor(0.0418), tensor(0.1259), tensor(-0.0706), tensor(0.1613), tensor(-0.0508), tensor(0.0676), tensor(-0.1075), tensor(0.0128), tensor(-0.0652), tensor(-0.0319), tensor(0.1262), tensor(-0.0093), tensor(0.0323), tensor(-0.1078), tensor(-0.0937), tensor(-0.0734), tensor(-0.0502), tensor(0.0126), tensor(0.0415), tensor(-0.0115), tensor(0.0917), tensor(0.0490), tensor(-0.0029), tensor(0.0865), tensor(-0.0932), tensor(0.0773), tensor(-0.0719), tensor(0.1001), tensor(0.1790), tensor(-0.0049), tensor(-0.0280), tensor(0.0205), tensor(0.1011), tensor(-0.0935), tensor(-0.0778)),\\n                              (tensor(-0.0027), tensor(0.0058), tensor(0.0044), tensor(0.0005), tensor(0.0004), tensor(-0.0029), tensor(-0.0014), tensor(-0.0014), tensor(-0.0018), tensor(-0.0006), tensor(-0.0005), tensor(-0.0041), tensor(0.0024), tensor(0.0002), tensor(-0.0009), tensor(0.0034), tensor(0.0004), tensor(0.0024), tensor(-0.0007), tensor(-0.0023), tensor(0.0022), tensor(-0.0061), tensor(0.0008), tensor(0.0017), tensor(-0.0044), tensor(-0.0001), tensor(-0.0003), tensor(-0.0015), tensor(0.0013), tensor(0.0013), tensor(0.0046), tensor(-0.0023), tensor(0.0061), tensor(-0.0020), tensor(0.0027), tensor(-0.0044), tensor(0.0004), tensor(-0.0022), tensor(-0.0006), tensor(0.0047), tensor(-0.0002), tensor(0.0018), tensor(-0.0040), tensor(-0.0036), tensor(-0.0030), tensor(-0.0016), tensor(1.4068e-05), tensor(0.0017), tensor(-0.0006), tensor(0.0032), tensor(0.0022), tensor(0.0005), tensor(0.0037), tensor(-0.0039), tensor(0.0032), tensor(-0.0030), tensor(0.0040), tensor(0.0067), tensor(-0.0004), tensor(-0.0015), tensor(0.0012), tensor(0.0039), tensor(-0.0042), tensor(-0.0029)),\\n                                 (tensor(-0.1088), tensor(0.2267), tensor(0.1610), tensor(0.0227), tensor(0.0182), tensor(-0.0920), tensor(-0.0446), tensor(-0.0385), tensor(-0.0585), tensor(-0.0174), tensor(-0.0290), tensor(-0.1551), tensor(0.0981), tensor(0.0053), tensor(-0.0316), tensor(0.1345), tensor(0.0046), tensor(0.0917), tensor(-0.0352), tensor(-0.0896), tensor(0.0752), tensor(-0.2362), tensor(0.0185), tensor(0.0649), tensor(-0.1672), tensor(-0.0229), tensor(-0.0036), tensor(-0.0498), tensor(0.0380), tensor(0.0440), tensor(0.1737), tensor(-0.0956), tensor(0.2241), tensor(-0.0710), tensor(0.0849), tensor(-0.1709), tensor(0.0098), tensor(-0.0894), tensor(-0.0175), tensor(0.1629), tensor(-0.0265), tensor(0.0554), tensor(-0.1545), tensor(-0.1304), tensor(-0.1148), tensor(-0.0569), tensor(0.0051), tensor(0.0765), tensor(-0.0172), tensor(0.1281), tensor(0.0803), tensor(-0.0070), tensor(0.1361), tensor(-0.1537), tensor(0.1283), tensor(-0.1016), tensor(0.1610), tensor(0.2450), tensor(-0.0081), tensor(-0.0529), tensor(0.0618), tensor(0.1482), tensor(-0.1656), tensor(-0.0880)),\\n                                  (tensor(-0.0062), tensor(0.0172), tensor(0.0128), tensor(0.0030), tensor(-0.0019), tensor(-0.0101), tensor(-0.0040), tensor(-0.0031), tensor(-0.0073), tensor(-0.0025), tensor(0.0022), tensor(-0.0116), tensor(0.0046), tensor(-0.0012), tensor(-0.0030), tensor(0.0086), tensor(-0.0013), tensor(0.0062), tensor(-0.0019), tensor(-0.0041), tensor(0.0076), tensor(-0.0164), tensor(0.0032), tensor(0.0024), tensor(-0.0103), tensor(0.0007), tensor(0.0023), tensor(-0.0049), tensor(0.0030), tensor(0.0012), tensor(0.0128), tensor(-0.0049), tensor(0.0172), tensor(-0.0024), tensor(0.0073), tensor(-0.0113), tensor(0.0030), tensor(-0.0079), tensor(-0.0026), tensor(0.0111), tensor(-0.0030), tensor(0.0057), tensor(-0.0102), tensor(-0.0073), tensor(-0.0095), tensor(-0.0017), tensor(0.0028), tensor(0.0064), tensor(0.0008), tensor(0.0093), tensor(0.0048), tensor(-0.0007), tensor(0.0067), tensor(-0.0073), tensor(0.0115), tensor(-0.0094), tensor(0.0091), tensor(0.0217), tensor(-0.0004), tensor(-0.0059), tensor(0.0019), tensor(0.0104), tensor(-0.0145), tensor(-0.0070)),\\n                                   (tensor(-0.0224), tensor(0.0599), tensor(0.0458), tensor(0.0095), tensor(0.0026), tensor(-0.0286), tensor(-0.0216), tensor(-0.0220), tensor(-0.0242), tensor(-0.0054), tensor(-0.0049), tensor(-0.0378), tensor(0.0165), tensor(0.0001), tensor(-0.0054), tensor(0.0374), tensor(0.0051), tensor(0.0142), tensor(-0.0064), tensor(-0.0204), tensor(0.0270), tensor(-0.0577), tensor(0.0098), tensor(0.0203), tensor(-0.0447), tensor(-0.0018), tensor(-0.0013), tensor(-0.0183), tensor(0.0240), tensor(0.0098), tensor(0.0399), tensor(-0.0183), tensor(0.0598), tensor(-0.0202), tensor(0.0318), tensor(-0.0424), tensor(0.0021), tensor(-0.0170), tensor(0.0018), tensor(0.0487), tensor(0.0035), tensor(0.0303), tensor(-0.0344), tensor(-0.0336), tensor(-0.0267), tensor(-0.0112), tensor(-0.0051), tensor(0.0126), tensor(-0.0093), tensor(0.0263), tensor(0.0275), tensor(0.0140), tensor(0.0392), tensor(-0.0381), tensor(0.0314), tensor(-0.0367), tensor(0.0413), tensor(0.0648), tensor(-0.0042), tensor(-0.0246), tensor(0.0146), tensor(0.0381), tensor(-0.0493), tensor(-0.0320)),\\n                         (tensor(-0.0022), tensor(0.0069), tensor(0.0051), tensor(0.0008), tensor(0.0007), tensor(-0.0034), tensor(-0.0021), tensor(-0.0025), tensor(-0.0025), tensor(-0.0006), tensor(-0.0006), tensor(-0.0043), tensor(0.0020), tensor(0.0005), tensor(-0.0008), tensor(0.0039), tensor(0.0002), tensor(0.0014), tensor(-0.0010), tensor(-0.0024), tensor(0.0032), tensor(-0.0068), tensor(0.0014), tensor(0.0026), tensor(-0.0052), tensor(-0.0003), tensor(-8.6641e-05), tensor(-0.0016), tensor(0.0025), tensor(0.0008), tensor(0.0043), tensor(-0.0020), tensor(0.0064), tensor(-0.0021), tensor(0.0033), tensor(-0.0051), tensor(0.0004), tensor(-0.0019), tensor(-2.7812e-05), tensor(0.0053), tensor(-0.0002), tensor(0.0031), tensor(-0.0040), tensor(-0.0037), tensor(-0.0035), tensor(-0.0013), tensor(-0.0008), tensor(0.0020), tensor(-0.0010), tensor(0.0030), tensor(0.0032), tensor(0.0013), tensor(0.0043), tensor(-0.0045), tensor(0.0041), tensor(-0.0040), tensor(0.0046), tensor(0.0074), tensor(-0.0010), tensor(-0.0025), tensor(0.0017), tensor(0.0045), tensor(-0.0057), tensor(-0.0032)),\\n                                  (tensor(-0.0055), tensor(0.0192), tensor(0.0141), tensor(0.0017), tensor(0.0021), tensor(-0.0116), tensor(-0.0066), tensor(-0.0071), tensor(-0.0070), tensor(-0.0020), tensor(-0.0008), tensor(-0.0128), tensor(0.0058), tensor(0.0019), tensor(-0.0032), tensor(0.0102), tensor(0.0009), tensor(0.0042), tensor(-0.0028), tensor(-0.0063), tensor(0.0095), tensor(-0.0193), tensor(0.0054), tensor(0.0076), tensor(-0.0145), tensor(0.0001), tensor(-0.0011), tensor(-0.0037), tensor(0.0067), tensor(0.0026), tensor(0.0121), tensor(-0.0058), tensor(0.0179), tensor(-0.0061), tensor(0.0099), tensor(-0.0142), tensor(0.0018), tensor(-0.0055), tensor(-0.0016), tensor(0.0161), tensor(-0.0004), tensor(0.0090), tensor(-0.0113), tensor(-0.0104), tensor(-0.0103), tensor(-0.0043), tensor(-0.0025), tensor(0.0056), tensor(-0.0030), tensor(0.0083), tensor(0.0088), tensor(0.0040), tensor(0.0116), tensor(-0.0122), tensor(0.0118), tensor(-0.0120), tensor(0.0122), tensor(0.0222), tensor(-0.0042), tensor(-0.0064), tensor(0.0034), tensor(0.0130), tensor(-0.0157), tensor(-0.0098)),\\n                                  (tensor(-0.0111), tensor(0.0438), tensor(0.0327), tensor(0.0055), tensor(0.0029), tensor(-0.0272), tensor(-0.0142), tensor(-0.0158), tensor(-0.0171), tensor(-0.0052), tensor(0.0009), tensor(-0.0278), tensor(0.0118), tensor(0.0024), tensor(-0.0075), tensor(0.0225), tensor(-0.0002), tensor(0.0087), tensor(-0.0057), tensor(-0.0139), tensor(0.0231), tensor(-0.0436), tensor(0.0131), tensor(0.0147), tensor(-0.0326), tensor(0.0017), tensor(-0.0005), tensor(-0.0091), tensor(0.0161), tensor(0.0046), tensor(0.0282), tensor(-0.0115), tensor(0.0411), tensor(-0.0123), tensor(0.0217), tensor(-0.0326), tensor(0.0049), tensor(-0.0143), tensor(-0.0033), tensor(0.0344), tensor(-0.0019), tensor(0.0208), tensor(-0.0244), tensor(-0.0222), tensor(-0.0238), tensor(-0.0076), tensor(-0.0039), tensor(0.0143), tensor(-0.0049), tensor(0.0188), tensor(0.0187), tensor(0.0092), tensor(0.0244), tensor(-0.0258), tensor(0.0283), tensor(-0.0272), tensor(0.0267), tensor(0.0517), tensor(-0.0089), tensor(-0.0161), tensor(0.0078), tensor(0.0287), tensor(-0.0375), tensor(-0.0213)),\\n                                 (tensor(-0.0060), tensor(0.0213), tensor(0.0152), tensor(0.0040), tensor(0.0002), tensor(-0.0100), tensor(-0.0056), tensor(-0.0067), tensor(-0.0089), tensor(-0.0023), tensor(-0.0002), tensor(-0.0120), tensor(0.0041), tensor(0.0003), tensor(-0.0018), tensor(0.0114), tensor(-0.0012), tensor(0.0029), tensor(-0.0034), tensor(-0.0058), tensor(0.0100), tensor(-0.0195), tensor(0.0034), tensor(0.0059), tensor(-0.0139), tensor(-0.0010), tensor(0.0026), tensor(-0.0057), tensor(0.0079), tensor(-0.0004), tensor(0.0124), tensor(-0.0050), tensor(0.0188), tensor(-0.0040), tensor(0.0093), tensor(-0.0145), tensor(0.0022), tensor(-0.0066), tensor(0.0006), tensor(0.0128), tensor(-0.0024), tensor(0.0100), tensor(-0.0111), tensor(-0.0093), tensor(-0.0107), tensor(-0.0013), tensor(-0.0009), tensor(0.0073), tensor(-0.0018), tensor(0.0094), tensor(0.0089), tensor(0.0026), tensor(0.0107), tensor(-0.0117), tensor(0.0139), tensor(-0.0120), tensor(0.0124), tensor(0.0228), tensor(-0.0021), tensor(-0.0089), tensor(0.0053), tensor(0.0131), tensor(-0.0191), tensor(-0.0081)),\\n       ...\\n                          (tensor(-0.0040), tensor(0.0119), tensor(0.0089), tensor(0.0013), tensor(0.0010), tensor(-0.0063), tensor(-0.0031), tensor(-0.0039), tensor(-0.0040), tensor(-0.0011), tensor(-0.0007), tensor(-0.0075), tensor(0.0036), tensor(0.0008), tensor(-0.0017), tensor(0.0064), tensor(5.0072e-05), tensor(0.0029), tensor(-0.0017), tensor(-0.0042), tensor(0.0055), tensor(-0.0120), tensor(0.0026), tensor(0.0040), tensor(-0.0091), tensor(-0.0005), tensor(6.5253e-05), tensor(-0.0025), tensor(0.0037), tensor(0.0016), tensor(0.0080), tensor(-0.0035), tensor(0.0112), tensor(-0.0034), tensor(0.0053), tensor(-0.0089), tensor(0.0009), tensor(-0.0038), tensor(-0.0006), tensor(0.0088), tensor(-0.0008), tensor(0.0047), tensor(-0.0072), tensor(-0.0063), tensor(-0.0065), tensor(-0.0022), tensor(-0.0008), tensor(0.0039), tensor(-0.0013), tensor(0.0054), tensor(0.0049), tensor(0.0016), tensor(0.0071), tensor(-0.0077), tensor(0.0074), tensor(-0.0065), tensor(0.0077), tensor(0.0133), tensor(-0.0017), tensor(-0.0038), tensor(0.0026), tensor(0.0077), tensor(-0.0098), tensor(-0.0052)),\\n                                  (tensor(-0.0581), tensor(0.2934), tensor(0.2196), tensor(0.0327), tensor(0.0399), tensor(-0.1607), tensor(-0.0516), tensor(-0.1195), tensor(-0.0881), tensor(-0.0207), tensor(-0.0152), tensor(-0.1494), tensor(0.0630), tensor(0.0413), tensor(-0.0373), tensor(0.1300), tensor(-0.0383), tensor(0.0231), tensor(-0.0538), tensor(-0.1040), tensor(0.1675), tensor(-0.2942), tensor(0.0877), tensor(0.1094), tensor(-0.2382), tensor(-0.0233), tensor(0.0303), tensor(-0.0338), tensor(0.1050), tensor(0.0065), tensor(0.1688), tensor(-0.0508), tensor(0.2372), tensor(-0.0627), tensor(0.1026), tensor(-0.2350), tensor(0.0300), tensor(-0.0843), tensor(0.0054), tensor(0.1699), tensor(-0.0494), tensor(0.1159), tensor(-0.1633), tensor(-0.1302), tensor(-0.1935), tensor(-0.0205), tensor(-0.0382), tensor(0.1306), tensor(-0.0211), tensor(0.1049), tensor(0.1220), tensor(0.0434), tensor(0.1668), tensor(-0.1943), tensor(0.2209), tensor(-0.1475), tensor(0.1763), tensor(0.3134), tensor(-0.0725), tensor(-0.0953), tensor(0.0754), tensor(0.1823), tensor(-0.2747), tensor(-0.0830)),\\n                                    (tensor(-0.1099), tensor(0.2329), tensor(0.2028), tensor(0.0450), tensor(0.0037), tensor(-0.0823), tensor(-0.0056), tensor(-0.0656), tensor(-0.0490), tensor(-0.0088), tensor(-0.0257), tensor(-0.1134), tensor(0.0800), tensor(-0.0152), tensor(-0.0192), tensor(0.1334), tensor(-0.0337), tensor(0.0821), tensor(-0.0157), tensor(-0.1191), tensor(0.1127), tensor(-0.2548), tensor(0.0259), tensor(0.0374), tensor(-0.2172), tensor(-0.0314), tensor(0.0369), tensor(-0.0569), tensor(0.0543), tensor(0.0555), tensor(0.2086), tensor(-0.0580), tensor(0.2441), tensor(-0.0655), tensor(0.0478), tensor(-0.2050), tensor(0.0015), tensor(-0.1097), tensor(0.0269), tensor(0.1153), tensor(-0.0319), tensor(0.0415), tensor(-0.1548), tensor(-0.1271), tensor(-0.1494), tensor(-0.0141), tensor(0.0273), tensor(0.1029), tensor(0.0169), tensor(0.1060), tensor(0.0570), tensor(0.0011), tensor(0.1588), tensor(-0.1678), tensor(0.1515), tensor(-0.0725), tensor(0.1794), tensor(0.2451), tensor(0.0175), tensor(-0.0685), tensor(0.0884), tensor(0.1234), tensor(-0.2091), tensor(-0.0518)),\\n                   (tensor(-0.0030), tensor(0.0088), tensor(0.0062), tensor(0.0017), tensor(-0.0002), tensor(-0.0037), tensor(-0.0017), tensor(-0.0019), tensor(-0.0034), tensor(-0.0010), tensor(-1.4659e-05), tensor(-0.0051), tensor(0.0022), tensor(-0.0002), tensor(-0.0008), tensor(0.0047), tensor(-0.0008), tensor(0.0019), tensor(-0.0014), tensor(-0.0025), tensor(0.0037), tensor(-0.0082), tensor(0.0009), tensor(0.0018), tensor(-0.0055), tensor(-0.0005), tensor(0.0013), tensor(-0.0024), tensor(0.0025), tensor(-0.0001), tensor(0.0057), tensor(-0.0024), tensor(0.0080), tensor(-0.0014), tensor(0.0033), tensor(-0.0060), tensor(0.0009), tensor(-0.0034), tensor(-3.4379e-05), tensor(0.0047), tensor(-0.0016), tensor(0.0032), tensor(-0.0049), tensor(-0.0038), tensor(-0.0044), tensor(-0.0006), tensor(0.0004), tensor(0.0035), tensor(-0.0003), tensor(0.0045), tensor(0.0031), tensor(5.5291e-05), tensor(0.0040), tensor(-0.0047), tensor(0.0058), tensor(-0.0043), tensor(0.0051), tensor(0.0095), tensor(-0.0004), tensor(-0.0033), tensor(0.0023), tensor(0.0054), tensor(-0.0076), tensor(-0.0029)),\\n                                  (tensor(-0.0056), tensor(0.0177), tensor(0.0128), tensor(0.0028), tensor(-0.0002), tensor(-0.0092), tensor(-0.0042), tensor(-0.0049), tensor(-0.0068), tensor(-0.0017), tensor(0.0003), tensor(-0.0106), tensor(0.0040), tensor(0.0002), tensor(-0.0021), tensor(0.0089), tensor(-0.0012), tensor(0.0041), tensor(-0.0026), tensor(-0.0046), tensor(0.0081), tensor(-0.0165), tensor(0.0033), tensor(0.0046), tensor(-0.0115), tensor(-0.0008), tensor(0.0022), tensor(-0.0042), tensor(0.0045), tensor(0.0007), tensor(0.0112), tensor(-0.0044), tensor(0.0159), tensor(-0.0030), tensor(0.0073), tensor(-0.0120), tensor(0.0022), tensor(-0.0062), tensor(-0.0008), tensor(0.0110), tensor(-0.0027), tensor(0.0068), tensor(-0.0101), tensor(-0.0075), tensor(-0.0099), tensor(-0.0015), tensor(0.0005), tensor(0.0063), tensor(-0.0005), tensor(0.0081), tensor(0.0063), tensor(0.0006), tensor(0.0083), tensor(-0.0092), tensor(0.0117), tensor(-0.0095), tensor(0.0100), tensor(0.0200), tensor(-0.0015), tensor(-0.0063), tensor(0.0032), tensor(0.0107), tensor(-0.0154), tensor(-0.0063)),\\n                                  (tensor(-0.0250), tensor(0.0599), tensor(0.0472), tensor(0.0088), tensor(0.0038), tensor(-0.0245), tensor(-0.0104), tensor(-0.0179), tensor(-0.0176), tensor(-0.0043), tensor(-0.0071), tensor(-0.0349), tensor(0.0195), tensor(0.0013), tensor(-0.0055), tensor(0.0348), tensor(-0.0016), tensor(0.0172), tensor(-0.0076), tensor(-0.0252), tensor(0.0260), tensor(-0.0616), tensor(0.0073), tensor(0.0163), tensor(-0.0485), tensor(-0.0058), tensor(0.0038), tensor(-0.0150), tensor(0.0174), tensor(0.0094), tensor(0.0446), tensor(-0.0183), tensor(0.0592), tensor(-0.0176), tensor(0.0221), tensor(-0.0471), tensor(0.0023), tensor(-0.0217), tensor(0.0022), tensor(0.0383), tensor(-0.0048), tensor(0.0193), tensor(-0.0381), tensor(-0.0333), tensor(-0.0328), tensor(-0.0087), tensor(-0.0005), tensor(0.0208), tensor(-0.0041), tensor(0.0285), tensor(0.0226), tensor(0.0051), tensor(0.0388), tensor(-0.0415), tensor(0.0365), tensor(-0.0271), tensor(0.0423), tensor(0.0635), tensor(-0.0023), tensor(-0.0189), tensor(0.0180), tensor(0.0370), tensor(-0.0499), tensor(-0.0221)),\\n                          (tensor(-0.0048), tensor(0.0128), tensor(0.0089), tensor(0.0025), tensor(-0.0013), tensor(-0.0065), tensor(-0.0038), tensor(-0.0022), tensor(-0.0058), tensor(-0.0017), tensor(0.0011), tensor(-0.0086), tensor(0.0032), tensor(-0.0011), tensor(-0.0017), tensor(0.0071), tensor(-0.0003), tensor(0.0041), tensor(-0.0016), tensor(-0.0027), tensor(0.0050), tensor(-0.0115), tensor(0.0015), tensor(0.0022), tensor(-0.0069), tensor(0.0002), tensor(0.0014), tensor(-0.0041), tensor(0.0029), tensor(0.0005), tensor(0.0087), tensor(-0.0040), tensor(0.0124), tensor(-0.0020), tensor(0.0060), tensor(-0.0078), tensor(0.0017), tensor(-0.0052), tensor(-0.0012), tensor(0.0085), tensor(-0.0016), tensor(0.0051), tensor(-0.0071), tensor(-0.0054), tensor(-0.0058), tensor(-0.0014), tensor(0.0015), tensor(0.0039), tensor(-0.0002), tensor(0.0069), tensor(0.0042), tensor(-9.2372e-05), tensor(0.0051), tensor(-0.0054), tensor(0.0077), tensor(-0.0074), tensor(0.0070), tensor(0.0151), tensor(2.9559e-05), tensor(-0.0050), tensor(0.0020), tensor(0.0078), tensor(-0.0105), tensor(-0.0056)),\\n                                 (tensor(-0.1317), tensor(0.2251), tensor(0.1706), tensor(0.0437), tensor(-0.0533), tensor(-0.0942), tensor(0.0081), tensor(0.0163), tensor(-0.0598), tensor(-0.0174), tensor(0.0263), tensor(-0.1442), tensor(0.0816), tensor(-0.0449), tensor(-0.0363), tensor(0.1085), tensor(-0.0526), tensor(0.1454), tensor(-0.0176), tensor(-0.0589), tensor(0.0683), tensor(-0.2209), tensor(0.0040), tensor(-0.0127), tensor(-0.1307), tensor(-0.0239), tensor(0.0712), tensor(-0.0587), tensor(-0.0452), tensor(0.0362), tensor(0.2160), tensor(-0.0743), tensor(0.2345), tensor(-0.0034), tensor(0.0303), tensor(-0.1492), tensor(0.0365), tensor(-0.1474), tensor(-0.0392), tensor(0.0861), tensor(-0.0937), tensor(-0.0158), tensor(-0.1640), tensor(-0.0784), tensor(-0.1516), tensor(-0.0052), tensor(0.1130), tensor(0.1136), tensor(0.0705), tensor(0.1462), tensor(0.0014), tensor(-0.1097), tensor(0.0650), tensor(-0.0828), tensor(0.1562), tensor(-0.0590), tensor(0.1242), tensor(0.2799), tensor(0.0574), tensor(-0.0398), tensor(0.0317), tensor(0.1081), tensor(-0.1825), tensor(-0.0229)),\\n       (tensor(-0.0004), tensor(0.0014), tensor(0.0010), tensor(0.0001), tensor(7.7193e-05), tensor(-0.0008), tensor(-0.0004), tensor(-0.0004), tensor(-0.0005), tensor(-0.0002), tensor(3.6554e-05), tensor(-0.0009), tensor(0.0004), tensor(8.2265e-05), tensor(-0.0003), tensor(0.0007), tensor(-3.4714e-05), tensor(0.0003), tensor(-0.0002), tensor(-0.0004), tensor(0.0007), tensor(-0.0014), tensor(0.0004), tensor(0.0004), tensor(-0.0010), tensor(3.1380e-05), tensor(1.5369e-05), tensor(-0.0003), tensor(0.0004), tensor(0.0001), tensor(0.0009), tensor(-0.0004), tensor(0.0013), tensor(-0.0003), tensor(0.0006), tensor(-0.0010), tensor(0.0002), tensor(-0.0005), tensor(-0.0002), tensor(0.0010), tensor(-0.0001), tensor(0.0005), tensor(-0.0008), tensor(-0.0007), tensor(-0.0008), tensor(-0.0002), tensor(-5.1753e-05), tensor(0.0005), tensor(-0.0001), tensor(0.0006), tensor(0.0005), tensor(0.0002), tensor(0.0007), tensor(-0.0008), tensor(0.0009), tensor(-0.0008), tensor(0.0008), tensor(0.0016), tensor(-0.0003), tensor(-0.0004), tensor(0.0002), tensor(0.0009), tensor(-0.0011), tensor(-0.0006)),\\n                                (tensor(-0.0027), tensor(0.0092), tensor(0.0068), tensor(0.0011), tensor(6.0358e-05), tensor(-0.0057), tensor(-0.0022), tensor(-0.0025), tensor(-0.0034), tensor(-0.0011), tensor(0.0006), tensor(-0.0060), tensor(0.0025), tensor(0.0003), tensor(-0.0017), tensor(0.0044), tensor(-0.0006), tensor(0.0026), tensor(-0.0013), tensor(-0.0026), tensor(0.0045), tensor(-0.0091), tensor(0.0024), tensor(0.0024), tensor(-0.0063), tensor(0.0002), tensor(0.0006), tensor(-0.0018), tensor(0.0021), tensor(0.0008), tensor(0.0063), tensor(-0.0025), tensor(0.0086), tensor(-0.0018), tensor(0.0040), tensor(-0.0066), tensor(0.0014), tensor(-0.0036), tensor(-0.0012), tensor(0.0064), tensor(-0.0014), tensor(0.0033), tensor(-0.0055), tensor(-0.0042), tensor(-0.0055), tensor(-0.0013), tensor(0.0003), tensor(0.0035), tensor(-0.0002), tensor(0.0044), tensor(0.0032), tensor(0.0004), tensor(0.0043), tensor(-0.0048), tensor(0.0063), tensor(-0.0051), tensor(0.0052), tensor(0.0113), tensor(-0.0013), tensor(-0.0029), tensor(0.0012), tensor(0.0058), tensor(-0.0078), tensor(-0.0037))],\\n      dtype='object', length=166958)] are in the [columns]\""
          ]
        }
      ],
      "source": [
        "from torch.optim import Adam\n",
        "\n",
        "sens_attr = 'bin_age'  # Sensitive attribute for fairness-aware training\n",
        "\n",
        "# Model, optimizer, and data setup\n",
        "model2 = FairnessAwareMessagePassingLayer(in_channels, out_channels)\n",
        "optimizer = Adam(model2.parameters(), lr=0.01)\n",
        "lambda_param = torch.tensor(0.01, requires_grad=True)  # Initial Lagrange multiplier\n",
        "lambda_optimizer = Adam([lambda_param], lr=0.01)  # Optimizer for the Lagrange multiplier\n",
        "lambda_entropy_coeff = 0.1  # Coefficient for entropy regularization\n",
        "\n",
        "for epoch in range(50):  # Number of epochs\n",
        "    model2.train()\n",
        "    optimizer.zero_grad()\n",
        "    lambda_optimizer.zero_grad()\n",
        "    \n",
        "    outputs = model2(data.x, data.edge_index)\n",
        "    total_loss, pred_loss, fairness_loss, entropy_reg = dual_objective_loss_with_lagrangian_relaxation(\n",
        "        outputs, data.y, sens_attr, lambda_param, lambda_entropy_coeff\n",
        "    )\n",
        "    \n",
        "    # Update model parameters\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    # Update the Lagrange multiplier (separate optimizer to allow for ascent)\n",
        "    (-total_loss).backward()  # Negate for gradient ascent on lambda\n",
        "    lambda_optimizer.step()\n",
        "    \n",
        "    print(f'Epoch: {epoch+1}, Total Loss: {total_loss.item()}, Pred Loss: {pred_loss.item()}, Fairness Loss: {fairness_loss.item()}, Lambda: {lambda_param.item()}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPV13PBARHQ8"
      },
      "source": [
        "### Second model: GCN, standard data, fairness-aware loss (alpha=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "qtUJdNJ0-Il8"
      },
      "outputs": [],
      "source": [
        "# Instantiate the second model, define loss function and optimizer\n",
        "gcn_model2 = GCN(data)\n",
        "gcn_optimizer2 = torch.optim.Adam(gcn_model2.parameters(), lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rimgm5h2PyAk",
        "outputId": "b823d397-c987-41e7-bd02-b2e2beac33b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 | Loss: 0.6220347881317139\n",
            "Epoch 10 | Loss: 0.5531500577926636\n",
            "Epoch 20 | Loss: 0.5012115240097046\n",
            "Epoch 30 | Loss: 0.4466976225376129\n",
            "Epoch 40 | Loss: 0.4097009301185608\n"
          ]
        }
      ],
      "source": [
        "# Train the second model: GCN, standard data, fairness-aware loss (alpha=0.01)\n",
        "training(model=gcn_model2, data=data, optimizer=gcn_optimizer2, epochs=50, fairness=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vrC8bFFiO_3",
        "outputId": "b8a78898-b373-4897-9256-40e20acdc6fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here are the values for the GCN model with the standard dataset and fairness-entropy loss(alpha=0.01): \n",
            "\n",
            "Accuracy: 0.8378653569717297\n",
            "Statistical Parity Difference: 0.030072450637817383\n",
            "Equal Opportunity Difference: 0.06199532747268677\n",
            "Overall Accuracy Equality Difference: 0.08194565773010254\n",
            "Treatment Equality Difference: 2.69785213470459\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.8378653569717297,\n",
              " {'Statistical Parity Difference': tensor(0.0301),\n",
              "  'Equal Opportunity Difference': tensor(0.0620),\n",
              "  'Overall Accuracy Equality Difference': tensor(0.0819),\n",
              "  'Treatment Equality Difference': tensor(2.6979)})"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Test the second model: GCN, standard data, fairness-aware loss (alpha=0.01)\n",
        "print(\"Here are the values for the GCN model with the standard dataset and fairness-entropy loss(alpha=0.01): \")\n",
        "print()\n",
        "test(gcn_model2, data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJ6_kBZqbwbC"
      },
      "source": [
        "### Ignore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kl_IU4mwTTXA"
      },
      "source": [
        "Test to check if stronger fairness-constraint produces a better model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EojDlWZYRACM"
      },
      "outputs": [],
      "source": [
        "gcn_model3 = GCN(data)\n",
        "gcn_optimizer3 = torch.optim.Adam(gcn_model3.parameters(), lr=0.01)\n",
        "\n",
        "training(model=gcn_model3, data=data, optimizer=gcn_optimizer3, epochs=30, fairness=True, alpha=0.05)\n",
        "\n",
        "print(\"Here are the values for the GCN model with the standard dataset and fairness-entropy loss(alpha=0.05): \")\n",
        "print()\n",
        "test(gcn_model3, data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "je4EZYbMXY7z"
      },
      "outputs": [],
      "source": [
        "gcn_model4 = GCN(data)\n",
        "gcn_optimizer4 = torch.optim.Adam(gcn_model4.parameters(), lr=0.01)\n",
        "\n",
        "training(model=gcn_model4, data=data, optimizer=gcn_optimizer4, epochs=30, fairness=True, beta=0.005)\n",
        "# print(\"\\nHere are the values for the GCN model with the standard dataset and fairness-entropy loss(alpha=0.005): \")\n",
        "print()\n",
        "test(gcn_model4, data)\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5k3Ee_qlZ26o",
        "outputId": "006e3512-6351-4550-aa35-37f690159a6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Here are the values for the GCN model with the standard dataset and fairness-entropy loss(alpha=0.005): \n",
            "\n",
            "Accuracy: 0.847747963584092\n",
            "Statistical Parity Difference: 0.047549135982990265\n",
            "Equal Opportunity Difference: 0.053668081760406494\n",
            "Overall Accuracy Equality Difference: 0.08105164766311646\n",
            "Treatment Equality Difference: 2.612381935119629\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nHere are the values for the GCN model with the standard dataset and fairness-entropy loss(alpha=0.005): \")\n",
        "print()\n",
        "test(gcn_model4, data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IviJbqMF_iEH"
      },
      "source": [
        "### Third model: GCN, re-weighed data, weighted-cross-entropy loss (alpha=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Z2xjarJ4-FCA"
      },
      "outputs": [],
      "source": [
        "# Instantiate the third model, define loss function and optimizer\n",
        "rw_data_gcn_model = GCN(rw_data)\n",
        "rw_data_gcn_model_optimizer = torch.optim.Adam(rw_data_gcn_model.parameters(), lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlzNiV0aPwQ_",
        "outputId": "17802556-58f1-4612-bae0-c8b3716521f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 | Loss: 0.6312071681022644\n",
            "Epoch 10 | Loss: 0.5407678484916687\n",
            "Epoch 20 | Loss: 0.4680361747741699\n",
            "Epoch 30 | Loss: 0.4228784739971161\n",
            "Epoch 40 | Loss: 0.3963108956813812\n"
          ]
        }
      ],
      "source": [
        "# Train the third model: GCN, re-weighed data, weighted-cross entropy loss\n",
        "training(model=rw_data_gcn_model, data=rw_data, optimizer=rw_data_gcn_model_optimizer, epochs=50, weighted=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIuaKSRpluun",
        "outputId": "558e8eb6-cc59-45aa-cd86-bee6f3ebff9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here are the values for the GCN model with the re-weighed dataset and weighted-cross-entropy loss: \n",
            "\n",
            "Accuracy: 0.8576305701964543\n",
            "Statistical Parity Difference: 0.034996867179870605\n",
            "Equal Opportunity Difference: 0.1110086739063263\n",
            "Overall Accuracy Equality Difference: 0.08673566579818726\n",
            "Treatment Equality Difference: 0.6095795631408691\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test the third model: GCN, re-weighed data, weighted-cross entropy loss\n",
        "print(\"Here are the values for the GCN model with the re-weighed dataset and weighted-cross-entropy loss: \")\n",
        "print()\n",
        "test(rw_data_gcn_model, data)\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTRqpdjA-93u",
        "outputId": "c8c57ee5-7384-4974-8426-c17cd0337d66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here are the values for the GCN model with the re-weighed dataset and weighted-cross-entropy loss: \n",
            "\n",
            "Accuracy: 0.8468495448011499\n",
            "Statistical Parity Difference: 0.030316658318042755\n",
            "Equal Opportunity Difference: 0.10094630718231201\n",
            "Overall Accuracy Equality Difference: 0.08911752700805664\n",
            "Treatment Equality Difference: 0.06582164764404297\n"
          ]
        }
      ],
      "source": [
        "# Test the third model: GCN, re-weighed data, weighted-cross entropy loss\n",
        "print(\"Here are the values for the GCN model with the re-weighed dataset and weighted-cross-entropy loss: \")\n",
        "print()\n",
        "test(rw_data_gcn_model, data)\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pAVsiTD_jOV"
      },
      "source": [
        "### Fourth model: GCN, re-weighed data, weighted- and fairness-aware cross entropy loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "6EvCQZQ3-tGp"
      },
      "outputs": [],
      "source": [
        "# Instantiate the fourth model, define loss function and optimizer\n",
        "rw_data_gcn_model2 = GCN(rw_data)\n",
        "rw_data_gcn_model_optimizer2 = torch.optim.Adam(rw_data_gcn_model2.parameters(), lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DA7shFyP4Ke",
        "outputId": "28608ac8-b618-42d6-fa03-c6e2ef8f4fcd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 | Loss: 0.5843248963356018\n",
            "Epoch 10 | Loss: 0.5047746896743774\n",
            "Epoch 20 | Loss: 0.44967859983444214\n",
            "Epoch 30 | Loss: 0.4109334349632263\n",
            "Epoch 40 | Loss: 0.3880102038383484\n"
          ]
        }
      ],
      "source": [
        "# Train the fourth model: GCN, re-weighed data, weighted- and fairness-aware cross entropy loss\n",
        "training(model=rw_data_gcn_model2, data=rw_data, optimizer=rw_data_gcn_model_optimizer2, epochs=50, weighted=True, fairness=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7-ieFXrlzKH",
        "outputId": "3ba446fb-152f-48ee-b815-2942c8fbbeb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here are the values for the GCN model with the re-weighed dataset and weighted- and fairness-aware cross-entropy loss: \n",
            "\n",
            "Accuracy: 0.8555342597029229\n",
            "Statistical Parity Difference: 0.034720584750175476\n",
            "Equal Opportunity Difference: 0.1112838089466095\n",
            "Overall Accuracy Equality Difference: 0.08701056241989136\n",
            "Treatment Equality Difference: 0.6815862655639648\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test the fourth model: GCN, re-weighed data, weighted- and fairness-aware cross entropy loss\n",
        "print(\"Here are the values for the GCN model with the re-weighed dataset and weighted- and fairness-aware cross-entropy loss: \")\n",
        "print()\n",
        "test(rw_data_gcn_model2, data)\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEz_RIiZvJ2A",
        "outputId": "fabb14bd-ab62-4298-8b74-9e80de05020c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 | Loss: 0.745595395565033\n",
            "Epoch 10 | Loss: 0.5941913723945618\n",
            "Epoch 20 | Loss: 0.5534152984619141\n",
            "Epoch 30 | Loss: 0.5050016641616821\n",
            "Epoch 40 | Loss: 0.4620010554790497\n"
          ]
        }
      ],
      "source": [
        "# Train the fourth model: GCN, re-weighed data, weighted- and fairness-aware cross entropy loss\n",
        "training(model=rw_data_gcn_model2, data=rw_data, optimizer=rw_data_gcn_model_optimizer2, epochs=50, weighted=True,\n",
        "         fairness=True, alpha=0.01, beta=0.005, gamma=0.015, delta=0.012)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2rwS89lvZdT",
        "outputId": "60a1a3c2-fe89-4c69-80af-0465e0640db5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here are the values for the GCN model with the re-weighed dataset and weighted- and fairness-aware cross-entropy loss: \n",
            "\n",
            "Accuracy: 0.8442441303306181\n",
            "Statistical Parity Difference: 0.02938065677881241\n",
            "Equal Opportunity Difference: 0.09115374088287354\n",
            "Overall Accuracy Equality Difference: 0.08772581815719604\n",
            "Treatment Equality Difference: 1.3014488220214844\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test the fourth model: GCN, re-weighed data, weighted- and fairness-aware cross entropy loss\n",
        "print(\"Here are the values for the GCN model with the re-weighed dataset and weighted- and fairness-aware cross-entropy loss: \")\n",
        "print()\n",
        "test(rw_data_gcn_model2, data)\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWD3uICM0ACJ",
        "outputId": "f89506cd-5d19-4c56-cf22-37cbceee4acd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 | Loss: 0.7213656306266785\n",
            "Epoch 10 | Loss: 0.5258520841598511\n",
            "Epoch 20 | Loss: 0.4605921804904938\n",
            "Epoch 30 | Loss: 0.424991250038147\n",
            "Epoch 40 | Loss: 0.40024101734161377\n"
          ]
        }
      ],
      "source": [
        "# Train the fourth model: GCN, re-weighed data, weighted- and fairness-aware cross entropy loss\n",
        "training(model=rw_data_gcn_model2, data=rw_data, optimizer=rw_data_gcn_model_optimizer2, epochs=50, weighted=True,\n",
        "         fairness=True, alpha=0.01, beta=0.012, gamma=0.015, delta=0.015)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAADgkO40AlK",
        "outputId": "157d1064-a99a-4c34-f8b5-d03be9c079da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here are the values for the GCN model with the re-weighed dataset and weighted- and fairness-aware cross-entropy loss: \n",
            "\n",
            "Accuracy: 0.8580797795879253\n",
            "Statistical Parity Difference: 0.042138680815696716\n",
            "Equal Opportunity Difference: 0.0997222363948822\n",
            "Overall Accuracy Equality Difference: 0.0845213532447815\n",
            "Treatment Equality Difference: 0.3458895683288574\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test the fourth model: GCN, re-weighed data, weighted- and fairness-aware cross entropy loss\n",
        "print(\"Here are the values for the GCN model with the re-weighed dataset and weighted- and fairness-aware cross-entropy loss: \")\n",
        "print()\n",
        "test(rw_data_gcn_model2, data)\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_SXFTgY9BT-"
      },
      "outputs": [],
      "source": [
        "# Train the fourth model: GCN, re-weighed data, weighted- and fairness-aware cross entropy loss\n",
        "training(model=rw_data_gcn_model2, data=rw_data, optimizer=rw_data_gcn_model_optimizer2, epochs=50, weighted=True,\n",
        "         fairness=True, alpha=0.01, beta=0.012, gamma=0.02, delta=0.015)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9xn8zjj5AmA",
        "outputId": "23600151-bf9b-4275-c2ab-06bf931cd839"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here are the values for the GCN model with the re-weighed dataset and weighted- and fairness-aware cross-entropy loss: \n",
            "\n",
            "Accuracy: 0.8528689506468615\n",
            "Statistical Parity Difference: 0.04595881700515747\n",
            "Equal Opportunity Difference: 0.0724097192287445\n",
            "Overall Accuracy Equality Difference: 0.08293139934539795\n",
            "Treatment Equality Difference: 1.1784725189208984\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test the fourth model: GCN, re-weighed data, weighted- and fairness-aware cross entropy loss\n",
        "print(\"Here are the values for the GCN model with the re-weighed dataset and weighted- and fairness-aware cross-entropy loss: \")\n",
        "print()\n",
        "test(rw_data_gcn_model2, data)\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_Y5U_AT4t8y",
        "outputId": "49d26726-f7bf-43ba-9ee1-53346a08224f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 | Loss: 0.6623578667640686\n",
            "Epoch 10 | Loss: 0.5442702770233154\n",
            "Epoch 20 | Loss: 0.47677215933799744\n",
            "Epoch 30 | Loss: 0.42851555347442627\n",
            "Epoch 40 | Loss: 0.39934295415878296\n"
          ]
        }
      ],
      "source": [
        "# Train the fourth model: GCN, re-weighed data, weighted- and fairness-aware cross entropy loss\n",
        "training(model=rw_data_gcn_model2, data=rw_data, optimizer=rw_data_gcn_model_optimizer2, epochs=50, weighted=True,\n",
        "         fairness=True, alpha=0.01, beta=0.012, gamma=0.018, delta=0.018)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXttj2T-899t",
        "outputId": "033b1ad4-1c80-4c1a-d279-f28ed26c5719"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here are the values for the GCN model with the re-weighed dataset and weighted- and fairness-aware cross-entropy loss: \n",
            "\n",
            "Accuracy: 0.8569717297556301\n",
            "Statistical Parity Difference: 0.03602193295955658\n",
            "Equal Opportunity Difference: 0.11044105887413025\n",
            "Overall Accuracy Equality Difference: 0.08629560470581055\n",
            "Treatment Equality Difference: 0.6496486663818359\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test the fourth model: GCN, re-weighed data, weighted- and fairness-aware cross entropy loss\n",
        "print(\"Here are the values for the GCN model with the re-weighed dataset and weighted- and fairness-aware cross-entropy loss: \")\n",
        "print()\n",
        "test(rw_data_gcn_model2, data)\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zlTkMUwDiDP"
      },
      "source": [
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "# Extras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvho1NeAdEFt"
      },
      "source": [
        "\n",
        "### DI Remover"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WeTriESBdJcl"
      },
      "outputs": [],
      "source": [
        "# Convert your data to a format suitable for AIF360\n",
        "dataset = BinaryLabelDataset(df=user_labels, label_names=['gender'], protected_attribute_names=['bin_age'])\n",
        "\n",
        "# Apply the Disparate Impact Remover\n",
        "DIR = DisparateImpactRemover(repair_level=1.0)\n",
        "dataset_transf = DIR.fit_transform(dataset)\n",
        "\n",
        "# Extract the transformed features\n",
        "transformed_features = dataset_transf.features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6x3GrdbdRIP"
      },
      "outputs": [],
      "source": [
        "rw_data.x = torch.tensor(transformed_features, dtype=torch.float)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJ-BAyYv-J8h"
      },
      "source": [
        "### DI GCN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pq2QOAU7eBLE"
      },
      "outputs": [],
      "source": [
        "class GCN2(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GCN2, self).__init__()\n",
        "        self.conv1 = GCNConv(rw_data.num_node_features, 16)\n",
        "        self.conv2 = GCNConv(16, 2)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HIbrQ6fdnJP"
      },
      "outputs": [],
      "source": [
        "# Instantiate the model, define loss function and optimizer\n",
        "gcn_model = GCN(rw_data)\n",
        "gcn_optimizer = torch.optim.Adam(gcn_model.parameters(), lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "CGFVV1dCdsU0",
        "outputId": "b0ef8a70-25b8-46a1-d473-3b642745d17f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 | Loss: 2247.96337890625\n",
            "Epoch 10 | Loss: 1941.6514892578125\n",
            "Epoch 20 | Loss: 1676.3948974609375\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-d50c80bfdf74>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgcn_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrw_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgcn_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-9cd473488045>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(model, data, optimizer, epochs, variant, alpha)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "training(model=gcn_model, data=rw_data, optimizer=gcn_optimizer, epochs=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BRhQ_ZQeJyD"
      },
      "outputs": [],
      "source": [
        "test(gcn_model, rw_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbQBmxjN1pqw"
      },
      "source": [
        "### Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y16OLuPEZvKK"
      },
      "outputs": [],
      "source": [
        "def fairness_aware_loss(output, data, sensitive_attr, weighted=False, alpha=0.01):\n",
        "    \"\"\"\n",
        "    A custom loss function to calculate a fairness-aware loss.\n",
        "    The fairness-factor measures the disparity in predictions between +ve and -ve sensitive attribute group.\n",
        "\n",
        "    Args:\n",
        "    output: Outputs from the model.\n",
        "    data: The torch-geometric data object used for the model.\n",
        "    sensitive_attr: The sensitive attribute in the data (in our case: bin_age)\n",
        "    weighted: Boolean value indicating re-weighing done to the data or not.\n",
        "    alpha: Parameter to control the strength of the fairness regularizer.\n",
        "\n",
        "    Returns:\n",
        "    A fairness-aware combined loss.\n",
        "    \"\"\"\n",
        "    if weighted:\n",
        "        # Call the weighted-cross entropy loss\n",
        "        standard_loss = weighted_cross_entropy(output, data)\n",
        "    else:\n",
        "        # Call standard cross-entropy loss\n",
        "        target = data.y[data.train_mask]\n",
        "        standard_loss = F.cross_entropy(output, target)\n",
        "\n",
        "    pos_prob = torch.sigmoid(output[:, 1])\n",
        "\n",
        "    fairness_reg = torch.abs(pos_prob[sensitive_attr == 1].mean() - pos_prob[sensitive_attr == 0].mean())\n",
        "    combined_loss = standard_loss + alpha * fairness_reg\n",
        "\n",
        "    return combined_loss"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "q_AV3JhlZf00",
        "VJBgMHhB0W-I",
        "Nlx4iq4S0auM",
        "k3Y-A-g50evv",
        "0oTGYnLz0nqq",
        "71oC_W9UZiND",
        "oVZzUZlfbSui",
        "eYc3PvX715v3",
        "bIXj8cPkMa_g",
        "98ygJtxSMfLr",
        "hJQ7eKEtMj8y",
        "n1BLAdefMmlB",
        "yi1s71rrDQH9",
        "LJJv17nM1xsH",
        "p8DljQFY98sL",
        "YPV13PBARHQ8",
        "hJ6_kBZqbwbC",
        "IviJbqMF_iEH",
        "3pAVsiTD_jOV",
        "1zlTkMUwDiDP",
        "rvho1NeAdEFt",
        "pJ-BAyYv-J8h",
        "PbQBmxjN1pqw"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
