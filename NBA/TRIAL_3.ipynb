{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Necessities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# Imports\n",
    "# ------------------------------------------------------------------\n",
    "# Basic data processing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Graph data processing libraries\n",
    "import networkx as nx\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import from_networkx\n",
    "\n",
    "# Libraries for (G)NNs\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "import torch.nn as nn\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Helper functions\n",
    "# ------------------------------------------------------------------\n",
    "def show_df_info(df):\n",
    "    print(df.info())\n",
    "    print('####### Repeat ####### \\n', df.duplicated().any())\n",
    "    print('####### Count ####### \\n', df.nunique())\n",
    "    print('####### Example ####### \\n',df.head())\n",
    "\n",
    "def label_statics(label_df, label_list):\n",
    "    print(\"####### nCount #######\")\n",
    "    for label in label_list:\n",
    "        print(label_df[label].value_counts())\n",
    "    print(\"####### nPercent #######\")\n",
    "    for label in label_list:\n",
    "        print(label_df[label].value_counts()/label_df.shape[0])\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Data stuff\n",
    "# ------------------------------------------------------------------\n",
    "base_path = os.getcwd()\n",
    "input_ali_data_path = base_path\n",
    "\n",
    "# Load the data files\n",
    "user_labels_path = os.path.join(input_ali_data_path, \"nba.csv\")\n",
    "user_edges_path = os.path.join(input_ali_data_path, \"nba_relationship.csv\")\n",
    "\n",
    "# Create dataframes to store the information from the .csv files\n",
    "user_labels = pd.read_csv(user_labels_path)\n",
    "user_edges = pd.read_csv(user_edges_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid1</th>\n",
       "      <th>uid2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55371339</td>\n",
       "      <td>188621689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>430230487</td>\n",
       "      <td>92337777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>164109237</td>\n",
       "      <td>725731259612196864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>164109237</td>\n",
       "      <td>3181228674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>164109237</td>\n",
       "      <td>184139015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16565</th>\n",
       "      <td>95649322</td>\n",
       "      <td>41861751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16566</th>\n",
       "      <td>95649322</td>\n",
       "      <td>153307097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16567</th>\n",
       "      <td>95649322</td>\n",
       "      <td>272116860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16568</th>\n",
       "      <td>95649322</td>\n",
       "      <td>70926844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16569</th>\n",
       "      <td>95649322</td>\n",
       "      <td>150367583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16570 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            uid1                uid2\n",
       "0       55371339           188621689\n",
       "1      430230487            92337777\n",
       "2      164109237  725731259612196864\n",
       "3      164109237          3181228674\n",
       "4      164109237           184139015\n",
       "...          ...                 ...\n",
       "16565   95649322            41861751\n",
       "16566   95649322           153307097\n",
       "16567   95649322           272116860\n",
       "16568   95649322            70926844\n",
       "16569   95649322           150367583\n",
       "\n",
       "[16570 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>SALARY</th>\n",
       "      <th>AGE</th>\n",
       "      <th>MP</th>\n",
       "      <th>FG</th>\n",
       "      <th>FGA</th>\n",
       "      <th>FG%</th>\n",
       "      <th>3P</th>\n",
       "      <th>3PA</th>\n",
       "      <th>3P%</th>\n",
       "      <th>...</th>\n",
       "      <th>ORL/TOR</th>\n",
       "      <th>PHI</th>\n",
       "      <th>PHI/OKC</th>\n",
       "      <th>PHX</th>\n",
       "      <th>POR</th>\n",
       "      <th>SA</th>\n",
       "      <th>SAC</th>\n",
       "      <th>TOR</th>\n",
       "      <th>UTAH</th>\n",
       "      <th>WSH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>105305397</td>\n",
       "      <td>-1</td>\n",
       "      <td>25</td>\n",
       "      <td>14.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>5.6</td>\n",
       "      <td>0.487</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.374</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49680175</td>\n",
       "      <td>-1</td>\n",
       "      <td>32</td>\n",
       "      <td>4.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.383</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.400</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>364013199</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>20.1</td>\n",
       "      <td>2.4</td>\n",
       "      <td>5.9</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.321</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>234811698</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>4.8</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.337</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1031967637561954304</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>8.4</td>\n",
       "      <td>2.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>24342206</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>18.7</td>\n",
       "      <td>2.5</td>\n",
       "      <td>6.4</td>\n",
       "      <td>0.390</td>\n",
       "      <td>1.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.392</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>270689028</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>34.7</td>\n",
       "      <td>8.1</td>\n",
       "      <td>18.3</td>\n",
       "      <td>0.444</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.6</td>\n",
       "      <td>0.399</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>47218790</td>\n",
       "      <td>-1</td>\n",
       "      <td>29</td>\n",
       "      <td>25.1</td>\n",
       "      <td>3.4</td>\n",
       "      <td>7.6</td>\n",
       "      <td>0.454</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.288</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>42562446</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>33.4</td>\n",
       "      <td>8.5</td>\n",
       "      <td>18.3</td>\n",
       "      <td>0.468</td>\n",
       "      <td>4.1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.411</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>473227811</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3.7</td>\n",
       "      <td>8.9</td>\n",
       "      <td>0.413</td>\n",
       "      <td>1.8</td>\n",
       "      <td>5.4</td>\n",
       "      <td>0.342</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>403 rows × 98 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 user_id  SALARY  AGE    MP   FG   FGA    FG%   3P   3PA  \\\n",
       "0              105305397      -1   25  14.8  2.7   5.6  0.487  0.8   2.1   \n",
       "1               49680175      -1   32   4.9  0.9   2.4  0.383  0.6   1.4   \n",
       "2              364013199       0   20  20.1  2.4   5.9  0.399  0.6   2.0   \n",
       "3              234811698       0   25  14.0  2.1   4.8  0.440  0.5   1.4   \n",
       "4    1031967637561954304       1   28   8.4  2.1   3.8  0.545  0.0   0.0   \n",
       "..                   ...     ...  ...   ...  ...   ...    ...  ...   ...   \n",
       "398             24342206       1   39  18.7  2.5   6.4  0.390  1.3   3.3   \n",
       "399            270689028       1   26  34.7  8.1  18.3  0.444  3.0   7.6   \n",
       "400             47218790      -1   29  25.1  3.4   7.6  0.454  0.5   1.7   \n",
       "401             42562446       1   28  33.4  8.5  18.3  0.468  4.1  10.0   \n",
       "402            473227811       1   25  24.0  3.7   8.9  0.413  1.8   5.4   \n",
       "\n",
       "       3P%  ...  ORL/TOR  PHI  PHI/OKC  PHX  POR  SA  SAC  TOR  UTAH  WSH  \n",
       "0    0.374  ...        0    0        0    0    0   0    0    0     0    0  \n",
       "1    0.400  ...        0    0        0    0    0   0    0    0     0    0  \n",
       "2    0.321  ...        0    0        0    0    0   0    0    0     0    0  \n",
       "3    0.337  ...        0    0        0    0    0   0    0    0     0    0  \n",
       "4    0.000  ...        0    0        0    0    0   0    0    0     0    0  \n",
       "..     ...  ...      ...  ...      ...  ...  ...  ..  ...  ...   ...  ...  \n",
       "398  0.392  ...        0    0        0    0    0   1    0    0     0    0  \n",
       "399  0.399  ...        0    0        0    0    0   0    0    0     0    0  \n",
       "400  0.288  ...        0    0        0    0    0   0    1    0     0    0  \n",
       "401  0.411  ...        0    0        0    0    0   0    0    0     0    0  \n",
       "402  0.342  ...        0    0        0    0    0   0    0    0     0    0  \n",
       "\n",
       "[403 rows x 98 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 403 entries, 0 to 402\n",
      "Data columns (total 98 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   user_id        403 non-null    int64  \n",
      " 1   SALARY         403 non-null    int64  \n",
      " 2   AGE            403 non-null    int64  \n",
      " 3   MP             403 non-null    float64\n",
      " 4   FG             403 non-null    float64\n",
      " 5   FGA            403 non-null    float64\n",
      " 6   FG%            403 non-null    float64\n",
      " 7   3P             403 non-null    float64\n",
      " 8   3PA            403 non-null    float64\n",
      " 9   3P%            403 non-null    float64\n",
      " 10  2P             403 non-null    float64\n",
      " 11  2PA            403 non-null    float64\n",
      " 12  2P%            403 non-null    float64\n",
      " 13  eFG%           403 non-null    float64\n",
      " 14  FT             403 non-null    float64\n",
      " 15  FTA            403 non-null    float64\n",
      " 16  FT%            403 non-null    float64\n",
      " 17  ORB            403 non-null    float64\n",
      " 18  DRB            403 non-null    float64\n",
      " 19  TRB            403 non-null    float64\n",
      " 20  AST            403 non-null    float64\n",
      " 21  STL            403 non-null    float64\n",
      " 22  BLK            403 non-null    float64\n",
      " 23  TOV            403 non-null    float64\n",
      " 24  PF_x           403 non-null    float64\n",
      " 25  POINTS         403 non-null    float64\n",
      " 26  GP             403 non-null    int64  \n",
      " 27  MPG            403 non-null    float64\n",
      " 28  ORPM           403 non-null    float64\n",
      " 29  DRPM           403 non-null    float64\n",
      " 30  RPM            403 non-null    float64\n",
      " 31  WINS_RPM       403 non-null    float64\n",
      " 32  PIE            403 non-null    float64\n",
      " 33  PACE           403 non-null    float64\n",
      " 34  W              403 non-null    int64  \n",
      " 35  player_height  403 non-null    float64\n",
      " 36  player_weight  403 non-null    float64\n",
      " 37  country        403 non-null    int64  \n",
      " 38  C              403 non-null    int64  \n",
      " 39  PF_y           403 non-null    int64  \n",
      " 40  PF-C           403 non-null    int64  \n",
      " 41  PG             403 non-null    int64  \n",
      " 42  SF             403 non-null    int64  \n",
      " 43  SG             403 non-null    int64  \n",
      " 44  ATL            403 non-null    int64  \n",
      " 45  ATL/CLE        403 non-null    int64  \n",
      " 46  ATL/LAL        403 non-null    int64  \n",
      " 47  BKN            403 non-null    int64  \n",
      " 48  BKN/WSH        403 non-null    int64  \n",
      " 49  BOS            403 non-null    int64  \n",
      " 50  CHA            403 non-null    int64  \n",
      " 51  CHI            403 non-null    int64  \n",
      " 52  CHI/OKC        403 non-null    int64  \n",
      " 53  CLE            403 non-null    int64  \n",
      " 54  CLE/DAL        403 non-null    int64  \n",
      " 55  CLE/MIA        403 non-null    int64  \n",
      " 56  DAL            403 non-null    int64  \n",
      " 57  DAL/BKN        403 non-null    int64  \n",
      " 58  DAL/PHI        403 non-null    int64  \n",
      " 59  DEN            403 non-null    int64  \n",
      " 60  DEN/CHA        403 non-null    int64  \n",
      " 61  DEN/POR        403 non-null    int64  \n",
      " 62  DET            403 non-null    int64  \n",
      " 63  GS             403 non-null    int64  \n",
      " 64  GS/CHA         403 non-null    int64  \n",
      " 65  GS/SAC         403 non-null    int64  \n",
      " 66  HOU            403 non-null    int64  \n",
      " 67  HOU/LAL        403 non-null    int64  \n",
      " 68  HOU/MEM        403 non-null    int64  \n",
      " 69  IND            403 non-null    int64  \n",
      " 70  LAC            403 non-null    int64  \n",
      " 71  LAL            403 non-null    int64  \n",
      " 72  MEM            403 non-null    int64  \n",
      " 73  MIA            403 non-null    int64  \n",
      " 74  MIL            403 non-null    int64  \n",
      " 75  MIL/CHA        403 non-null    int64  \n",
      " 76  MIN            403 non-null    int64  \n",
      " 77  NO             403 non-null    int64  \n",
      " 78  NO/DAL         403 non-null    int64  \n",
      " 79  NO/MEM         403 non-null    int64  \n",
      " 80  NO/MIL         403 non-null    int64  \n",
      " 81  NO/MIN/SAC     403 non-null    int64  \n",
      " 82  NO/ORL         403 non-null    int64  \n",
      " 83  NO/SAC         403 non-null    int64  \n",
      " 84  NY             403 non-null    int64  \n",
      " 85  NY/PHI         403 non-null    int64  \n",
      " 86  OKC            403 non-null    int64  \n",
      " 87  ORL            403 non-null    int64  \n",
      " 88  ORL/TOR        403 non-null    int64  \n",
      " 89  PHI            403 non-null    int64  \n",
      " 90  PHI/OKC        403 non-null    int64  \n",
      " 91  PHX            403 non-null    int64  \n",
      " 92  POR            403 non-null    int64  \n",
      " 93  SA             403 non-null    int64  \n",
      " 94  SAC            403 non-null    int64  \n",
      " 95  TOR            403 non-null    int64  \n",
      " 96  UTAH           403 non-null    int64  \n",
      " 97  WSH            403 non-null    int64  \n",
      "dtypes: float64(32), int64(66)\n",
      "memory usage: 308.7 KB\n",
      "None\n",
      "####### Repeat ####### \n",
      " False\n",
      "####### Count ####### \n",
      " user_id    403\n",
      "SALARY       3\n",
      "AGE         21\n",
      "MP         228\n",
      "FG          87\n",
      "          ... \n",
      "SA           2\n",
      "SAC          2\n",
      "TOR          2\n",
      "UTAH         2\n",
      "WSH          2\n",
      "Length: 98, dtype: int64\n",
      "####### Example ####### \n",
      "                user_id  SALARY  AGE    MP   FG  FGA    FG%   3P  3PA    3P%  \\\n",
      "0            105305397      -1   25  14.8  2.7  5.6  0.487  0.8  2.1  0.374   \n",
      "1             49680175      -1   32   4.9  0.9  2.4  0.383  0.6  1.4  0.400   \n",
      "2            364013199       0   20  20.1  2.4  5.9  0.399  0.6  2.0  0.321   \n",
      "3            234811698       0   25  14.0  2.1  4.8  0.440  0.5  1.4  0.337   \n",
      "4  1031967637561954304       1   28   8.4  2.1  3.8  0.545  0.0  0.0  0.000   \n",
      "\n",
      "   ...  ORL/TOR  PHI  PHI/OKC  PHX  POR  SA  SAC  TOR  UTAH  WSH  \n",
      "0  ...        0    0        0    0    0   0    0    0     0    0  \n",
      "1  ...        0    0        0    0    0   0    0    0     0    0  \n",
      "2  ...        0    0        0    0    0   0    0    0     0    0  \n",
      "3  ...        0    0        0    0    0   0    0    0     0    0  \n",
      "4  ...        0    0        0    0    0   0    0    0     0    0  \n",
      "\n",
      "[5 rows x 98 columns]\n"
     ]
    }
   ],
   "source": [
    "show_df_info(user_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### nCount #######\n",
      "0    296\n",
      "1    107\n",
      "Name: country, dtype: int64\n",
      " 1    159\n",
      " 0    154\n",
      "-1     90\n",
      "Name: SALARY, dtype: int64\n",
      "####### nPercent #######\n",
      "0    0.734491\n",
      "1    0.265509\n",
      "Name: country, dtype: float64\n",
      " 1    0.394541\n",
      " 0    0.382134\n",
      "-1    0.223325\n",
      "Name: SALARY, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "label_statics(user_labels, ['country', 'SALARY'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continue Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(            uid1                uid2\n",
       " 0       55371339           188621689\n",
       " 1      430230487            92337777\n",
       " 2      164109237  725731259612196864\n",
       " 3      164109237          3181228674\n",
       " 4      164109237           184139015\n",
       " ...          ...                 ...\n",
       " 16565   95649322            41861751\n",
       " 16566   95649322           153307097\n",
       " 16567   95649322           272116860\n",
       " 16568   95649322            70926844\n",
       " 16569   95649322           150367583\n",
       " \n",
       " [16570 rows x 2 columns],\n",
       " (16570, 2))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_edges = user_edges[user_edges['uid1'].isin(user_labels['user_id']) & user_edges['uid2'].isin(user_labels['user_id'])]\n",
    "user_edges, user_edges.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract node features from user_labels dataframe\n",
    "node_features = user_labels.iloc[:, 1:] # Replace 'attribute1', 'attribute2', ... with the actual attribute columns you want to use\n",
    "node_features = torch.tensor(node_features.values, dtype=torch.float)\n",
    "\n",
    "# Extract edges from user_edges dataframe\n",
    "edges = user_edges[['uid1', 'uid2']]\n",
    "edges['uid1'] = edges['uid1'].map(dict(zip(user_labels['user_id'], range(len(user_labels)))))\n",
    "edges['uid2'] = edges['uid2'].map(dict(zip(user_labels['user_id'], range(len(user_labels)))))\n",
    "\n",
    "# Convert edges dataframe to tensor\n",
    "edges_tensor = torch.tensor(edges.values, dtype=torch.long).t().contiguous()\n",
    "\n",
    "# Create edge_index tensor\n",
    "edge_index = edges_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_labels['SALARY'] = user_labels['SALARY'].map({-1: 0, 0: 1, 1: 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for GNNs\n",
    "# node_features = torch.tensor(user_labels.iloc[:, 1:].values, dtype=torch.float)\n",
    "# edge_index = torch.tensor(user_edges.values, dtype=torch.long).t().contiguous()\n",
    "\n",
    "# node_features = torch.tensor(filtered_user_labels.iloc[:, 1:].values, dtype=torch.float)\n",
    "# edge_index = torch.tensor(filtered_edges.values, dtype=torch.long).t().contiguous()\n",
    "\n",
    "# Create torch-geometric data\n",
    "data = Data(x=node_features, edge_index=edge_index)\n",
    "\n",
    "num_nodes = node_features.size(0)\n",
    "num_classes = 2 \n",
    "num_node_features = data.num_node_features\n",
    "\n",
    "# Create masks for training, and testing\n",
    "train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "\n",
    "# 80 - 20 Train and Test data split\n",
    "num_train = int(num_nodes * 0.8)\n",
    "train_mask[:num_train] = True\n",
    "test_mask[num_train:] = True\n",
    "\n",
    "data.train_mask = train_mask\n",
    "data.test_mask = test_mask\n",
    "\n",
    "# Labels from the data (in this case: Job Classification)\n",
    "data.y = torch.tensor(user_labels['SALARY'].values, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### nCount #######\n",
      "0    296\n",
      "1    107\n",
      "Name: country, dtype: int64\n",
      "1    313\n",
      "0     90\n",
      "Name: SALARY, dtype: int64\n",
      "####### nPercent #######\n",
      "0    0.734491\n",
      "1    0.265509\n",
      "Name: country, dtype: float64\n",
      "1    0.776675\n",
      "0    0.223325\n",
      "Name: SALARY, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "label_statics(user_labels, ['country', 'SALARY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([403, 97]), torch.Size([2, 16570]), torch.Size([403]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_features.shape, edge_index.shape, data.y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[403, 97], edge_index=[2, 16570], train_mask=[403], test_mask=[403], y=[403]) 403 torch.Size([403]) torch.Size([403])\n"
     ]
    }
   ],
   "source": [
    "print(data, num_nodes, train_mask.size(), test_mask.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([403, 97]), torch.Size([403]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.x.shape, data.y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fairness_aware_loss(output, data, sensitive_attr, alpha=0, beta=0, gamma=0, delta=0):\n",
    "    target = data.y[data.train_mask]\n",
    "    standard_loss = F.cross_entropy(output, target)\n",
    "\n",
    "    labels = data.y[train_mask]\n",
    "    pos_prob = torch.sigmoid(output[:, 1])\n",
    "    neg_prob = 1 - pos_prob\n",
    "    predictions = output.argmax(dim=1)\n",
    "\n",
    "    # Statistical Parity Regularization\n",
    "    sp_reg = torch.abs(pos_prob[sensitive_attr == 1].mean() - pos_prob[sensitive_attr == 0].mean())\n",
    "\n",
    "    # Calculating FPR and TPR for each group\n",
    "    fpr_group1 = ((predictions == 1) & (labels == 0) & (sensitive_attr == 1)).float().mean()\n",
    "    fpr_group0 = ((predictions == 1) & (labels == 0) & (sensitive_attr == 0)).float().mean()\n",
    "    tpr_group1 = ((predictions == 1) & (labels == 1) & (sensitive_attr == 1)).float().mean()\n",
    "    tpr_group0 = ((predictions == 1) & (labels == 1) & (sensitive_attr == 0)).float().mean()\n",
    "\n",
    "    # Difference in FPR and TPR between the two groups for Equalized Odds\n",
    "    fpr_diff = torch.abs(fpr_group1 - fpr_group0)\n",
    "    tpr_diff = torch.abs(tpr_group1 - tpr_group0)\n",
    "\n",
    "    # Combine FPR and TPR differences for Equalized Odds Regularization\n",
    "    equalized_odds_reg = fpr_diff + tpr_diff\n",
    "\n",
    "    # Treatment Equality Regularization\n",
    "    fp_diff = (neg_prob * (labels == 0) * (sensitive_attr == 1)).float().mean() - \\\n",
    "              (neg_prob * (labels == 0) * (sensitive_attr == 0)).float().mean()\n",
    "    fn_diff = (pos_prob * (labels == 1) * (sensitive_attr == 1)).float().mean() - \\\n",
    "              (pos_prob * (labels == 1) * (sensitive_attr == 0)).float().mean()\n",
    "    treatment_reg = torch.abs(fp_diff) + torch.abs(fn_diff)\n",
    "    # treatment_reg = torch.abs(fn_diff)\n",
    "\n",
    "    # fn_group_1 = ((predictions == 0) & (labels == 1) & (sensitive_attr == 1)).sum()\n",
    "    # fp_group_1 = ((predictions == 1) & (labels == 0) & (sensitive_attr == 1)).sum()\n",
    "\n",
    "    # fn_group_0 = ((predictions == 0) & (labels == 1) & (sensitive_attr == 0)).sum()\n",
    "    # fp_group_0 = ((predictions == 1) & (labels == 0) & (sensitive_attr == 0)).sum()\n",
    "    \n",
    "    # ratio_group_1 = fn_group_1 / fp_group_1 if fp_group_1 != 0 else torch.tensor(float('inf'))\n",
    "    # ratio_group_0 = fn_group_0 / fp_group_0 if fp_group_0 != 0 else torch.tensor(float('inf'))\n",
    "    # treatment_reg = torch.abs(ratio_group_1 - ratio_group_0)\n",
    "\n",
    "    # Equal Opportunity Difference Regularization\n",
    "    eod_reg = torch.abs((pos_prob * (labels == 1) * (sensitive_attr == 1)).float().mean() - \\\n",
    "                        (pos_prob * (labels == 1) * (sensitive_attr == 0)).float().mean())\n",
    "\n",
    "    # Overall Accuracy Equality Difference Regularization\n",
    "    oaed_reg = torch.abs((pos_prob * (sensitive_attr == 1)).float().mean() - \\\n",
    "                         (pos_prob * (sensitive_attr == 0)).float().mean())\n",
    "\n",
    "    penalty = alpha + beta + gamma + delta\n",
    "    \n",
    "    # Combine losses\n",
    "    combined_loss = (1-penalty)*standard_loss\n",
    "    + alpha * equalized_odds_reg\n",
    "    + beta * treatment_reg\n",
    "    + gamma * eod_reg\n",
    "    + delta * oaed_reg\n",
    "    \n",
    "    return combined_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fairness(label, predictions, sens_attr='country', balanced=False):\n",
    "    \"\"\"\n",
    "    Calculate various fairness metrics.\n",
    "\n",
    "    Args:\n",
    "    label: Actual labels (binary).\n",
    "    predictions: Model predictions (binary).\n",
    "    sens_attr: Binary sensitive attribute for fairness evaluation.\n",
    "\n",
    "    Returns:\n",
    "    A dictionary containing SPD, EOD, OAED, and TED values.\n",
    "    \"\"\"\n",
    "    if balanced is False:\n",
    "        labels = torch.tensor(user_labels[label].values, dtype=torch.long)\n",
    "        sensitive_attribute = torch.tensor(user_labels[sens_attr].values, dtype=torch.long)\n",
    "    else:\n",
    "        labels = torch.tensor(filtered_user_labels[label].values, dtype=torch.long)\n",
    "        sensitive_attribute = torch.tensor(filtered_user_labels[sens_attr].values, dtype=torch.long)\n",
    "    \n",
    "    predictions = predictions.float()\n",
    "    labels = labels.float()\n",
    "    sensitive_attribute = sensitive_attribute.float()\n",
    "\n",
    "    def statistical_parity_difference():\n",
    "        prob_group_1 = predictions[sensitive_attribute == 1].mean()\n",
    "        prob_group_0 = predictions[sensitive_attribute == 0].mean()\n",
    "        return abs(prob_group_1 - prob_group_0), prob_group_0, prob_group_1\n",
    "\n",
    "    def equal_opportunity_difference():\n",
    "        tpr_group_1 = predictions[(labels == 1) & (sensitive_attribute == 1)].mean()\n",
    "        tpr_group_0 = predictions[(labels == 1) & (sensitive_attribute == 0)].mean()\n",
    "        return abs(tpr_group_1 - tpr_group_0), tpr_group_0, tpr_group_1\n",
    "\n",
    "    def overall_accuracy_equality_difference():\n",
    "        acc_group_1 = (predictions[sensitive_attribute == 1] == labels[sensitive_attribute == 1]).float().mean()\n",
    "        acc_group_0 = (predictions[sensitive_attribute == 0] == labels[sensitive_attribute == 0]).float().mean()\n",
    "        return abs(acc_group_1 - acc_group_0), acc_group_0, acc_group_1\n",
    "\n",
    "    def treatment_equality_difference():\n",
    "        fn_group_1 = ((predictions == 0) & (labels == 1) & (sensitive_attribute == 1)).sum()\n",
    "        fp_group_1 = ((predictions == 1) & (labels == 0) & (sensitive_attribute == 1)).sum()\n",
    "\n",
    "        fn_group_0 = ((predictions == 0) & (labels == 1) & (sensitive_attribute == 0)).sum()\n",
    "        fp_group_0 = ((predictions == 1) & (labels == 0) & (sensitive_attribute == 0)).sum()\n",
    "\n",
    "        ratio_group_1 = fn_group_1 / fp_group_1 if fp_group_1 != 0 else float('inf')\n",
    "        ratio_group_0 = fn_group_0 / fp_group_0 if fp_group_0 != 0 else float('inf')\n",
    "\n",
    "        return abs(ratio_group_1 - ratio_group_0), ratio_group_0, ratio_group_1, fn_group_1, fp_group_1, fn_group_0, fp_group_0\n",
    "\n",
    "    # Calculating each fairness metric\n",
    "    spd, sp_g0, sp_g1 = statistical_parity_difference()\n",
    "    eod, eod_g0, eod_g1 = equal_opportunity_difference()\n",
    "    oaed, oaed_g0, oaed_g1 = overall_accuracy_equality_difference()\n",
    "    ted, ted_g0, ted_g1, fn_group_1, fp_group_1, fn_group_0, fp_group_0 = treatment_equality_difference()\n",
    "\n",
    "    return {\n",
    "        'Statistical Parity Difference': spd,\n",
    "        'Statistical Parity Group with S=0': sp_g0,\n",
    "        'Statistical Parity Group S=1': sp_g1,\n",
    "        'Equal Opportunity Difference': eod,\n",
    "        'Equal Opportunity Group with S=0': eod_g0,\n",
    "        'Equal Opportunity Group S=1': eod_g1,\n",
    "        'Overall Accuracy Equality Difference': oaed,\n",
    "        'Overall Accuracy Group with S=0': oaed_g0,\n",
    "        'Overall Accuracy Group S=1': oaed_g1,\n",
    "        'Treatment Equality Difference': ted,\n",
    "        'Treatment Equality Group with S=0': ted_g0,\n",
    "        'Treatment Equality Group S=1': ted_g1,\n",
    "        'False Negatives Group 1': fn_group_1,\n",
    "        'False Positives Group 1': fp_group_1,\n",
    "        'False Negatives Group 0': fn_group_0,\n",
    "        'False Positives Group 0': fp_group_0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fairness(label, predictions, sens_attr='country', balanced=False):\n",
    "    \n",
    "    labels = user_labels[label].values\n",
    "    labels = labels.astype(float)\n",
    "\n",
    "    predictions = predictions.int()\n",
    "    # predictions = predictions.float()\n",
    "\n",
    "    sens_attr_values = user_labels[sens_attr].values\n",
    "    sens_attr_values = sens_attr_values.astype(float)\n",
    "    # labels = label.int()\n",
    "    \n",
    "    # Sensitivity attributes, assuming binary where 0 is unprivileged and 1 is privileged\n",
    "    # sens_attr_values = user_labels[sens_attr].int()\n",
    "    \n",
    "    # Indices for privileged and unprivileged groups\n",
    "    privileged_indices = sens_attr_values == 1\n",
    "    unprivileged_indices = sens_attr_values == 0\n",
    "    \n",
    "    # Calculating metrics for both groups\n",
    "    def calc_metrics(preds, lbls):\n",
    "        tp = ((preds == 1) & (lbls == 1)).sum().item()\n",
    "        tn = ((preds == 0) & (lbls == 0)).sum().item()\n",
    "        fp = ((preds == 1) & (lbls == 0)).sum().item()\n",
    "        fn = ((preds == 0) & (lbls == 1)).sum().item()\n",
    "        \n",
    "        accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "        tpr = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "        fpr = fp / (fp + tn) if (fp + tn) != 0 else 0\n",
    "        \n",
    "        return accuracy, tpr, fpr, fp, fn\n",
    "    \n",
    "    # Metrics for unprivileged\n",
    "    acc_unpriv, tpr_unpriv, _, fp_unpriv, fn_unpriv = calc_metrics(predictions[unprivileged_indices], labels[unprivileged_indices])\n",
    "    # Metrics for privileged\n",
    "    acc_priv, tpr_priv, _, fp_priv, fn_priv = calc_metrics(predictions[privileged_indices], labels[privileged_indices])\n",
    "    \n",
    "    # Calculating fairness metrics\n",
    "    spd = (predictions[privileged_indices].mean() - predictions[unprivileged_indices].mean()).item()\n",
    "    oaed = acc_priv - acc_unpriv\n",
    "    eod = tpr_priv - tpr_unpriv\n",
    "    ted = (fn_unpriv - fn_priv) / (fp_unpriv - fp_priv) if (fp_unpriv - fp_priv) != 0 else float('inf')\n",
    "    \n",
    "    return {\n",
    "        'SPD': spd,\n",
    "        'OAED': oaed,\n",
    "        'EOD': eod,\n",
    "        'Treatment Equality Difference': ted\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fairness(label, predictions, sens_attr='region', balanced=False):\n",
    "    \n",
    "    labels = user_labels[label].values\n",
    "    # labels = labels.astype(float)\n",
    "\n",
    "    predictions = predictions.int()\n",
    "    # predictions = predictions.float()\n",
    "\n",
    "    sens_attr_values = user_labels[sens_attr].values\n",
    "    # sens_attr_values = sens_attr_values.astype(float)    \n",
    "    \n",
    "    # Indices for privileged and unprivileged groups    \n",
    "    privileged_indices = sens_attr_values == 1\n",
    "    unprivileged_indices = sens_attr_values == 0\n",
    "    \n",
    "    # Calculating metrics for both groups\n",
    "    def calc_metrics(preds, lbls):\n",
    "        tp = ((preds == 1) & (lbls == 1)).sum().item()\n",
    "        tn = ((preds == 0) & (lbls == 0)).sum().item()\n",
    "        fp = ((preds == 1) & (lbls == 0)).sum().item()\n",
    "        fn = ((preds == 0) & (lbls == 1)).sum().item()\n",
    "        \n",
    "        accuracy = (tp + tn) / max(tp + tn + fp + fn, 1)  # Prevent division by zero\n",
    "        tpr = tp / max(tp + fn, 1)  # True Positive Rate\n",
    "        fpr = fp / max(fp + tn, 1)  # False Positive Rate\n",
    "        \n",
    "        return accuracy, tpr, fpr, fp, fn\n",
    "    \n",
    "    # Metrics for unprivileged\n",
    "    acc_unpriv, tpr_unpriv, _, fp_unpriv, fn_unpriv = calc_metrics(predictions[unprivileged_indices], labels[unprivileged_indices])\n",
    "    # Metrics for privileged\n",
    "    acc_priv, tpr_priv, _, fp_priv, fn_priv = calc_metrics(predictions[privileged_indices], labels[privileged_indices])\n",
    "    \n",
    "    # Statistical Parity for both groups\n",
    "    sp_unpriv = predictions[unprivileged_indices].float().mean().item()\n",
    "    sp_priv = predictions[privileged_indices].float().mean().item()\n",
    "    \n",
    "    # Calculating fairness metrics differences\n",
    "    spd = sp_priv - sp_unpriv  # Statistical Parity Difference\n",
    "    oaed = acc_priv - acc_unpriv  # Overall Accuracy Equality Difference\n",
    "    eod = tpr_priv - tpr_unpriv  # Equal Opportunity Difference\n",
    "    \n",
    "    # Treatment Equality for both groups\n",
    "    ted_unpriv = fp_unpriv / max(fn_unpriv, 1) if fn_unpriv != 0 else float('inf')  # Avoid division by zero\n",
    "    ted_priv = fp_priv / max(fn_priv, 1) if fn_priv != 0 else float('inf')\n",
    "    ted_diff = ted_unpriv - ted_priv  # Difference in Treatment Equality\n",
    "\n",
    "    unique, counts = torch.unique(predictions, return_counts=True)\n",
    "    prediction_distribution = dict(zip(unique.tolist(), counts.tolist()))\n",
    "\n",
    "    print(\"Prediction Distribution:\", prediction_distribution)\n",
    "    \n",
    "    privileged_preds = predictions[sens_attr_values == 1]\n",
    "    unprivileged_preds = predictions[sens_attr_values == 0]\n",
    "\n",
    "    # Analyze distribution for privileged group\n",
    "    priv_unique, priv_counts = torch.unique(privileged_preds, return_counts=True)\n",
    "    priv_distribution = dict(zip(priv_unique.tolist(), priv_counts.tolist()))\n",
    "\n",
    "    # Analyze distribution for unprivileged group\n",
    "    unpriv_unique, unpriv_counts = torch.unique(unprivileged_preds, return_counts=True)\n",
    "    unpriv_distribution = dict(zip(unpriv_unique.tolist(), unpriv_counts.tolist()))\n",
    "\n",
    "    print(\"Privileged Prediction Distribution:\", priv_distribution)\n",
    "    print(\"Unprivileged Prediction Distribution:\", unpriv_distribution)\n",
    "\n",
    "    priv_positive_rate = privileged_preds.float().mean().item()\n",
    "    unpriv_positive_rate = unprivileged_preds.float().mean().item()\n",
    "\n",
    "    print(\"Privileged Positive Prediction Rate:\", priv_positive_rate)\n",
    "    print(\"Unprivileged Positive Prediction Rate:\", unpriv_positive_rate)\n",
    "\n",
    "    \n",
    "    return {\n",
    "        'SPD': spd,\n",
    "        'OAED': oaed,\n",
    "        'EOD': eod,\n",
    "        'Treatment Equality Difference': ted_diff,\n",
    "        'SP_Unprivileged': sp_unpriv,\n",
    "        'SP_Privileged': sp_priv,\n",
    "        'OAED_Unprivileged': acc_unpriv,\n",
    "        'OAED_Privileged': acc_priv,\n",
    "        'EOD_Unprivileged': tpr_unpriv,\n",
    "        'EOD_Privileged': tpr_priv,\n",
    "        'TED_Unprivileged': ted_unpriv,\n",
    "        'TED_Privileged': ted_priv\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "def training(model, data, optimizer, epochs, fairness=False, alpha=0, beta=0, gamma=0, delta=0):\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index)\n",
    "        \n",
    "        if fairness:\n",
    "            loss = fairness_aware_loss(out[data.train_mask], data, data.x[data.train_mask, -1],\n",
    "                                       alpha=alpha, beta=beta, gamma=gamma, delta=delta)\n",
    "            \n",
    "        else:\n",
    "            # criterion = torch.nn.CrossEntropyLoss()\n",
    "            # criterion = torch.nn.BCELoss()\n",
    "            criterion = torch.nn.NLLLoss()\n",
    "            loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch} | Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "def test(model, data, balanced=False):\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "      out = model(data.x, data.edge_index)\n",
    "\n",
    "    _, pred = model(data.x, data.edge_index).max(dim=1)\n",
    "    correct = int(pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
    "    accuracy = correct / int(data.test_mask.sum())\n",
    "    # print(f'Accuracy: {accuracy}')\n",
    "\n",
    "    # Convert model outputs to binary predictions\n",
    "    predictions = out.argmax(dim=1)\n",
    "    # print(predictions[0:20])\n",
    "    \n",
    "    # unique, counts = torch.unique(predictions, return_counts=True)\n",
    "    # prediction_distribution = dict(zip(unique.tolist(), counts.tolist()))\n",
    "\n",
    "    # print(\"Prediction Distribution:\", prediction_distribution)\n",
    "\n",
    "    fairness_metrics = calculate_fairness(label='SALARY', predictions=predictions, sens_attr='country', balanced=balanced)\n",
    "    fairness_metrics['Accuracy'] = accuracy\n",
    "    # # Print the fairness metrics\n",
    "    # for metric, value in fairness_metrics.items():\n",
    "    #     print(f\"{metric}: {value}\")\n",
    "\n",
    "    return fairness_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(metrics):\n",
    "    count = -1\n",
    "\n",
    "    for key, value in metrics.items():\n",
    "        count += 1\n",
    "        if count == 3:\n",
    "            print(f\"\\n\\n{key} : {value:.5f}\")\n",
    "            count = 0\n",
    "        else:\n",
    "            print(f\"{key} : {value:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCN class that takes in the data as an input for dimensions of the convolutions\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, x, edge_index):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(data.num_node_features, 16)\n",
    "        self.conv2 = GCNConv(16, num_classes)\n",
    "        \n",
    "    def forward(self, x, edge_index, *args, **kwargs):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.conv2(x, edge_index)\n",
    "        # x = F.softmax(x, dim=1)\n",
    "        # return x\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([403, 97])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16570])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.edge_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model, define loss function and optimizer\n",
    "gcn_model = GCN(data.x, data.edge_index)\n",
    "gcn_optimizer = torch.optim.Adam(gcn_model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Loss: 85.22936248779297\n",
      "Epoch 10 | Loss: 12.499253273010254\n",
      "Epoch 20 | Loss: 10.388657569885254\n",
      "Epoch 30 | Loss: 0.4728127717971802\n",
      "Epoch 40 | Loss: 2.6040358543395996\n",
      "Epoch 50 | Loss: 1.0704957246780396\n",
      "Epoch 60 | Loss: 0.6616677641868591\n",
      "Epoch 70 | Loss: 0.5696073770523071\n",
      "Epoch 80 | Loss: 0.5207356810569763\n",
      "Epoch 90 | Loss: 0.4701775908470154\n",
      "Epoch 100 | Loss: 0.4744795858860016\n",
      "Epoch 110 | Loss: 0.46973034739494324\n",
      "Epoch 120 | Loss: 0.4693574011325836\n",
      "Epoch 130 | Loss: 0.4682544767856598\n",
      "Epoch 140 | Loss: 0.46799734234809875\n",
      "Epoch 150 | Loss: 0.46761539578437805\n",
      "Epoch 160 | Loss: 0.46723872423171997\n",
      "Epoch 170 | Loss: 0.46688640117645264\n",
      "Epoch 180 | Loss: 0.4665371775627136\n",
      "Epoch 190 | Loss: 0.46618619561195374\n",
      "Epoch 200 | Loss: 0.4658331871032715\n",
      "Epoch 210 | Loss: 0.46547842025756836\n",
      "Epoch 220 | Loss: 0.4651217758655548\n",
      "Epoch 230 | Loss: 0.4647635817527771\n",
      "Epoch 240 | Loss: 0.46440380811691284\n",
      "Epoch 250 | Loss: 0.46404242515563965\n",
      "Epoch 260 | Loss: 0.4636791944503784\n",
      "Epoch 270 | Loss: 0.4633142650127411\n",
      "Epoch 280 | Loss: 0.46294763684272766\n",
      "Epoch 290 | Loss: 0.46257925033569336\n",
      "Epoch 300 | Loss: 0.4622088074684143\n",
      "Epoch 310 | Loss: 0.46183666586875916\n",
      "Epoch 320 | Loss: 0.4614626467227936\n",
      "Epoch 330 | Loss: 0.4610867202281952\n",
      "Epoch 340 | Loss: 0.4607086777687073\n",
      "Epoch 350 | Loss: 0.4603288471698761\n",
      "Epoch 360 | Loss: 0.45994701981544495\n",
      "Epoch 370 | Loss: 0.45956307649612427\n",
      "Epoch 380 | Loss: 0.45917704701423645\n",
      "Epoch 390 | Loss: 0.45878899097442627\n",
      "Epoch 400 | Loss: 0.4583989977836609\n",
      "Epoch 410 | Loss: 0.4580068588256836\n",
      "Epoch 420 | Loss: 0.45761287212371826\n",
      "Epoch 430 | Loss: 0.45721668004989624\n",
      "Epoch 440 | Loss: 0.45681843161582947\n",
      "Epoch 450 | Loss: 0.45641839504241943\n",
      "Epoch 460 | Loss: 0.4560161530971527\n",
      "Epoch 470 | Loss: 0.4556120038032532\n",
      "Epoch 480 | Loss: 0.45520591735839844\n",
      "Epoch 490 | Loss: 0.45479798316955566\n",
      "Epoch 500 | Loss: 0.45438817143440247\n",
      "Epoch 510 | Loss: 0.45397618412971497\n",
      "Epoch 520 | Loss: 0.45356228947639465\n",
      "Epoch 530 | Loss: 0.45314669609069824\n",
      "Epoch 540 | Loss: 0.4527292251586914\n",
      "Epoch 550 | Loss: 0.4523099362850189\n",
      "Epoch 560 | Loss: 0.4518885612487793\n",
      "Epoch 570 | Loss: 0.45146578550338745\n",
      "Epoch 580 | Loss: 0.4510408937931061\n",
      "Epoch 590 | Loss: 0.45061442255973816\n",
      "Epoch 600 | Loss: 0.450186163187027\n",
      "Epoch 610 | Loss: 0.44975608587265015\n",
      "Epoch 620 | Loss: 0.4493243396282196\n",
      "Epoch 630 | Loss: 0.4488908350467682\n",
      "Epoch 640 | Loss: 0.448455810546875\n",
      "Epoch 650 | Loss: 0.44801902770996094\n",
      "Epoch 660 | Loss: 0.4475805163383484\n",
      "Epoch 670 | Loss: 0.44714048504829407\n",
      "Epoch 680 | Loss: 0.44669896364212036\n",
      "Epoch 690 | Loss: 0.44625556468963623\n",
      "Epoch 700 | Loss: 0.4458109140396118\n",
      "Epoch 710 | Loss: 0.445364773273468\n",
      "Epoch 720 | Loss: 0.44491681456565857\n",
      "Epoch 730 | Loss: 0.4444676339626312\n",
      "Epoch 740 | Loss: 0.4440169334411621\n",
      "Epoch 750 | Loss: 0.44356465339660645\n",
      "Epoch 760 | Loss: 0.44311100244522095\n",
      "Epoch 770 | Loss: 0.44265609979629517\n",
      "Epoch 780 | Loss: 0.4421996474266052\n",
      "Epoch 790 | Loss: 0.44174203276634216\n",
      "Epoch 800 | Loss: 0.44128283858299255\n",
      "Epoch 810 | Loss: 0.44082266092300415\n",
      "Epoch 820 | Loss: 0.4403611719608307\n",
      "Epoch 830 | Loss: 0.4398983418941498\n",
      "Epoch 840 | Loss: 0.4394344091415405\n",
      "Epoch 850 | Loss: 0.4389691650867462\n",
      "Epoch 860 | Loss: 0.43850305676460266\n",
      "Epoch 870 | Loss: 0.4380358159542084\n",
      "Epoch 880 | Loss: 0.4375673830509186\n",
      "Epoch 890 | Loss: 0.43709802627563477\n",
      "Epoch 900 | Loss: 0.43662768602371216\n",
      "Epoch 910 | Loss: 0.43615642189979553\n",
      "Epoch 920 | Loss: 0.4356842637062073\n",
      "Epoch 930 | Loss: 0.43521133065223694\n",
      "Epoch 940 | Loss: 0.43473735451698303\n",
      "Epoch 950 | Loss: 0.434262752532959\n",
      "Epoch 960 | Loss: 0.4337873160839081\n",
      "Epoch 970 | Loss: 0.4333111643791199\n",
      "Epoch 980 | Loss: 0.4328345060348511\n",
      "Epoch 990 | Loss: 0.43235716223716736\n"
     ]
    }
   ],
   "source": [
    "# Train the first model: GCN, standard data, NLL loss\n",
    "training(model=gcn_model, data=data, optimizer=gcn_optimizer, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the values for the standard GCN model with the standard NLL loss: \n",
      "Prediction Distribution: {0: 23, 1: 380}\n",
      "Privileged Prediction Distribution: {0: 7, 1: 100}\n",
      "Unprivileged Prediction Distribution: {0: 16, 1: 280}\n",
      "Privileged Positive Prediction Rate: 0.9345794320106506\n",
      "Unprivileged Positive Prediction Rate: 0.9459459185600281\n",
      "\n",
      "SPD : -0.01137\n",
      "OAED : 0.02513\n",
      "EOD : 0.00969\n",
      "\n",
      "\n",
      "Treatment Equality Difference : -7.00000\n",
      "SP_Unprivileged : 0.94595\n",
      "SP_Privileged : 0.93458\n",
      "\n",
      "\n",
      "OAED_Unprivileged : 0.79730\n",
      "OAED_Privileged : 0.82243\n",
      "EOD_Unprivileged : 0.97826\n",
      "\n",
      "\n",
      "EOD_Privileged : 0.98795\n",
      "TED_Unprivileged : 11.00000\n",
      "TED_Privileged : 18.00000\n",
      "\n",
      "\n",
      "Accuracy : 0.74074\n"
     ]
    }
   ],
   "source": [
    "# Test the first model: GCN, standard data, NLL loss\n",
    "print(\"Here are the values for the standard GCN model with the standard NLL loss: \")\n",
    "\n",
    "metrics_base_gcn_model = test(gcn_model, data)\n",
    "print()\n",
    "print_metrics(metrics_base_gcn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.explain import Explainer, GNNExplainer\n",
    "\n",
    "explainer = Explainer(\n",
    "    model=gcn_model,\n",
    "    algorithm=GNNExplainer(epochs=50),\n",
    "    explanation_type='model',\n",
    "    node_mask_type='attributes',\n",
    "    edge_mask_type='object',\n",
    "    model_config=dict(\n",
    "        mode='multiclass_classification',\n",
    "        task_level='node',\n",
    "        return_type='log_probs', \n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3742, 0.3539, 0.3692,  ..., 0.3750, 0.3746, 0.3753])\n",
      "tensor([[0.4206, 0.4052, 0.3808,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.3945, 0.3694, 0.3747,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3753, 0.4082,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.3973, 0.3880, 0.4192,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.3506, 0.4383, 0.4063,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.4261, 0.4202, 0.3942,  ..., 0.0000, 0.0000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "explanation = explainer(data.x, edge_index)\n",
    "print(explanation.edge_mask)\n",
    "print(explanation.node_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>SALARY</th>\n",
       "      <th>AGE</th>\n",
       "      <th>MP</th>\n",
       "      <th>FG</th>\n",
       "      <th>FGA</th>\n",
       "      <th>FG%</th>\n",
       "      <th>3P</th>\n",
       "      <th>3PA</th>\n",
       "      <th>3P%</th>\n",
       "      <th>...</th>\n",
       "      <th>ORL/TOR</th>\n",
       "      <th>PHI</th>\n",
       "      <th>PHI/OKC</th>\n",
       "      <th>PHX</th>\n",
       "      <th>POR</th>\n",
       "      <th>SA</th>\n",
       "      <th>SAC</th>\n",
       "      <th>TOR</th>\n",
       "      <th>UTAH</th>\n",
       "      <th>WSH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>105305397</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>14.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>5.6</td>\n",
       "      <td>0.487</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.374</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49680175</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>4.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.383</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.400</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>364013199</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>20.1</td>\n",
       "      <td>2.4</td>\n",
       "      <td>5.9</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.321</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>234811698</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>4.8</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.337</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1031967637561954304</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>8.4</td>\n",
       "      <td>2.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>24342206</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>18.7</td>\n",
       "      <td>2.5</td>\n",
       "      <td>6.4</td>\n",
       "      <td>0.390</td>\n",
       "      <td>1.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.392</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>270689028</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>34.7</td>\n",
       "      <td>8.1</td>\n",
       "      <td>18.3</td>\n",
       "      <td>0.444</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.6</td>\n",
       "      <td>0.399</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>47218790</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>25.1</td>\n",
       "      <td>3.4</td>\n",
       "      <td>7.6</td>\n",
       "      <td>0.454</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.288</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>42562446</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>33.4</td>\n",
       "      <td>8.5</td>\n",
       "      <td>18.3</td>\n",
       "      <td>0.468</td>\n",
       "      <td>4.1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.411</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>473227811</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3.7</td>\n",
       "      <td>8.9</td>\n",
       "      <td>0.413</td>\n",
       "      <td>1.8</td>\n",
       "      <td>5.4</td>\n",
       "      <td>0.342</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>403 rows × 98 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 user_id  SALARY  AGE    MP   FG   FGA    FG%   3P   3PA  \\\n",
       "0              105305397       0   25  14.8  2.7   5.6  0.487  0.8   2.1   \n",
       "1               49680175       0   32   4.9  0.9   2.4  0.383  0.6   1.4   \n",
       "2              364013199       0   20  20.1  2.4   5.9  0.399  0.6   2.0   \n",
       "3              234811698       0   25  14.0  2.1   4.8  0.440  0.5   1.4   \n",
       "4    1031967637561954304       1   28   8.4  2.1   3.8  0.545  0.0   0.0   \n",
       "..                   ...     ...  ...   ...  ...   ...    ...  ...   ...   \n",
       "398             24342206       1   39  18.7  2.5   6.4  0.390  1.3   3.3   \n",
       "399            270689028       1   26  34.7  8.1  18.3  0.444  3.0   7.6   \n",
       "400             47218790       0   29  25.1  3.4   7.6  0.454  0.5   1.7   \n",
       "401             42562446       1   28  33.4  8.5  18.3  0.468  4.1  10.0   \n",
       "402            473227811       1   25  24.0  3.7   8.9  0.413  1.8   5.4   \n",
       "\n",
       "       3P%  ...  ORL/TOR  PHI  PHI/OKC  PHX  POR  SA  SAC  TOR  UTAH  WSH  \n",
       "0    0.374  ...        0    0        0    0    0   0    0    0     0    0  \n",
       "1    0.400  ...        0    0        0    0    0   0    0    0     0    0  \n",
       "2    0.321  ...        0    0        0    0    0   0    0    0     0    0  \n",
       "3    0.337  ...        0    0        0    0    0   0    0    0     0    0  \n",
       "4    0.000  ...        0    0        0    0    0   0    0    0     0    0  \n",
       "..     ...  ...      ...  ...      ...  ...  ...  ..  ...  ...   ...  ...  \n",
       "398  0.392  ...        0    0        0    0    0   1    0    0     0    0  \n",
       "399  0.399  ...        0    0        0    0    0   0    0    0     0    0  \n",
       "400  0.288  ...        0    0        0    0    0   0    1    0     0    0  \n",
       "401  0.411  ...        0    0        0    0    0   0    0    0     0    0  \n",
       "402  0.342  ...        0    0        0    0    0   0    0    0     0    0  \n",
       "\n",
       "[403 rows x 98 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "explanation.visualize_feature_importance(top_k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fair Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FairnessAwareMessagePassingLayer(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(FairnessAwareMessagePassingLayer, self).__init__(aggr='mean')  \n",
    "        self.lin = nn.Linear(in_channels, out_channels)\n",
    "        self.a_fair = nn.Parameter(torch.rand(out_channels)) \n",
    "        self.sensitive_attr = torch.tensor(user_labels['country'].values, dtype=torch.float) \n",
    "        self.bias_correction = nn.Parameter(torch.rand(1))\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # Add self-loops \n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "\n",
    "        x = self.lin(x)\n",
    "\n",
    "        return self.propagate(edge_index, size=(x.size(0), x.size(0)), x=x)\n",
    "    \n",
    "    def message(self, x_j, edge_index, size):\n",
    "        row, col = edge_index\n",
    "        deg = degree(row, size[0], dtype=x_j.dtype)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "\n",
    "        # Compute statistical parity difference for each edge\n",
    "        group_difference = self.sensitive_attr[row] - self.sensitive_attr[col]\n",
    "        \n",
    "        # Adjust messages based on statistical parity\n",
    "        fairness_adjustment = (1 + self.bias_correction * group_difference.view(-1, 1))\n",
    "\n",
    "        return fairness_adjustment * norm.view(-1, 1) * x_j\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        return aggr_out * self.a_fair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_GCN(torch.nn.Module):\n",
    "    def __init__(self, data):\n",
    "        super(custom_GCN, self).__init__()\n",
    "        self.conv1 = FairnessAwareMessagePassingLayer(data.num_node_features, 16)\n",
    "        self.conv2 = FairnessAwareMessagePassingLayer(16, 2) # 2 output classes for gender\n",
    "\n",
    "    def forward(self, x, edge_index, *args, **kwargs):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Fair Model\n",
    "\n",
    "This model is an instantiation of the `custom_GCN` using the standard cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.optim.SGD: Stochastic Gradient Descent optimizer.\n",
    "torch.optim.Adam: Adam optimizer.\n",
    "torch.optim.Adagrad: Adagrad optimizer.\n",
    "torch.optim.Adadelta: Adadelta optimizer.\n",
    "torch.optim.RMSprop: RMSprop optimizer.\n",
    "torch.optim.AdamW: AdamW optimizer.\n",
    "torch.optim.SparseAdam: SparseAdam optimizer.\n",
    "torch.optim.LBFGS: L-BFGS optimizer.\n",
    "torch.optim.Rprop: Rprop optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Loss: 0.6950044631958008\n",
      "Epoch 10 | Loss: 0.6877216696739197\n",
      "Epoch 20 | Loss: 0.6860019564628601\n",
      "Epoch 30 | Loss: 0.6852525472640991\n",
      "Epoch 40 | Loss: 0.6845036745071411\n",
      "Epoch 50 | Loss: 0.6834521889686584\n",
      "Epoch 60 | Loss: 0.6819733381271362\n",
      "Epoch 70 | Loss: 0.6798995137214661\n",
      "Epoch 80 | Loss: 0.6769739389419556\n",
      "Epoch 90 | Loss: 0.6725189685821533\n",
      "Epoch 100 | Loss: 0.6645283102989197\n",
      "Epoch 110 | Loss: 0.6504476070404053\n",
      "Epoch 120 | Loss: 0.6287883520126343\n",
      "Epoch 130 | Loss: 0.6013134717941284\n",
      "Epoch 140 | Loss: 0.5786999464035034\n",
      "Epoch 150 | Loss: 0.567398726940155\n",
      "Epoch 160 | Loss: 0.5606890320777893\n",
      "Epoch 170 | Loss: 0.55605149269104\n",
      "Epoch 180 | Loss: 0.5529772639274597\n",
      "Epoch 190 | Loss: 0.5504505038261414\n",
      "Epoch 200 | Loss: 0.5480257868766785\n",
      "Epoch 210 | Loss: 0.54550701379776\n",
      "Epoch 220 | Loss: 0.5428222417831421\n",
      "Epoch 230 | Loss: 0.5399263501167297\n",
      "Epoch 240 | Loss: 0.5367610454559326\n",
      "Epoch 250 | Loss: 0.5334406495094299\n",
      "Epoch 260 | Loss: 0.5299265384674072\n",
      "Epoch 270 | Loss: 0.5262117385864258\n",
      "Epoch 280 | Loss: 0.5221956372261047\n",
      "Epoch 290 | Loss: 0.5179950594902039\n",
      "Epoch 300 | Loss: 0.5139548182487488\n",
      "Epoch 310 | Loss: 0.5102739930152893\n",
      "Epoch 320 | Loss: 0.506371796131134\n",
      "Epoch 330 | Loss: 0.5025128126144409\n",
      "Epoch 340 | Loss: 0.4989892840385437\n",
      "Epoch 350 | Loss: 0.4953855872154236\n",
      "Epoch 360 | Loss: 0.49150773882865906\n",
      "Epoch 370 | Loss: 0.4877648651599884\n",
      "Epoch 380 | Loss: 0.4840431809425354\n",
      "Epoch 390 | Loss: 0.480384886264801\n",
      "Epoch 400 | Loss: 0.4767792522907257\n",
      "Epoch 410 | Loss: 0.47371137142181396\n",
      "Epoch 420 | Loss: 0.4710322320461273\n",
      "Epoch 430 | Loss: 0.46882250905036926\n",
      "Epoch 440 | Loss: 0.46721696853637695\n",
      "Epoch 450 | Loss: 0.46519115567207336\n",
      "Epoch 460 | Loss: 0.46366190910339355\n",
      "Epoch 470 | Loss: 0.4621709883213043\n",
      "Epoch 480 | Loss: 0.4610205590724945\n",
      "Epoch 490 | Loss: 0.4599289000034332\n",
      "Epoch 500 | Loss: 0.4590439796447754\n",
      "Epoch 510 | Loss: 0.4582524299621582\n",
      "Epoch 520 | Loss: 0.4575119614601135\n",
      "Epoch 530 | Loss: 0.45681455731391907\n",
      "Epoch 540 | Loss: 0.4561504125595093\n",
      "Epoch 550 | Loss: 0.4554792046546936\n",
      "Epoch 560 | Loss: 0.4548635482788086\n",
      "Epoch 570 | Loss: 0.4542975425720215\n",
      "Epoch 580 | Loss: 0.45375773310661316\n",
      "Epoch 590 | Loss: 0.45323196053504944\n",
      "Epoch 600 | Loss: 0.45271167159080505\n",
      "Epoch 610 | Loss: 0.45220571756362915\n",
      "Epoch 620 | Loss: 0.4517795741558075\n",
      "Epoch 630 | Loss: 0.4512808322906494\n",
      "Epoch 640 | Loss: 0.4508173167705536\n",
      "Epoch 650 | Loss: 0.45036959648132324\n",
      "Epoch 660 | Loss: 0.4499025046825409\n",
      "Epoch 670 | Loss: 0.44943100214004517\n",
      "Epoch 680 | Loss: 0.4489753246307373\n",
      "Epoch 690 | Loss: 0.4484666585922241\n",
      "Epoch 700 | Loss: 0.4479997158050537\n",
      "Epoch 710 | Loss: 0.447480708360672\n",
      "Epoch 720 | Loss: 0.446962833404541\n",
      "Epoch 730 | Loss: 0.44642922282218933\n",
      "Epoch 740 | Loss: 0.4458898603916168\n",
      "Epoch 750 | Loss: 0.4453345239162445\n",
      "Epoch 760 | Loss: 0.44476866722106934\n",
      "Epoch 770 | Loss: 0.44418641924858093\n",
      "Epoch 780 | Loss: 0.44358566403388977\n",
      "Epoch 790 | Loss: 0.4429756700992584\n",
      "Epoch 800 | Loss: 0.4426034986972809\n",
      "Epoch 810 | Loss: 0.44205915927886963\n",
      "Epoch 820 | Loss: 0.44148117303848267\n",
      "Epoch 830 | Loss: 0.4406658709049225\n",
      "Epoch 840 | Loss: 0.44022756814956665\n",
      "Epoch 850 | Loss: 0.4395965337753296\n",
      "Epoch 860 | Loss: 0.43904611468315125\n",
      "Epoch 870 | Loss: 0.43846556544303894\n",
      "Epoch 880 | Loss: 0.4381856620311737\n",
      "Epoch 890 | Loss: 0.43803516030311584\n",
      "Epoch 900 | Loss: 0.43700602650642395\n",
      "Epoch 910 | Loss: 0.4365164339542389\n",
      "Epoch 920 | Loss: 0.43583717942237854\n",
      "Epoch 930 | Loss: 0.4350546598434448\n",
      "Epoch 940 | Loss: 0.43450382351875305\n",
      "Epoch 950 | Loss: 0.4338195025920868\n",
      "Epoch 960 | Loss: 0.4331847131252289\n",
      "Epoch 970 | Loss: 0.4326189458370209\n",
      "Epoch 980 | Loss: 0.4320216774940491\n",
      "Epoch 990 | Loss: 0.43120062351226807\n"
     ]
    }
   ],
   "source": [
    "model2 = custom_GCN(data)\n",
    "optimizer2 = torch.optim.Adam(model2.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model: Custom MP GNN, cross-entropy loss\n",
    "training(model=model2, data=data, optimizer=optimizer2, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the values for the CustomGNN model with the standard cross-entropy loss: \n",
      "Prediction Distribution: {0: 30, 1: 373}\n",
      "Privileged Prediction Distribution: {0: 8, 1: 99}\n",
      "Unprivileged Prediction Distribution: {0: 22, 1: 274}\n",
      "Privileged Positive Prediction Rate: 0.9252336621284485\n",
      "Unprivileged Positive Prediction Rate: 0.9256756901741028\n",
      "\n",
      "SPD : -0.00044\n",
      "OAED : 0.00903\n",
      "EOD : 0.00634\n",
      "\n",
      "\n",
      "Treatment Equality Difference : -1.71429\n",
      "SP_Unprivileged : 0.92568\n",
      "SP_Privileged : 0.92523\n",
      "\n",
      "\n",
      "OAED_Unprivileged : 0.80405\n",
      "OAED_Privileged : 0.81308\n",
      "EOD_Unprivileged : 0.96957\n",
      "\n",
      "\n",
      "EOD_Privileged : 0.97590\n",
      "TED_Unprivileged : 7.28571\n",
      "TED_Privileged : 9.00000\n",
      "\n",
      "\n",
      "Accuracy : 0.72840\n"
     ]
    }
   ],
   "source": [
    "# Test the first FAIR model: CustomGNN, cross-entropy loss\n",
    "print(\"Here are the values for the CustomGNN model with the standard cross-entropy loss: \")\n",
    "\n",
    "metrics_custom_gnn_model_1 = test(model2, data)\n",
    "print()\n",
    "print_metrics(metrics_custom_gnn_model_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the values for the CustomGNN model with the standard cross-entropy loss: \n",
      "Prediction Distribution: {0: 39, 1: 364}\n",
      "Privileged Prediction Distribution: {0: 10, 1: 97}\n",
      "Unprivileged Prediction Distribution: {0: 29, 1: 267}\n",
      "Privileged Positive Prediction Rate: 0.9065420627593994\n",
      "Unprivileged Positive Prediction Rate: 0.9020270109176636\n",
      "\n",
      "SPD : 0.00452\n",
      "OAED : 0.03110\n",
      "EOD : 0.02373\n",
      "\n",
      "\n",
      "Treatment Equality Difference : -3.63636\n",
      "SP_Unprivileged : 0.90203\n",
      "SP_Privileged : 0.90654\n",
      "\n",
      "\n",
      "OAED_Unprivileged : 0.80068\n",
      "OAED_Privileged : 0.83178\n",
      "EOD_Unprivileged : 0.95217\n",
      "\n",
      "\n",
      "EOD_Privileged : 0.97590\n",
      "TED_Unprivileged : 4.36364\n",
      "TED_Privileged : 8.00000\n",
      "\n",
      "\n",
      "Accuracy : 0.70370\n"
     ]
    }
   ],
   "source": [
    "# Test the first FAIR model: CustomGNN, cross-entropy loss\n",
    "print(\"Here are the values for the CustomGNN model with the standard cross-entropy loss: \")\n",
    "\n",
    "metrics_custom_gnn_model_1 = test(model2, data)\n",
    "print()\n",
    "print_metrics(metrics_custom_gnn_model_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPD : 0.06413\n",
    "OAED : -0.01326\n",
    "EOD : 0.09307\n",
    "\n",
    "\n",
    "Treatment Equality Difference : -5.26667\n",
    "SP_Unprivileged : 0.49662\n",
    "SP_Privileged : 0.56075\n",
    "\n",
    "\n",
    "OAED_Unprivileged : 0.77027\n",
    "OAED_Privileged : 0.75701\n",
    "EOD_Unprivileged : 0.83193\n",
    "\n",
    "\n",
    "EOD_Privileged : 0.92500\n",
    "TED_Unprivileged : 2.40000\n",
    "TED_Privileged : 7.66667\n",
    "\n",
    "\n",
    "Accuracy : 0.70370"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Fair Model\n",
    "\n",
    "This model is an instantiation of the `custom_GCN` using the fair cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Loss: 0.4254482686519623\n",
      "Epoch 10 | Loss: 0.4117738902568817\n",
      "Epoch 20 | Loss: 0.4113517701625824\n",
      "Epoch 30 | Loss: 0.4107479155063629\n",
      "Epoch 40 | Loss: 0.41003385186195374\n",
      "Epoch 50 | Loss: 0.40915536880493164\n",
      "Epoch 60 | Loss: 0.40795350074768066\n",
      "Epoch 70 | Loss: 0.4060845375061035\n",
      "Epoch 80 | Loss: 0.40246859192848206\n",
      "Epoch 90 | Loss: 0.39417195320129395\n",
      "Epoch 100 | Loss: 0.3776552975177765\n",
      "Epoch 110 | Loss: 0.3560972809791565\n",
      "Epoch 120 | Loss: 0.33953672647476196\n",
      "Epoch 130 | Loss: 0.3305354118347168\n",
      "Epoch 140 | Loss: 0.3265801966190338\n",
      "Epoch 150 | Loss: 0.32426169514656067\n",
      "Epoch 160 | Loss: 0.3220602571964264\n",
      "Epoch 170 | Loss: 0.31975382566452026\n",
      "Epoch 180 | Loss: 0.31733980774879456\n",
      "Epoch 190 | Loss: 0.3149070143699646\n",
      "Epoch 200 | Loss: 0.3125245273113251\n",
      "Epoch 210 | Loss: 0.3103429079055786\n",
      "Epoch 220 | Loss: 0.3080013692378998\n",
      "Epoch 230 | Loss: 0.3054915964603424\n",
      "Epoch 240 | Loss: 0.30289605259895325\n",
      "Epoch 250 | Loss: 0.30009761452674866\n",
      "Epoch 260 | Loss: 0.2974262237548828\n",
      "Epoch 270 | Loss: 0.2951871454715729\n",
      "Epoch 280 | Loss: 0.29321709275245667\n",
      "Epoch 290 | Loss: 0.2916015088558197\n",
      "Epoch 300 | Loss: 0.29018551111221313\n",
      "Epoch 310 | Loss: 0.2889983355998993\n",
      "Epoch 320 | Loss: 0.2877044379711151\n",
      "Epoch 330 | Loss: 0.2865731418132782\n",
      "Epoch 340 | Loss: 0.2854894697666168\n",
      "Epoch 350 | Loss: 0.28446173667907715\n",
      "Epoch 360 | Loss: 0.2834673821926117\n",
      "Epoch 370 | Loss: 0.2825048267841339\n",
      "Epoch 380 | Loss: 0.28157439827919006\n",
      "Epoch 390 | Loss: 0.280672162771225\n",
      "Epoch 400 | Loss: 0.2798171043395996\n",
      "Epoch 410 | Loss: 0.2789958417415619\n",
      "Epoch 420 | Loss: 0.2781998813152313\n",
      "Epoch 430 | Loss: 0.27743765711784363\n",
      "Epoch 440 | Loss: 0.2766896188259125\n",
      "Epoch 450 | Loss: 0.2759855091571808\n",
      "Epoch 460 | Loss: 0.27528953552246094\n",
      "Epoch 470 | Loss: 0.2746780514717102\n",
      "Epoch 480 | Loss: 0.2739912271499634\n",
      "Epoch 490 | Loss: 0.27336642146110535\n",
      "Epoch 500 | Loss: 0.2727943956851959\n",
      "Epoch 510 | Loss: 0.27224859595298767\n",
      "Epoch 520 | Loss: 0.2717001736164093\n",
      "Epoch 530 | Loss: 0.2711530923843384\n",
      "Epoch 540 | Loss: 0.2706776559352875\n",
      "Epoch 550 | Loss: 0.2700531780719757\n",
      "Epoch 560 | Loss: 0.2695893347263336\n",
      "Epoch 570 | Loss: 0.26904934644699097\n",
      "Epoch 580 | Loss: 0.2699600160121918\n",
      "Epoch 590 | Loss: 0.26665085554122925\n",
      "Epoch 600 | Loss: 0.26570165157318115\n",
      "Epoch 610 | Loss: 0.26521697640419006\n",
      "Epoch 620 | Loss: 0.2646794617176056\n",
      "Epoch 630 | Loss: 0.26291707158088684\n",
      "Epoch 640 | Loss: 0.25984999537467957\n",
      "Epoch 650 | Loss: 0.2579827904701233\n",
      "Epoch 660 | Loss: 0.25713807344436646\n",
      "Epoch 670 | Loss: 0.25556230545043945\n",
      "Epoch 680 | Loss: 0.2545117437839508\n",
      "Epoch 690 | Loss: 0.2546663284301758\n",
      "Epoch 700 | Loss: 0.2522643506526947\n",
      "Epoch 710 | Loss: 0.25116389989852905\n",
      "Epoch 720 | Loss: 0.25099125504493713\n",
      "Epoch 730 | Loss: 0.25041115283966064\n",
      "Epoch 740 | Loss: 0.24905304610729218\n",
      "Epoch 750 | Loss: 0.24867062270641327\n",
      "Epoch 760 | Loss: 0.24698860943317413\n",
      "Epoch 770 | Loss: 0.24538026750087738\n",
      "Epoch 780 | Loss: 0.24461285769939423\n",
      "Epoch 790 | Loss: 0.24338576197624207\n",
      "Epoch 800 | Loss: 0.24270935356616974\n",
      "Epoch 810 | Loss: 0.24140216410160065\n",
      "Epoch 820 | Loss: 0.24061691761016846\n",
      "Epoch 830 | Loss: 0.2398735135793686\n",
      "Epoch 840 | Loss: 0.23947151005268097\n",
      "Epoch 850 | Loss: 0.23840586841106415\n",
      "Epoch 860 | Loss: 0.2385375052690506\n",
      "Epoch 870 | Loss: 0.23626349866390228\n",
      "Epoch 880 | Loss: 0.2353190928697586\n",
      "Epoch 890 | Loss: 0.23518036305904388\n",
      "Epoch 900 | Loss: 0.23419617116451263\n",
      "Epoch 910 | Loss: 0.23351548612117767\n",
      "Epoch 920 | Loss: 0.23460455238819122\n",
      "Epoch 930 | Loss: 0.23230595886707306\n",
      "Epoch 940 | Loss: 0.23512698709964752\n",
      "Epoch 950 | Loss: 0.23227737843990326\n",
      "Epoch 960 | Loss: 0.23113109171390533\n",
      "Epoch 970 | Loss: 0.23055094480514526\n",
      "Epoch 980 | Loss: 0.23015925288200378\n",
      "Epoch 990 | Loss: 0.22956044971942902\n"
     ]
    }
   ],
   "source": [
    "custom_gnn_model_2 = custom_GCN(data)\n",
    "optimizer_custom_gnn_model_2 = torch.optim.Adam(custom_gnn_model_2.parameters(), lr=0.01)\n",
    "\n",
    "fairness=True\n",
    "alpha, beta, gamma, delta = 0.1, 0.1, 0.1, 0.1\n",
    "\n",
    "# Train the model: Custom MP GNN, FAIR cross-entropy loss\n",
    "training(model=custom_gnn_model_2, \n",
    "         data=data, \n",
    "         optimizer=optimizer_custom_gnn_model_2, \n",
    "         fairness=fairness, \n",
    "         alpha=alpha, \n",
    "         beta=beta, \n",
    "         gamma=gamma, \n",
    "         delta=delta, \n",
    "         epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the values for the CustomGNN model with the FAIR standard cross-entropy loss: \n",
      "Prediction Distribution: {0: 36, 1: 367}\n",
      "Privileged Prediction Distribution: {0: 10, 1: 97}\n",
      "Unprivileged Prediction Distribution: {0: 26, 1: 270}\n",
      "Privileged Positive Prediction Rate: 0.9065420627593994\n",
      "Unprivileged Positive Prediction Rate: 0.912162184715271\n",
      "\n",
      "SPD : -0.00562\n",
      "OAED : 0.04641\n",
      "EOD : 0.02708\n",
      "\n",
      "\n",
      "Treatment Equality Difference : -9.55556\n",
      "SP_Unprivileged : 0.91216\n",
      "SP_Privileged : 0.90654\n",
      "\n",
      "\n",
      "OAED_Unprivileged : 0.80405\n",
      "OAED_Privileged : 0.85047\n",
      "EOD_Unprivileged : 0.96087\n",
      "\n",
      "\n",
      "EOD_Privileged : 0.98795\n",
      "TED_Unprivileged : 5.44444\n",
      "TED_Privileged : 15.00000\n",
      "\n",
      "\n",
      "Accuracy : 0.71605\n"
     ]
    }
   ],
   "source": [
    "# Test the second FAIR model: CustomGNN, FAIR cross-entropy loss\n",
    "print(\"Here are the values for the CustomGNN model with the FAIR standard cross-entropy loss: \")\n",
    "\n",
    "metrics_custom_gnn_model_2 = test(custom_gnn_model_2, data)\n",
    "print()\n",
    "print_metrics(metrics_custom_gnn_model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the values for the CustomGNN model with the FAIR standard cross-entropy loss: \n",
      "Prediction Distribution: {0: 46, 1: 357}\n",
      "Privileged Prediction Distribution: {0: 12, 1: 95}\n",
      "Unprivileged Prediction Distribution: {0: 34, 1: 262}\n",
      "Privileged Positive Prediction Rate: 0.8878504633903503\n",
      "Unprivileged Positive Prediction Rate: 0.8851351141929626\n",
      "\n",
      "SPD : 0.00272\n",
      "OAED : 0.05993\n",
      "EOD : 0.04112\n",
      "\n",
      "\n",
      "Treatment Equality Difference : -3.86667\n",
      "SP_Unprivileged : 0.88514\n",
      "SP_Privileged : 0.88785\n",
      "\n",
      "\n",
      "OAED_Unprivileged : 0.79054\n",
      "OAED_Privileged : 0.85047\n",
      "EOD_Unprivileged : 0.93478\n",
      "\n",
      "\n",
      "EOD_Privileged : 0.97590\n",
      "TED_Unprivileged : 3.13333\n",
      "TED_Privileged : 7.00000\n",
      "\n",
      "\n",
      "Accuracy : 0.69136\n"
     ]
    }
   ],
   "source": [
    "# Test the second FAIR model: CustomGNN, FAIR cross-entropy loss\n",
    "print(\"Here are the values for the CustomGNN model with the FAIR standard cross-entropy loss: \")\n",
    "\n",
    "metrics_custom_gnn_model_2 = test(custom_gnn_model_2, data)\n",
    "print()\n",
    "print_metrics(metrics_custom_gnn_model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Loss: 0.4283818006515503\n",
      "Epoch 10 | Loss: 0.4110504388809204\n",
      "Epoch 20 | Loss: 0.41055238246917725\n",
      "Epoch 30 | Loss: 0.40999820828437805\n",
      "Epoch 40 | Loss: 0.40917593240737915\n",
      "Epoch 50 | Loss: 0.40783610939979553\n",
      "Epoch 60 | Loss: 0.40523719787597656\n",
      "Epoch 70 | Loss: 0.4000367224216461\n",
      "Epoch 80 | Loss: 0.3906148374080658\n",
      "Epoch 90 | Loss: 0.37659043073654175\n",
      "Epoch 100 | Loss: 0.3592928946018219\n",
      "Epoch 110 | Loss: 0.3450210392475128\n",
      "Epoch 120 | Loss: 0.33666667342185974\n",
      "Epoch 130 | Loss: 0.33226776123046875\n",
      "Epoch 140 | Loss: 0.32896682620048523\n",
      "Epoch 150 | Loss: 0.32094353437423706\n",
      "Epoch 160 | Loss: 0.304307758808136\n",
      "Epoch 170 | Loss: 0.2959948182106018\n",
      "Epoch 180 | Loss: 0.29064521193504333\n",
      "Epoch 190 | Loss: 0.287928968667984\n",
      "Epoch 200 | Loss: 0.2862420082092285\n",
      "Epoch 210 | Loss: 0.3039364218711853\n",
      "Epoch 220 | Loss: 0.2859782874584198\n",
      "Epoch 230 | Loss: 0.29414093494415283\n",
      "Epoch 240 | Loss: 0.28795966506004333\n",
      "Epoch 250 | Loss: 0.2881815433502197\n",
      "Epoch 260 | Loss: 0.28648123145103455\n",
      "Epoch 270 | Loss: 0.28592002391815186\n",
      "Epoch 280 | Loss: 0.28520485758781433\n",
      "Epoch 290 | Loss: 0.28466182947158813\n",
      "Epoch 300 | Loss: 0.2842182517051697\n",
      "Epoch 310 | Loss: 0.2838208079338074\n",
      "Epoch 320 | Loss: 0.28346824645996094\n",
      "Epoch 330 | Loss: 0.2831445038318634\n",
      "Epoch 340 | Loss: 0.2828391194343567\n",
      "Epoch 350 | Loss: 0.28254666924476624\n",
      "Epoch 360 | Loss: 0.28226351737976074\n",
      "Epoch 370 | Loss: 0.2819890081882477\n",
      "Epoch 380 | Loss: 0.2817411720752716\n",
      "Epoch 390 | Loss: 0.2815302610397339\n",
      "Epoch 400 | Loss: 0.28134316205978394\n",
      "Epoch 410 | Loss: 0.2811685800552368\n",
      "Epoch 420 | Loss: 0.28100091218948364\n",
      "Epoch 430 | Loss: 0.28083789348602295\n",
      "Epoch 440 | Loss: 0.2806778848171234\n",
      "Epoch 450 | Loss: 0.2805199921131134\n",
      "Epoch 460 | Loss: 0.28036364912986755\n",
      "Epoch 470 | Loss: 0.28020891547203064\n",
      "Epoch 480 | Loss: 0.2800544202327728\n",
      "Epoch 490 | Loss: 0.2799004912376404\n",
      "Epoch 500 | Loss: 0.279746413230896\n",
      "Epoch 510 | Loss: 0.2795928120613098\n",
      "Epoch 520 | Loss: 0.27943867444992065\n",
      "Epoch 530 | Loss: 0.27928420901298523\n",
      "Epoch 540 | Loss: 0.27912890911102295\n",
      "Epoch 550 | Loss: 0.2789734899997711\n",
      "Epoch 560 | Loss: 0.2788166403770447\n",
      "Epoch 570 | Loss: 0.2786599397659302\n",
      "Epoch 580 | Loss: 0.278499573469162\n",
      "Epoch 590 | Loss: 0.27834415435791016\n",
      "Epoch 600 | Loss: 0.27817821502685547\n",
      "Epoch 610 | Loss: 0.27800896763801575\n",
      "Epoch 620 | Loss: 0.2778375446796417\n",
      "Epoch 630 | Loss: 0.2776604890823364\n",
      "Epoch 640 | Loss: 0.27749013900756836\n",
      "Epoch 650 | Loss: 0.2772970199584961\n",
      "Epoch 660 | Loss: 0.27711278200149536\n",
      "Epoch 670 | Loss: 0.27694711089134216\n",
      "Epoch 680 | Loss: 0.28012701869010925\n",
      "Epoch 690 | Loss: 0.3002854585647583\n",
      "Epoch 700 | Loss: 0.297372967004776\n",
      "Epoch 710 | Loss: 0.2820638418197632\n",
      "Epoch 720 | Loss: 0.2838151156902313\n",
      "Epoch 730 | Loss: 0.28180310130119324\n",
      "Epoch 740 | Loss: 0.28017717599868774\n",
      "Epoch 750 | Loss: 0.2794739305973053\n",
      "Epoch 760 | Loss: 0.27889811992645264\n",
      "Epoch 770 | Loss: 0.27842849493026733\n",
      "Epoch 780 | Loss: 0.27805274724960327\n",
      "Epoch 790 | Loss: 0.27773305773735046\n",
      "Epoch 800 | Loss: 0.27744078636169434\n",
      "Epoch 810 | Loss: 0.27716541290283203\n",
      "Epoch 820 | Loss: 0.2769027352333069\n",
      "Epoch 830 | Loss: 0.2766513526439667\n",
      "Epoch 840 | Loss: 0.2764105796813965\n",
      "Epoch 850 | Loss: 0.2761799693107605\n",
      "Epoch 860 | Loss: 0.2759576439857483\n",
      "Epoch 870 | Loss: 0.2757435441017151\n",
      "Epoch 880 | Loss: 0.2755347490310669\n",
      "Epoch 890 | Loss: 0.27533242106437683\n",
      "Epoch 900 | Loss: 0.27513664960861206\n",
      "Epoch 910 | Loss: 0.27494463324546814\n",
      "Epoch 920 | Loss: 0.27475401759147644\n",
      "Epoch 930 | Loss: 0.274565726518631\n",
      "Epoch 940 | Loss: 0.27437999844551086\n",
      "Epoch 950 | Loss: 0.2741951644420624\n",
      "Epoch 960 | Loss: 0.274008184671402\n",
      "Epoch 970 | Loss: 0.27382203936576843\n",
      "Epoch 980 | Loss: 0.2736363708972931\n",
      "Epoch 990 | Loss: 0.27345117926597595\n"
     ]
    }
   ],
   "source": [
    "custom_gnn_model_2 = custom_GCN(data)\n",
    "optimizer_custom_gnn_model_2 = torch.optim.Adam(custom_gnn_model_2.parameters(), lr=0.01)\n",
    "\n",
    "fairness=True\n",
    "alpha, beta, gamma, delta = 0.1, 0.1, 0.1, 0.1\n",
    "\n",
    "# Train the model: Custom MP GNN, FAIR cross-entropy loss\n",
    "training(model=custom_gnn_model_2, \n",
    "         data=data, \n",
    "         optimizer=optimizer_custom_gnn_model_2, \n",
    "         fairness=fairness, \n",
    "         alpha=alpha, \n",
    "         beta=beta, \n",
    "         gamma=gamma, \n",
    "         delta=delta, \n",
    "         epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the values for the CustomGNN model with the FAIR standard cross-entropy loss: \n",
      "Prediction Distribution: {0: 26, 1: 377}\n",
      "Privileged Prediction Distribution: {0: 8, 1: 99}\n",
      "Unprivileged Prediction Distribution: {0: 18, 1: 278}\n",
      "Privileged Positive Prediction Rate: 0.9252336621284485\n",
      "Unprivileged Positive Prediction Rate: 0.9391891956329346\n",
      "\n",
      "SPD : -0.01396\n",
      "OAED : 0.01579\n",
      "EOD : 0.00199\n",
      "\n",
      "\n",
      "Treatment Equality Difference : 0.00000\n",
      "SP_Unprivileged : 0.93919\n",
      "SP_Privileged : 0.92523\n",
      "\n",
      "\n",
      "OAED_Unprivileged : 0.79730\n",
      "OAED_Privileged : 0.81308\n",
      "EOD_Unprivileged : 0.97391\n",
      "\n",
      "\n",
      "EOD_Privileged : 0.97590\n",
      "TED_Unprivileged : 9.00000\n",
      "TED_Privileged : 9.00000\n",
      "\n",
      "\n",
      "Accuracy : 0.74074\n"
     ]
    }
   ],
   "source": [
    "# Test the second FAIR model: CustomGNN, FAIR cross-entropy loss\n",
    "print(\"Here are the values for the CustomGNN model with the FAIR standard cross-entropy loss: \")\n",
    "\n",
    "metrics_custom_gnn_model_2 = test(custom_gnn_model_2, data)\n",
    "print()\n",
    "print_metrics(metrics_custom_gnn_model_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third FAIR Model\n",
    "\n",
    "This model is an instantiation of the `custom_GCN` using the fair cross-entropy loss using ONLY large treatment equality penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Loss: 0.41327139735221863\n",
      "Epoch 10 | Loss: 0.41146597266197205\n",
      "Epoch 20 | Loss: 0.41044187545776367\n",
      "Epoch 30 | Loss: 0.40890181064605713\n",
      "Epoch 40 | Loss: 0.40478065609931946\n",
      "Epoch 50 | Loss: 0.39385733008384705\n",
      "Epoch 60 | Loss: 0.37082433700561523\n",
      "Epoch 70 | Loss: 0.33717066049575806\n",
      "Epoch 80 | Loss: 0.3074527680873871\n",
      "Epoch 90 | Loss: 0.29284733533859253\n",
      "Epoch 100 | Loss: 0.28904932737350464\n",
      "Epoch 110 | Loss: 0.30863308906555176\n",
      "Epoch 120 | Loss: 0.2879018783569336\n",
      "Epoch 130 | Loss: 0.2932957410812378\n",
      "Epoch 140 | Loss: 0.2883414924144745\n",
      "Epoch 150 | Loss: 0.28799816966056824\n",
      "Epoch 160 | Loss: 0.2869376242160797\n",
      "Epoch 170 | Loss: 0.28620851039886475\n",
      "Epoch 180 | Loss: 0.28568848967552185\n",
      "Epoch 190 | Loss: 0.28518587350845337\n",
      "Epoch 200 | Loss: 0.28477051854133606\n",
      "Epoch 210 | Loss: 0.2844032347202301\n",
      "Epoch 220 | Loss: 0.2840660810470581\n",
      "Epoch 230 | Loss: 0.28375518321990967\n",
      "Epoch 240 | Loss: 0.2834884524345398\n",
      "Epoch 250 | Loss: 0.28326350450515747\n",
      "Epoch 260 | Loss: 0.2830566465854645\n",
      "Epoch 270 | Loss: 0.2828565239906311\n",
      "Epoch 280 | Loss: 0.2826594114303589\n",
      "Epoch 290 | Loss: 0.2824653387069702\n",
      "Epoch 300 | Loss: 0.2822738587856293\n",
      "Epoch 310 | Loss: 0.282084584236145\n",
      "Epoch 320 | Loss: 0.2818969190120697\n",
      "Epoch 330 | Loss: 0.2817107141017914\n",
      "Epoch 340 | Loss: 0.28152555227279663\n",
      "Epoch 350 | Loss: 0.2813416123390198\n",
      "Epoch 360 | Loss: 0.28115856647491455\n",
      "Epoch 370 | Loss: 0.2809765040874481\n",
      "Epoch 380 | Loss: 0.28079527616500854\n",
      "Epoch 390 | Loss: 0.2806151807308197\n",
      "Epoch 400 | Loss: 0.2804355323314667\n",
      "Epoch 410 | Loss: 0.28025659918785095\n",
      "Epoch 420 | Loss: 0.2800782322883606\n",
      "Epoch 430 | Loss: 0.27990031242370605\n",
      "Epoch 440 | Loss: 0.2797231078147888\n",
      "Epoch 450 | Loss: 0.2795464098453522\n",
      "Epoch 460 | Loss: 0.2793700397014618\n",
      "Epoch 470 | Loss: 0.279194176197052\n",
      "Epoch 480 | Loss: 0.27901849150657654\n",
      "Epoch 490 | Loss: 0.27884307503700256\n",
      "Epoch 500 | Loss: 0.2786678373813629\n",
      "Epoch 510 | Loss: 0.27849265933036804\n",
      "Epoch 520 | Loss: 0.2783174514770508\n",
      "Epoch 530 | Loss: 0.27814221382141113\n",
      "Epoch 540 | Loss: 0.2779667377471924\n",
      "Epoch 550 | Loss: 0.2777910530567169\n",
      "Epoch 560 | Loss: 0.2776149809360504\n",
      "Epoch 570 | Loss: 0.2774384617805481\n",
      "Epoch 580 | Loss: 0.2772613763809204\n",
      "Epoch 590 | Loss: 0.2770836353302002\n",
      "Epoch 600 | Loss: 0.2769051790237427\n",
      "Epoch 610 | Loss: 0.2767258584499359\n",
      "Epoch 620 | Loss: 0.27654555439949036\n",
      "Epoch 630 | Loss: 0.276364266872406\n",
      "Epoch 640 | Loss: 0.27618175745010376\n",
      "Epoch 650 | Loss: 0.27599793672561646\n",
      "Epoch 660 | Loss: 0.27581265568733215\n",
      "Epoch 670 | Loss: 0.2756258547306061\n",
      "Epoch 680 | Loss: 0.27543744444847107\n",
      "Epoch 690 | Loss: 0.275247186422348\n",
      "Epoch 700 | Loss: 0.2750551104545593\n",
      "Epoch 710 | Loss: 0.27486103773117065\n",
      "Epoch 720 | Loss: 0.2746647894382477\n",
      "Epoch 730 | Loss: 0.27446624636650085\n",
      "Epoch 740 | Loss: 0.2742654085159302\n",
      "Epoch 750 | Loss: 0.27406203746795654\n",
      "Epoch 760 | Loss: 0.27385595440864563\n",
      "Epoch 770 | Loss: 0.27364709973335266\n",
      "Epoch 780 | Loss: 0.2734352946281433\n",
      "Epoch 790 | Loss: 0.27322039008140564\n",
      "Epoch 800 | Loss: 0.2730022072792053\n",
      "Epoch 810 | Loss: 0.2727806270122528\n",
      "Epoch 820 | Loss: 0.272555410861969\n",
      "Epoch 830 | Loss: 0.27232640981674194\n",
      "Epoch 840 | Loss: 0.27209338545799255\n",
      "Epoch 850 | Loss: 0.27185624837875366\n",
      "Epoch 860 | Loss: 0.2716147005558014\n",
      "Epoch 870 | Loss: 0.27136853337287903\n",
      "Epoch 880 | Loss: 0.2711174786090851\n",
      "Epoch 890 | Loss: 0.27086132764816284\n",
      "Epoch 900 | Loss: 0.2705997824668884\n",
      "Epoch 910 | Loss: 0.27033257484436035\n",
      "Epoch 920 | Loss: 0.2700594365596771\n",
      "Epoch 930 | Loss: 0.2697800099849701\n",
      "Epoch 940 | Loss: 0.26948240399360657\n",
      "Epoch 950 | Loss: 0.2691478729248047\n",
      "Epoch 960 | Loss: 0.26879826188087463\n",
      "Epoch 970 | Loss: 0.26841649413108826\n",
      "Epoch 980 | Loss: 0.2678961753845215\n",
      "Epoch 990 | Loss: 0.26730647683143616\n"
     ]
    }
   ],
   "source": [
    "custom_gnn_model_3 = custom_GCN(data)\n",
    "optimizer_custom_gnn_model_3 = torch.optim.Adam(custom_gnn_model_3.parameters(), lr=0.01)\n",
    "\n",
    "fairness=True\n",
    "alpha, beta, gamma, delta = 0, 0.4, 0, 0\n",
    "\n",
    "# Train the model: Custom MP GNN, FAIR cross-entropy loss\n",
    "training(model=custom_gnn_model_3, \n",
    "         data=data, \n",
    "         optimizer=optimizer_custom_gnn_model_3, \n",
    "         fairness=fairness, \n",
    "         alpha=alpha, \n",
    "         beta=beta, \n",
    "         gamma=gamma, \n",
    "         delta=delta, \n",
    "         epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the values for the CustomGNN model with the FAIR standard cross-entropy loss with LARGE TE penalty: \n",
      "Prediction Distribution: {0: 25, 1: 378}\n",
      "Privileged Prediction Distribution: {0: 7, 1: 100}\n",
      "Unprivileged Prediction Distribution: {0: 18, 1: 278}\n",
      "Privileged Positive Prediction Rate: 0.9345794320106506\n",
      "Unprivileged Positive Prediction Rate: 0.9391891956329346\n",
      "\n",
      "SPD : -0.00461\n",
      "OAED : 0.02513\n",
      "EOD : 0.01404\n",
      "\n",
      "\n",
      "Treatment Equality Difference : -9.00000\n",
      "SP_Unprivileged : 0.93919\n",
      "SP_Privileged : 0.93458\n",
      "\n",
      "\n",
      "OAED_Unprivileged : 0.79730\n",
      "OAED_Privileged : 0.82243\n",
      "EOD_Unprivileged : 0.97391\n",
      "\n",
      "\n",
      "EOD_Privileged : 0.98795\n",
      "TED_Unprivileged : 9.00000\n",
      "TED_Privileged : 18.00000\n",
      "\n",
      "\n",
      "Accuracy : 0.74074\n"
     ]
    }
   ],
   "source": [
    "# Test the third FAIR model: CustomGNN, FAIR cross-entropy loss with LARGE TE penalty\n",
    "print(\"Here are the values for the CustomGNN model with the FAIR standard cross-entropy loss with LARGE TE penalty: \")\n",
    "\n",
    "metrics_custom_gnn_model_3 = test(custom_gnn_model_3, data)\n",
    "print()\n",
    "print_metrics(metrics_custom_gnn_model_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention-based Message Passing\n",
    "\n",
    "In this section, the models are trained using a custom attention-based message passing model.  \n",
    "This custom attention should take the sensitive attribute into consideration when calculating the attention weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GATConv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline GAT Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAT class that takes in the data as an input for dimensions of the convolutions\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, x, edge_index):\n",
    "        super(GAT, self).__init__()\n",
    "        self.conv1 = GATConv(data.num_node_features, 16)\n",
    "        self.conv2 = GATConv(16, 2) # 2 output classes for gender\n",
    "\n",
    "    def forward(self, x , edge_index, *args, **kwargs):\n",
    "        # x, edge_index = data.x, data.edge_index\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model, define loss function and optimizer\n",
    "gat_model = GAT(data.x, data.edge_index)\n",
    "gat_optimizer = torch.optim.Adam(gat_model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Loss: 7.0629987716674805\n",
      "Epoch 10 | Loss: 0.5814804434776306\n",
      "Epoch 20 | Loss: 0.5734091997146606\n",
      "Epoch 30 | Loss: 0.5857208371162415\n",
      "Epoch 40 | Loss: 0.5447983145713806\n",
      "Epoch 50 | Loss: 0.5309887528419495\n",
      "Epoch 60 | Loss: 0.5218015313148499\n",
      "Epoch 70 | Loss: 0.5178550481796265\n",
      "Epoch 80 | Loss: 0.512703537940979\n",
      "Epoch 90 | Loss: 0.5083928108215332\n",
      "Epoch 100 | Loss: 0.5040437579154968\n",
      "Epoch 110 | Loss: 0.49939557909965515\n",
      "Epoch 120 | Loss: 0.4948519170284271\n",
      "Epoch 130 | Loss: 0.48780879378318787\n",
      "Epoch 140 | Loss: 0.48294326663017273\n",
      "Epoch 150 | Loss: 0.47700750827789307\n",
      "Epoch 160 | Loss: 0.4725720286369324\n",
      "Epoch 170 | Loss: 0.4689479470252991\n",
      "Epoch 180 | Loss: 0.46532031893730164\n",
      "Epoch 190 | Loss: 0.4622240364551544\n",
      "Epoch 200 | Loss: 0.4589582085609436\n",
      "Epoch 210 | Loss: 0.4562106728553772\n",
      "Epoch 220 | Loss: 0.45351916551589966\n",
      "Epoch 230 | Loss: 0.45060938596725464\n",
      "Epoch 240 | Loss: 0.4476146399974823\n",
      "Epoch 250 | Loss: 0.4446745216846466\n",
      "Epoch 260 | Loss: 0.44202351570129395\n",
      "Epoch 270 | Loss: 0.43933555483818054\n",
      "Epoch 280 | Loss: 0.4363672435283661\n",
      "Epoch 290 | Loss: 0.43373075127601624\n",
      "Epoch 300 | Loss: 0.4312187433242798\n",
      "Epoch 310 | Loss: 0.4285498261451721\n",
      "Epoch 320 | Loss: 0.4279206693172455\n",
      "Epoch 330 | Loss: 0.4325442910194397\n",
      "Epoch 340 | Loss: 0.4311787784099579\n",
      "Epoch 350 | Loss: 0.4233439564704895\n",
      "Epoch 360 | Loss: 0.4282761216163635\n",
      "Epoch 370 | Loss: 0.4223444163799286\n",
      "Epoch 380 | Loss: 0.4159405529499054\n",
      "Epoch 390 | Loss: 0.4156550168991089\n",
      "Epoch 400 | Loss: 0.412644624710083\n",
      "Epoch 410 | Loss: 0.41945719718933105\n",
      "Epoch 420 | Loss: 0.4099849462509155\n",
      "Epoch 430 | Loss: 0.4261011779308319\n",
      "Epoch 440 | Loss: 0.41696232557296753\n",
      "Epoch 450 | Loss: 0.41641634702682495\n",
      "Epoch 460 | Loss: 0.4071241617202759\n",
      "Epoch 470 | Loss: 0.40496182441711426\n",
      "Epoch 480 | Loss: 0.40333083271980286\n",
      "Epoch 490 | Loss: 0.3993317484855652\n",
      "Epoch 500 | Loss: 0.4070075452327728\n",
      "Epoch 510 | Loss: 0.5912795066833496\n",
      "Epoch 520 | Loss: 1.0014543533325195\n",
      "Epoch 530 | Loss: 0.602737545967102\n",
      "Epoch 540 | Loss: 0.44604596495628357\n",
      "Epoch 550 | Loss: 0.4556429386138916\n",
      "Epoch 560 | Loss: 0.43775805830955505\n",
      "Epoch 570 | Loss: 0.43040043115615845\n",
      "Epoch 580 | Loss: 0.42787662148475647\n",
      "Epoch 590 | Loss: 0.42352592945098877\n",
      "Epoch 600 | Loss: 0.421044260263443\n",
      "Epoch 610 | Loss: 0.41841912269592285\n",
      "Epoch 620 | Loss: 0.4150012135505676\n",
      "Epoch 630 | Loss: 0.4093281030654907\n",
      "Epoch 640 | Loss: 0.40406420826911926\n",
      "Epoch 650 | Loss: 0.40064120292663574\n",
      "Epoch 660 | Loss: 0.3977145552635193\n",
      "Epoch 670 | Loss: 0.3953896760940552\n",
      "Epoch 680 | Loss: 0.3935041129589081\n",
      "Epoch 690 | Loss: 0.3919096887111664\n",
      "Epoch 700 | Loss: 0.39044955372810364\n",
      "Epoch 710 | Loss: 0.38916099071502686\n",
      "Epoch 720 | Loss: 0.38795003294944763\n",
      "Epoch 730 | Loss: 0.3867884874343872\n",
      "Epoch 740 | Loss: 0.3857479691505432\n",
      "Epoch 750 | Loss: 0.3848387598991394\n",
      "Epoch 760 | Loss: 0.38398411870002747\n",
      "Epoch 770 | Loss: 0.3831152319908142\n",
      "Epoch 780 | Loss: 0.3821692168712616\n",
      "Epoch 790 | Loss: 0.38084831833839417\n",
      "Epoch 800 | Loss: 0.3797358274459839\n",
      "Epoch 810 | Loss: 0.3785247206687927\n",
      "Epoch 820 | Loss: 0.37710118293762207\n",
      "Epoch 830 | Loss: 0.37616443634033203\n",
      "Epoch 840 | Loss: 0.37530040740966797\n",
      "Epoch 850 | Loss: 0.3744867444038391\n",
      "Epoch 860 | Loss: 0.3739614188671112\n",
      "Epoch 870 | Loss: 0.37359631061553955\n",
      "Epoch 880 | Loss: 0.37253740429878235\n",
      "Epoch 890 | Loss: 0.3774714469909668\n",
      "Epoch 900 | Loss: 0.3858838677406311\n",
      "Epoch 910 | Loss: 0.3804256021976471\n",
      "Epoch 920 | Loss: 0.3751595914363861\n",
      "Epoch 930 | Loss: 0.37110406160354614\n",
      "Epoch 940 | Loss: 0.3669263422489166\n",
      "Epoch 950 | Loss: 0.3706047832965851\n",
      "Epoch 960 | Loss: 0.3647044003009796\n",
      "Epoch 970 | Loss: 0.36673930287361145\n",
      "Epoch 980 | Loss: 0.3696480393409729\n",
      "Epoch 990 | Loss: 0.36297380924224854\n"
     ]
    }
   ],
   "source": [
    "training(model=gat_model, data=data, optimizer=gat_optimizer, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the values for the standard GAT model with the standard cross-entropy loss: \n",
      "Prediction Distribution: {0: 259, 1: 144}\n",
      "Privileged Prediction Distribution: {0: 71, 1: 36}\n",
      "Unprivileged Prediction Distribution: {0: 188, 1: 108}\n",
      "Privileged Positive Prediction Rate: 0.336448609828949\n",
      "Unprivileged Positive Prediction Rate: 0.36486485600471497\n",
      "\n",
      "SPD : -0.02842\n",
      "OAED : -0.01822\n",
      "EOD : -0.04748\n",
      "\n",
      "\n",
      "Treatment Equality Difference : -0.01984\n",
      "SP_Unprivileged : 0.36486\n",
      "SP_Privileged : 0.33645\n",
      "\n",
      "\n",
      "OAED_Unprivileged : 0.79392\n",
      "OAED_Privileged : 0.77570\n",
      "EOD_Unprivileged : 0.69748\n",
      "\n",
      "\n",
      "EOD_Privileged : 0.65000\n",
      "TED_Unprivileged : 0.69444\n",
      "TED_Privileged : 0.71429\n",
      "\n",
      "\n",
      "Accuracy : 0.71605\n"
     ]
    }
   ],
   "source": [
    "# Test the first model: GAT, standard data, cross-entropy loss\n",
    "print(\"Here are the values for the standard GAT model with the standard cross-entropy loss: \")\n",
    "\n",
    "metrics_base_gat_model = test(gat_model, data)\n",
    "print()\n",
    "print_metrics(metrics_base_gat_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FairMP GAT Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops\n",
    "\n",
    "class Attention_FairMessagePassing(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Attention_FairMessagePassing, self).__init__(aggr='mean')\n",
    "        self.lin = nn.Linear(in_channels, out_channels)\n",
    "        self.att = nn.Linear(out_channels, 1)\n",
    "        self.sensitive_attr = torch.tensor(user_labels['country'].values, dtype=torch.float)\n",
    "        self.bias_correction = nn.Parameter(torch.rand(1))\n",
    "        # self.a_fair = nn.Parameter(torch.rand(out_channels))\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # Add self-loops\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "\n",
    "        x = self.lin(x)\n",
    "        x = self.propagate(edge_index, size=(x.size(0), x.size(0)), x=x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def message(self, x_i, x_j):\n",
    "        # Calculate attention weights\n",
    "        alpha = self.att(torch.abs(x_i - x_j))\n",
    "        alpha = torch.exp(alpha) / (torch.exp(alpha).sum(dim=1, keepdim=True) + self.bias_correction)\n",
    "        \n",
    "        # Apply attention weights to messages\n",
    "        return x_j * alpha\n",
    "\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        # return aggr_out * self.a_fair\n",
    "        return aggr_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAT class that takes in the data as an input for dimensions of the convolutions\n",
    "class CustomGAT(torch.nn.Module):\n",
    "    def __init__(self, x, edge_index):\n",
    "        super(CustomGAT, self).__init__()\n",
    "        self.conv1 = Attention_FairMessagePassing(data.num_node_features, 16)\n",
    "        self.conv2 = Attention_FairMessagePassing(16, 2) # 2 output classes for gender\n",
    "\n",
    "    def forward(self, x , edge_index, *args, **kwargs):\n",
    "        # x, edge_index = data.x, data.edge_index\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First Fair GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model, define loss function and optimizer\n",
    "custom_gat_model = CustomGAT(data.x, data.edge_index)\n",
    "custom_gat_optimizer = torch.optim.Adam(custom_gat_model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Loss: 0.624671459197998\n",
      "Epoch 10 | Loss: 0.5139187574386597\n",
      "Epoch 20 | Loss: 0.5238528251647949\n",
      "Epoch 30 | Loss: 0.5085595846176147\n",
      "Epoch 40 | Loss: 0.5023036003112793\n",
      "Epoch 50 | Loss: 0.4943685531616211\n",
      "Epoch 60 | Loss: 0.48887965083122253\n",
      "Epoch 70 | Loss: 0.48474106192588806\n",
      "Epoch 80 | Loss: 0.4806733727455139\n",
      "Epoch 90 | Loss: 0.47654473781585693\n",
      "Epoch 100 | Loss: 0.4720362722873688\n",
      "Epoch 110 | Loss: 0.46712276339530945\n",
      "Epoch 120 | Loss: 0.46174269914627075\n",
      "Epoch 130 | Loss: 0.4558955132961273\n",
      "Epoch 140 | Loss: 0.4493055045604706\n",
      "Epoch 150 | Loss: 0.44156286120414734\n",
      "Epoch 160 | Loss: 0.4320017695426941\n",
      "Epoch 170 | Loss: 0.41921907663345337\n",
      "Epoch 180 | Loss: 0.40207117795944214\n",
      "Epoch 190 | Loss: 0.37890517711639404\n",
      "Epoch 200 | Loss: 0.3464687764644623\n",
      "Epoch 210 | Loss: 0.301862895488739\n",
      "Epoch 220 | Loss: 0.24449342489242554\n",
      "Epoch 230 | Loss: 0.18289054930210114\n",
      "Epoch 240 | Loss: 0.13241472840309143\n",
      "Epoch 250 | Loss: 0.10094685107469559\n",
      "Epoch 260 | Loss: 0.074183389544487\n",
      "Epoch 270 | Loss: 0.050613775849342346\n",
      "Epoch 280 | Loss: 0.03559025004506111\n",
      "Epoch 290 | Loss: 0.03565707802772522\n",
      "Epoch 300 | Loss: 0.025351403281092644\n",
      "Epoch 310 | Loss: 0.01956978440284729\n",
      "Epoch 320 | Loss: 0.016487449407577515\n",
      "Epoch 330 | Loss: 0.01463729701936245\n",
      "Epoch 340 | Loss: 0.01343532931059599\n",
      "Epoch 350 | Loss: 0.012611695565283298\n",
      "Epoch 360 | Loss: 0.011968011036515236\n",
      "Epoch 370 | Loss: 0.01142258383333683\n",
      "Epoch 380 | Loss: 0.01097068190574646\n",
      "Epoch 390 | Loss: 0.010602585971355438\n",
      "Epoch 400 | Loss: 0.010308744385838509\n",
      "Epoch 410 | Loss: 0.010070495307445526\n",
      "Epoch 420 | Loss: 0.00986666139215231\n",
      "Epoch 430 | Loss: 0.009690383449196815\n",
      "Epoch 440 | Loss: 0.009538284502923489\n",
      "Epoch 450 | Loss: 0.009402907453477383\n",
      "Epoch 460 | Loss: 0.009280567988753319\n",
      "Epoch 470 | Loss: 0.00917461235076189\n",
      "Epoch 480 | Loss: 0.009074763394892216\n",
      "Epoch 490 | Loss: 0.008984380401670933\n",
      "Epoch 500 | Loss: 0.00890345685184002\n",
      "Epoch 510 | Loss: 0.008827565237879753\n",
      "Epoch 520 | Loss: 0.008761376142501831\n",
      "Epoch 530 | Loss: 0.008693906478583813\n",
      "Epoch 540 | Loss: 0.008634121157228947\n",
      "Epoch 550 | Loss: 0.008576394990086555\n",
      "Epoch 560 | Loss: 0.008522064425051212\n",
      "Epoch 570 | Loss: 0.008471877314150333\n",
      "Epoch 580 | Loss: 0.008425621315836906\n",
      "Epoch 590 | Loss: 0.008380325511097908\n",
      "Epoch 600 | Loss: 0.008339179679751396\n",
      "Epoch 610 | Loss: 0.008297390304505825\n",
      "Epoch 620 | Loss: 0.00825911108404398\n",
      "Epoch 630 | Loss: 0.008222172036767006\n",
      "Epoch 640 | Loss: 0.008187197148799896\n",
      "Epoch 650 | Loss: 0.008153584785759449\n",
      "Epoch 660 | Loss: 0.008120245300233364\n",
      "Epoch 670 | Loss: 0.008088914677500725\n",
      "Epoch 680 | Loss: 0.008057944476604462\n",
      "Epoch 690 | Loss: 0.008028288371860981\n",
      "Epoch 700 | Loss: 0.007999734953045845\n",
      "Epoch 710 | Loss: 0.007971474900841713\n",
      "Epoch 720 | Loss: 0.0079440763220191\n",
      "Epoch 730 | Loss: 0.007917800918221474\n",
      "Epoch 740 | Loss: 0.007890287786722183\n",
      "Epoch 750 | Loss: 0.007863761857151985\n",
      "Epoch 760 | Loss: 0.0078378701582551\n",
      "Epoch 770 | Loss: 0.007814420387148857\n",
      "Epoch 780 | Loss: 0.0077878376469016075\n",
      "Epoch 790 | Loss: 0.007764309179037809\n",
      "Epoch 800 | Loss: 0.007739413063973188\n",
      "Epoch 810 | Loss: 0.007715137675404549\n",
      "Epoch 820 | Loss: 0.007691653445363045\n",
      "Epoch 830 | Loss: 0.007667745929211378\n",
      "Epoch 840 | Loss: 0.007643767166882753\n",
      "Epoch 850 | Loss: 0.007620178163051605\n",
      "Epoch 860 | Loss: 0.007595754694193602\n",
      "Epoch 870 | Loss: 0.007569734938442707\n",
      "Epoch 880 | Loss: 0.007546983193606138\n",
      "Epoch 890 | Loss: 0.007522978354245424\n",
      "Epoch 900 | Loss: 0.007499787490814924\n",
      "Epoch 910 | Loss: 0.007477490697056055\n",
      "Epoch 920 | Loss: 0.0074552688747644424\n",
      "Epoch 930 | Loss: 0.007434200029820204\n",
      "Epoch 940 | Loss: 0.007412075065076351\n",
      "Epoch 950 | Loss: 0.007391125429421663\n",
      "Epoch 960 | Loss: 0.0073711806908249855\n",
      "Epoch 970 | Loss: 0.007350400555878878\n",
      "Epoch 980 | Loss: 0.0073303841054439545\n",
      "Epoch 990 | Loss: 0.0073095872066915035\n"
     ]
    }
   ],
   "source": [
    "training(model=custom_gat_model, data=data, optimizer=custom_gat_optimizer, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the values for the CustomGAT model with the standard cross-entropy loss: \n",
      "Prediction Distribution: {0: 94, 1: 309}\n",
      "Privileged Prediction Distribution: {0: 26, 1: 81}\n",
      "Unprivileged Prediction Distribution: {0: 68, 1: 228}\n",
      "Privileged Positive Prediction Rate: 0.7570093274116516\n",
      "Unprivileged Positive Prediction Rate: 0.7702702879905701\n",
      "\n",
      "SPD : -0.01326\n",
      "OAED : 0.00474\n",
      "EOD : -0.00471\n",
      "\n",
      "\n",
      "Treatment Equality Difference : 0.30000\n",
      "SP_Unprivileged : 0.77027\n",
      "SP_Privileged : 0.75701\n",
      "\n",
      "\n",
      "OAED_Unprivileged : 0.93919\n",
      "OAED_Privileged : 0.94393\n",
      "EOD_Unprivileged : 0.95652\n",
      "\n",
      "\n",
      "EOD_Privileged : 0.95181\n",
      "TED_Unprivileged : 0.80000\n",
      "TED_Privileged : 0.50000\n",
      "\n",
      "\n",
      "Accuracy : 0.72840\n"
     ]
    }
   ],
   "source": [
    "# Test the first model: CustomGAT, standard data, cross-entropy loss\n",
    "print(\"Here are the values for the CustomGAT model with the standard cross-entropy loss: \")\n",
    "\n",
    "metrics_custom_gat_model = test(custom_gat_model, data)\n",
    "print()\n",
    "print_metrics(metrics_custom_gat_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the values for the CustomGAT model with the standard cross-entropy loss: \n",
      "Prediction Distribution: {0: 250, 1: 153}\n",
      "Privileged Prediction Distribution: {0: 67, 1: 40}\n",
      "Unprivileged Prediction Distribution: {0: 183, 1: 113}\n",
      "Privileged Positive Prediction Rate: 0.37383177876472473\n",
      "Unprivileged Positive Prediction Rate: 0.3817567527294159\n",
      "\n",
      "SPD : -0.00792\n",
      "OAED : 0.03018\n",
      "EOD : 0.05924\n",
      "\n",
      "\n",
      "Treatment Equality Difference : -0.46154\n",
      "SP_Unprivileged : 0.38176\n",
      "SP_Privileged : 0.37383\n",
      "\n",
      "\n",
      "OAED_Unprivileged : 0.93243\n",
      "OAED_Privileged : 0.96262\n",
      "EOD_Unprivileged : 0.89076\n",
      "\n",
      "\n",
      "EOD_Privileged : 0.95000\n",
      "TED_Unprivileged : 0.53846\n",
      "TED_Privileged : 1.00000\n",
      "\n",
      "\n",
      "Accuracy : 0.72840\n"
     ]
    }
   ],
   "source": [
    "# Test the first model: CustomGAT, standard data, cross-entropy loss\n",
    "print(\"Here are the values for the CustomGAT model with the standard cross-entropy loss: \")\n",
    "\n",
    "metrics_custom_gat_model = test(custom_gat_model, data)\n",
    "print()\n",
    "print_metrics(metrics_custom_gat_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPD : -0.02842\n",
    "OAED : -0.01822\n",
    "EOD : -0.04748\n",
    "\n",
    "\n",
    "Treatment Equality Difference : -0.01984\n",
    "SP_Unprivileged : 0.36486\n",
    "SP_Privileged : 0.33645\n",
    "\n",
    "\n",
    "OAED_Unprivileged : 0.79392\n",
    "OAED_Privileged : 0.77570\n",
    "EOD_Unprivileged : 0.69748\n",
    "\n",
    "\n",
    "EOD_Privileged : 0.65000\n",
    "TED_Unprivileged : 0.69444\n",
    "TED_Privileged : 0.71429\n",
    "\n",
    "\n",
    "Accuracy : 0.71605"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second Fair GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model, define loss function and optimizer\n",
    "custom_gat_model_2 = CustomGAT(data.x, edge_index)\n",
    "custom_gat_optimizer_2 = torch.optim.Adam(custom_gat_model_2.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Loss: 2.773331642150879\n",
      "Epoch 10 | Loss: 0.31782299280166626\n",
      "Epoch 20 | Loss: 0.3180868625640869\n",
      "Epoch 30 | Loss: 0.29893550276756287\n",
      "Epoch 40 | Loss: 0.28631526231765747\n",
      "Epoch 50 | Loss: 0.28105273842811584\n",
      "Epoch 60 | Loss: 0.2770257592201233\n",
      "Epoch 70 | Loss: 0.27365851402282715\n",
      "Epoch 80 | Loss: 0.2707068622112274\n",
      "Epoch 90 | Loss: 0.26821669936180115\n"
     ]
    }
   ],
   "source": [
    "training(model=custom_gat_model_2, data=data, optimizer=custom_gat_optimizer_2, fairness=True, alpha=0.1, beta=0.1, gamma=0.1, delta=0.1, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the values for the CustomGAT model with the FAIR cross-entropy loss: \n",
      "Prediction Distribution: {0: 26, 1: 377}\n",
      "Privileged Prediction Distribution: {0: 7, 1: 100}\n",
      "Unprivileged Prediction Distribution: {0: 19, 1: 277}\n",
      "Privileged Positive Prediction Rate: 0.9345794320106506\n",
      "Unprivileged Positive Prediction Rate: 0.9358108043670654\n",
      "SPD : -0.00123\n",
      "OAED : 0.01658\n",
      "EOD : 0.01069\n",
      "\n",
      "\n",
      "Treatment Equality Difference : -2.62500\n",
      "SP_Unprivileged : 0.93581\n",
      "SP_Privileged : 0.93458\n",
      "\n",
      "\n",
      "OAED_Unprivileged : 0.78716\n",
      "OAED_Privileged : 0.80374\n",
      "EOD_Unprivileged : 0.96522\n",
      "\n",
      "\n",
      "EOD_Privileged : 0.97590\n",
      "TED_Unprivileged : 6.87500\n",
      "TED_Privileged : 9.50000\n",
      "\n",
      "\n",
      "Accuracy : 0.71605\n"
     ]
    }
   ],
   "source": [
    "# Test the second model: CustomGAT, FAIR cross-entropy loss\n",
    "print(\"Here are the values for the CustomGAT model with the FAIR cross-entropy loss: \")\n",
    "\n",
    "metrics_custom_gat_model_2 = test(custom_gat_model_2, data)\n",
    "\n",
    "print_metrics(metrics_custom_gat_model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the values for the CustomGAT model with the FAIR cross-entropy loss: \n",
      "Prediction Distribution: {0: 245, 1: 158}\n",
      "Privileged Prediction Distribution: {0: 68, 1: 39}\n",
      "Unprivileged Prediction Distribution: {0: 177, 1: 119}\n",
      "Privileged Positive Prediction Rate: 0.3644859790802002\n",
      "Unprivileged Positive Prediction Rate: 0.40202704071998596\n",
      "SPD : -0.03754\n",
      "OAED : 0.00733\n",
      "EOD : -0.00777\n",
      "\n",
      "\n",
      "Treatment Equality Difference : 0.33333\n",
      "SP_Unprivileged : 0.40203\n",
      "SP_Privileged : 0.36449\n",
      "\n",
      "\n",
      "OAED_Unprivileged : 0.94595\n",
      "OAED_Privileged : 0.95327\n",
      "EOD_Unprivileged : 0.93277\n",
      "\n",
      "\n",
      "EOD_Privileged : 0.92500\n",
      "TED_Unprivileged : 1.00000\n",
      "TED_Privileged : 0.66667\n",
      "\n",
      "\n",
      "Accuracy : 0.81481\n"
     ]
    }
   ],
   "source": [
    "# Test the second model: CustomGAT, FAIR cross-entropy loss\n",
    "print(\"Here are the values for the CustomGAT model with the FAIR cross-entropy loss: \")\n",
    "\n",
    "metrics_custom_gat_model_2 = test(custom_gat_model_2, data)\n",
    "\n",
    "print_metrics(metrics_custom_gat_model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPD : -0.02842\n",
    "OAED : -0.01822\n",
    "EOD : -0.04748\n",
    "\n",
    "\n",
    "Treatment Equality Difference : -0.01984\n",
    "SP_Unprivileged : 0.36486\n",
    "SP_Privileged : 0.33645\n",
    "\n",
    "\n",
    "OAED_Unprivileged : 0.79392\n",
    "OAED_Privileged : 0.77570\n",
    "EOD_Unprivileged : 0.69748\n",
    "\n",
    "\n",
    "EOD_Privileged : 0.65000\n",
    "TED_Unprivileged : 0.69444\n",
    "TED_Privileged : 0.71429\n",
    "\n",
    "\n",
    "Accuracy : 0.71605"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
