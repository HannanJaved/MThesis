{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "q_AV3JhlZf00",
        "VJBgMHhB0W-I",
        "Nlx4iq4S0auM",
        "k3Y-A-g50evv",
        "0oTGYnLz0nqq",
        "71oC_W9UZiND",
        "oVZzUZlfbSui",
        "eYc3PvX715v3",
        "bIXj8cPkMa_g",
        "98ygJtxSMfLr",
        "hJQ7eKEtMj8y",
        "n1BLAdefMmlB",
        "yi1s71rrDQH9",
        "LJJv17nM1xsH",
        "p8DljQFY98sL",
        "YPV13PBARHQ8",
        "hJ6_kBZqbwbC",
        "IviJbqMF_iEH",
        "3pAVsiTD_jOV",
        "1zlTkMUwDiDP",
        "rvho1NeAdEFt",
        "pJ-BAyYv-J8h",
        "PbQBmxjN1pqw"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fairness in GCNs\n",
        "In this python notebook, we have explored the use of Graph Convolutional Networks(GCNs) for the [Alibaba](https://tianchi.aliyun.com/dataset/56) dataset.\n",
        "\n",
        "The dataset was pre-processed using the code from Erasmo Purificato's [CatGCN notebook](https://colab.research.google.com/drive/1zsx4an6BKYhJ_UT-mSl1_qPB-zyjTmrA#scrollTo=xxzSlLj3LDIu).  \n",
        "This pre-processing provided us with various .csv files, which are used to form graph data.  \n",
        "\n",
        "The nodes represent the user ids, with the node features being attributes such as buy, gender, student, etc. The edges between the nodes have been created through various relations between the users such as items bought, items clicked on, etc.  \n",
        "\n",
        "In this notebook, we have only focused on GCNs and used fairness methods from the [AIF360](https://github.com/Trusted-AI/AIF360) framework."
      ],
      "metadata": {
        "id": "VKUtR8NboAGh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "q_AV3JhlZf00"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install missing libraries"
      ],
      "metadata": {
        "id": "VJBgMHhB0W-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_geometric\n",
        "!pip install torch torchvision\n",
        "!pip install Blackboxauditing\n",
        "!pip install aif360"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K33_WbAtZV5-",
        "outputId": "303884ad-7a63-4d4d-f5b3-6303e1bf0ffa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.4.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.11.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2023.11.17)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.2.0)\n",
            "Installing collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.4.0\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Collecting Blackboxauditing\n",
            "  Downloading BlackBoxAuditing-0.1.54.tar.gz (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from Blackboxauditing) (3.2.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from Blackboxauditing) (3.7.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from Blackboxauditing) (1.5.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from Blackboxauditing) (1.23.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->Blackboxauditing) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->Blackboxauditing) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->Blackboxauditing) (4.47.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->Blackboxauditing) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->Blackboxauditing) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->Blackboxauditing) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->Blackboxauditing) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->Blackboxauditing) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->Blackboxauditing) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->Blackboxauditing) (1.16.0)\n",
            "Building wheels for collected packages: Blackboxauditing\n",
            "  Building wheel for Blackboxauditing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Blackboxauditing: filename=BlackBoxAuditing-0.1.54-py2.py3-none-any.whl size=1394755 sha256=4254a1bd39be76b26c1e7b3ee562920b1fa7b7b31793f46408e7e98910b2b04e\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/4f/b1/80e1b0790df07536470758fe0a4f9ff8fa942fd9fe30bbb192\n",
            "Successfully built Blackboxauditing\n",
            "Installing collected packages: Blackboxauditing\n",
            "Successfully installed Blackboxauditing-0.1.54\n",
            "Collecting aif360\n",
            "  Downloading aif360-0.5.0-py3-none-any.whl (214 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.1/214.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.10/dist-packages (from aif360) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from aif360) (1.11.4)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from aif360) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn>=1.0 in /usr/local/lib/python3.10/dist-packages (from aif360) (1.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from aif360) (3.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->aif360) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->aif360) (2023.3.post1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0->aif360) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0->aif360) (3.2.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->aif360) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->aif360) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->aif360) (4.47.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->aif360) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->aif360) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->aif360) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->aif360) (3.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=0.24.0->aif360) (1.16.0)\n",
            "Installing collected packages: aif360\n",
            "Successfully installed aif360-0.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import necessary libraries"
      ],
      "metadata": {
        "id": "Nlx4iq4S0auM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic data processing libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "\n",
        "# Graph data processing libraries\n",
        "import networkx as nx\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.utils import from_networkx\n",
        "\n",
        "# Libraries for (G)NNs\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "# AIF360\n",
        "from aif360.datasets import BinaryLabelDataset\n",
        "from aif360.algorithms.preprocessing import Reweighing\n",
        "from aif360.algorithms.preprocessing import DisparateImpactRemover"
      ],
      "metadata": {
        "id": "JHSmhafSZMnf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7652507e-e752-47c8-82d4-2140018a3988"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:No module named 'tempeh': LawSchoolGPADataset will be unavailable. To install, run:\n",
            "pip install 'aif360[LawSchoolGPA]'\n",
            "WARNING:root:No module named 'fairlearn': ExponentiatedGradientReduction will be unavailable. To install, run:\n",
            "pip install 'aif360[Reductions]'\n",
            "WARNING:root:No module named 'fairlearn': GridSearchReduction will be unavailable. To install, run:\n",
            "pip install 'aif360[Reductions]'\n",
            "WARNING:root:No module named 'fairlearn': GridSearchReduction will be unavailable. To install, run:\n",
            "pip install 'aif360[Reductions]'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mount and connect to personal Drive"
      ],
      "metadata": {
        "id": "k3Y-A-g50evv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67SSNMA4Ztzh",
        "outputId": "6517e355-da73-4b78-e31f-9808757964f1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialise paths"
      ],
      "metadata": {
        "id": "0oTGYnLz0nqq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_path = \"/content/drive/MyDrive/ColabNotebooks/Winter Semester 2023 24/HCAT\"\n",
        "raw_data_path = os.path.join(base_path, \"Dataset\", \"Alibaba\")\n",
        "catgcn_path = os.path.join(base_path, \"models\", \"CatGCN\")\n",
        "input_ali_data_path = os.path.join(catgcn_path, \"input_ali_data\")"
      ],
      "metadata": {
        "id": "-9cqZLujZzno"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Data Stuff"
      ],
      "metadata": {
        "id": "f-cz1bZUbyqg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the data from .csv files"
      ],
      "metadata": {
        "id": "71oC_W9UZiND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data files\n",
        "user_labels_path = os.path.join(input_ali_data_path, \"user_labels.csv\")\n",
        "user_edges_path = os.path.join(input_ali_data_path, \"user_edge.csv\")"
      ],
      "metadata": {
        "id": "ZkX7IkpVbMC-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataframes to store the information from the .csv files\n",
        "user_labels = pd.read_csv(user_labels_path)\n",
        "user_edges = pd.read_csv(user_edges_path)"
      ],
      "metadata": {
        "id": "ejJgu5rdw89n"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pre-processing the data"
      ],
      "metadata": {
        "id": "oVZzUZlfbSui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the data for GNNs\n",
        "node_features = torch.tensor(user_labels.iloc[:, 1:].values, dtype=torch.float)\n",
        "edge_index = torch.tensor(user_edges.values, dtype=torch.long).t().contiguous()"
      ],
      "metadata": {
        "id": "jS3qr14B2t5d"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_df_info(df):\n",
        "    print(df.info())\n",
        "    print('####### Repeat ####### \\n', df.duplicated().any())\n",
        "    print('####### Count ####### \\n', df.nunique())\n",
        "    print('####### Example ####### \\n',df.head())"
      ],
      "metadata": {
        "id": "KbmyUI5QcyKx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "node_features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDtgaVODdFit",
        "outputId": "329b730d-7780-4287-a0da-45fe6b1def0e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 1., 1.,  ..., 0., 1., 1.],\n",
              "        [0., 2., 1.,  ..., 0., 1., 1.],\n",
              "        ...,\n",
              "        [0., 0., 1.,  ..., 2., 0., 1.],\n",
              "        [0., 4., 1.,  ..., 3., 0., 1.],\n",
              "        [0., 0., 1.,  ..., 0., 0., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "show_df_info(user_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThCC75tjcN3j",
        "outputId": "8008e917-9156-42d7-a415-95de9cadf893"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 166958 entries, 0 to 166957\n",
            "Data columns (total 8 columns):\n",
            " #   Column   Non-Null Count   Dtype\n",
            "---  ------   --------------   -----\n",
            " 0   uid      166958 non-null  int64\n",
            " 1   gender   166958 non-null  int64\n",
            " 2   age      166958 non-null  int64\n",
            " 3   buy      166958 non-null  int64\n",
            " 4   student  166958 non-null  int64\n",
            " 5   city     166958 non-null  int64\n",
            " 6   bin_age  166958 non-null  int64\n",
            " 7   bin_buy  166958 non-null  int64\n",
            "dtypes: int64(8)\n",
            "memory usage: 10.2 MB\n",
            "None\n",
            "####### Repeat ####### \n",
            " False\n",
            "####### Count ####### \n",
            " uid        166958\n",
            "gender          2\n",
            "age             7\n",
            "buy             3\n",
            "student         2\n",
            "city            4\n",
            "bin_age         2\n",
            "bin_buy         2\n",
            "dtype: int64\n",
            "####### Example ####### \n",
            "    uid  gender  age  buy  student  city  bin_age  bin_buy\n",
            "0    0       0    0    0        0     0        0        0\n",
            "1    1       0    1    1        1     0        1        1\n",
            "2    2       0    2    1        1     0        1        1\n",
            "3    3       0    2    1        1     1        1        1\n",
            "4    4       0    0    1        0     0        0        1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create torch-geometric data\n",
        "data = Data(x=node_features, edge_index=edge_index)"
      ],
      "metadata": {
        "id": "mO8xfud6xbvG"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_nodes = node_features.size(0)\n",
        "num_classes = 2 # Binarised gender values from the data\n",
        "num_node_features = data.num_node_features\n",
        "\n",
        "# Create masks for training, and testing\n",
        "train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "\n",
        "# 80 - 20 Train and Test data split\n",
        "num_train = int(num_nodes * 0.8)\n",
        "train_mask[:num_train] = True\n",
        "test_mask[num_train:] = True\n",
        "\n",
        "data.train_mask = train_mask\n",
        "data.test_mask = test_mask"
      ],
      "metadata": {
        "id": "iTSWiW3lzDPc"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_nodes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_emUdZ-WdB8S",
        "outputId": "b667972f-1402-4d03-9ec4-f85633568ec4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "166958"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Labels from the data (in this case: Gender Classification)\n",
        "data.y = torch.tensor(user_labels['gender'].values, dtype=torch.long)"
      ],
      "metadata": {
        "id": "IBliHF665WBC"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Utils"
      ],
      "metadata": {
        "id": "eYc3PvX715v3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function to clone the dataset"
      ],
      "metadata": {
        "id": "bIXj8cPkMa_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clone(data):\n",
        "    \"\"\"\n",
        "    Create a new cloned torch-geometric data object.\n",
        "\n",
        "    Args:\n",
        "    data: Actual data to be cloned.\n",
        "\n",
        "    Returns:\n",
        "    A torch-geometric data object.\n",
        "    \"\"\"\n",
        "    clone_data = Data()\n",
        "\n",
        "    # Copy the data's features and edges\n",
        "    clone_data.x = data.x.clone()\n",
        "    clone_data.edge_index = data.edge_index.clone()\n",
        "\n",
        "    # Mask the data similar to the original train-test split\n",
        "    clone_data.train_mask = data.train_mask.clone()\n",
        "    clone_data.test_mask = data.test_mask.clone()\n",
        "\n",
        "    # Copy the labels\n",
        "    clone_data.y = data.y.clone()\n",
        "\n",
        "    return clone_data"
      ],
      "metadata": {
        "id": "ZtwOmxUSWU53"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom Loss Functions"
      ],
      "metadata": {
        "id": "98ygJtxSMfLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def weighted_cross_entropy(output, data):\n",
        "    \"\"\"\n",
        "    A custom loss function to calculate a weighted-cross entropy loss.\n",
        "\n",
        "    Args:\n",
        "    output: Outputs from the model.\n",
        "    data: The torch-geometric data object used for the model.\n",
        "\n",
        "    Returns:\n",
        "    A weighted cross-entropy loss.\n",
        "    \"\"\"\n",
        "    target = data.y[data.train_mask]\n",
        "    weights = data.instance_weights[data.train_mask]\n",
        "\n",
        "    loss = F.cross_entropy(output, target, reduction='none')\n",
        "    weighted_loss = loss * weights\n",
        "\n",
        "    return weighted_loss.mean()"
      ],
      "metadata": {
        "id": "ANsMDbvnV0tY"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fairness_aware_loss(output, data, sensitive_attr, weighted=False, alpha=0.01, beta=0.01, gamma=0.01, delta=0.01):\n",
        "    \"\"\"\n",
        "    Custom loss function to calculate a fairness-aware loss.\n",
        "    This includes measures for statistical parity, treatment equality, equal opportunity difference, and overall accuracy equality difference.\n",
        "\n",
        "    Args:\n",
        "    output: Outputs from the model.\n",
        "    data: The torch-geometric data object used for the model.\n",
        "    sensitive_attr: The sensitive attribute in the data (e.g., bin_age).\n",
        "    weighted: Boolean value indicating re-weighing done to the data or not.\n",
        "    alpha: Parameter for statistical parity regularizer strength.\n",
        "    beta: Parameter for treatment equality regularizer strength.\n",
        "    gamma: Parameter for equal opportunity difference regularizer strength.\n",
        "    delta: Parameter for overall accuracy equality difference regularizer strength.\n",
        "\n",
        "    Returns:\n",
        "    A fairness-aware combined loss.\n",
        "    \"\"\"\n",
        "    if weighted:\n",
        "        # Weighted cross-entropy loss\n",
        "        standard_loss = weighted_cross_entropy(output, data)\n",
        "    else:\n",
        "        # Standard cross-entropy loss\n",
        "        target = data.y[data.train_mask]\n",
        "        standard_loss = F.cross_entropy(output, target)\n",
        "\n",
        "    labels = data.y[train_mask]\n",
        "    pos_prob = torch.sigmoid(output[:, 1])\n",
        "    neg_prob = 1 - pos_prob\n",
        "    predictions = output.argmax(dim=1)\n",
        "\n",
        "    # Statistical Parity Regularization\n",
        "    sp_reg = torch.abs(pos_prob[sensitive_attr == 1].mean() - pos_prob[sensitive_attr == 0].mean())\n",
        "\n",
        "    # Treatment Equality Regularization\n",
        "    fp_diff = (neg_prob * (labels == 0) * (sensitive_attr == 1)).float().mean() - \\\n",
        "              (neg_prob * (labels == 0) * (sensitive_attr == 0)).float().mean()\n",
        "    fn_diff = (pos_prob * (labels == 1) * (sensitive_attr == 1)).float().mean() - \\\n",
        "              (pos_prob * (labels == 1) * (sensitive_attr == 0)).float().mean()\n",
        "    treatment_reg = torch.abs(fp_diff) + torch.abs(fn_diff)\n",
        "\n",
        "    # Equal Opportunity Difference Regularization\n",
        "    eod_reg = torch.abs((pos_prob * (labels == 1) * (sensitive_attr == 1)).float().mean() - \\\n",
        "                        (pos_prob * (labels == 1) * (sensitive_attr == 0)).float().mean())\n",
        "\n",
        "    # Overall Accuracy Equality Difference Regularization\n",
        "    oaed_reg = torch.abs((pos_prob * (sensitive_attr == 1)).float().mean() - \\\n",
        "                         (pos_prob * (sensitive_attr == 0)).float().mean())\n",
        "\n",
        "    # Combine losses\n",
        "    combined_loss = standard_loss + alpha * sp_reg + beta * treatment_reg + gamma * eod_reg + delta * oaed_reg\n",
        "\n",
        "    return combined_loss"
      ],
      "metadata": {
        "id": "tcBBcsJo8CMs"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fairness Metrics"
      ],
      "metadata": {
        "id": "hJQ7eKEtMj8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_fairness(label, predictions, sens_attr='bin_age'):\n",
        "    \"\"\"\n",
        "    Calculate various fairness metrics.\n",
        "\n",
        "    Args:\n",
        "    label: Actual labels (binary).\n",
        "    predictions: Model predictions (binary).\n",
        "    sens_attr: Binary sensitive attribute for fairness evaluation.\n",
        "\n",
        "    Returns:\n",
        "    A dictionary containing SPD, EOD, OAED, and TED values.\n",
        "    \"\"\"\n",
        "\n",
        "    labels = torch.tensor(user_labels[label].values, dtype=torch.long)\n",
        "    sensitive_attribute = torch.tensor(user_labels[sens_attr].values, dtype=torch.long)\n",
        "\n",
        "    predictions = predictions.float()\n",
        "    labels = labels.float()\n",
        "    sensitive_attribute = sensitive_attribute.float()\n",
        "\n",
        "    def statistical_parity_difference():\n",
        "        prob_group_1 = predictions[sensitive_attribute == 1].mean()\n",
        "        prob_group_0 = predictions[sensitive_attribute == 0].mean()\n",
        "        return abs(prob_group_1 - prob_group_0)\n",
        "\n",
        "    def equal_opportunity_difference():\n",
        "        tpr_group_1 = predictions[(labels == 1) & (sensitive_attribute == 1)].mean()\n",
        "        tpr_group_0 = predictions[(labels == 1) & (sensitive_attribute == 0)].mean()\n",
        "        return abs(tpr_group_1 - tpr_group_0)\n",
        "\n",
        "    def overall_accuracy_equality_difference():\n",
        "        acc_group_1 = (predictions[sensitive_attribute == 1] == labels[sensitive_attribute == 1]).float().mean()\n",
        "        acc_group_0 = (predictions[sensitive_attribute == 0] == labels[sensitive_attribute == 0]).float().mean()\n",
        "        return abs(acc_group_1 - acc_group_0)\n",
        "\n",
        "    def treatment_equality_difference():\n",
        "        fn_group_1 = ((predictions == 0) & (labels == 1) & (sensitive_attribute == 1)).sum()\n",
        "        fp_group_1 = ((predictions == 1) & (labels == 0) & (sensitive_attribute == 1)).sum()\n",
        "\n",
        "        fn_group_0 = ((predictions == 0) & (labels == 1) & (sensitive_attribute == 0)).sum()\n",
        "        fp_group_0 = ((predictions == 1) & (labels == 0) & (sensitive_attribute == 0)).sum()\n",
        "\n",
        "        ratio_group_1 = fn_group_1 / fp_group_1 if fp_group_1 != 0 else float('inf')\n",
        "        ratio_group_0 = fn_group_0 / fp_group_0 if fp_group_0 != 0 else float('inf')\n",
        "\n",
        "        return abs(ratio_group_1 - ratio_group_0)\n",
        "\n",
        "    # Calculating each fairness metric\n",
        "    spd = statistical_parity_difference()\n",
        "    eod = equal_opportunity_difference()\n",
        "    oaed = overall_accuracy_equality_difference()\n",
        "    ted = treatment_equality_difference()\n",
        "\n",
        "    return {\n",
        "        'Statistical Parity Difference': spd,\n",
        "        'Equal Opportunity Difference': eod,\n",
        "        'Overall Accuracy Equality Difference': oaed,\n",
        "        'Treatment Equality Difference': ted\n",
        "    }"
      ],
      "metadata": {
        "id": "7sRvHIK62wso"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train and Test"
      ],
      "metadata": {
        "id": "n1BLAdefMmlB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "def training(model, data, optimizer, epochs, weighted=False, fairness=False, alpha=0.01, beta=0.01, gamma=0.01, delta=0.01):\n",
        "    \"\"\"\n",
        "    Helper function to train the GNN model.\n",
        "\n",
        "    Args:\n",
        "    model: Initialized GNN model.\n",
        "    data: The torch_geometric data used to train the model.\n",
        "    optimizer: Optimizer used to train the model.\n",
        "    weighted: Boolean value indicating re-weighing done to the data or not.\n",
        "    fairness: Boolean value indicating whether to use fairness-aware loss or not.\n",
        "    alpha: Parameter for statistical parity regularizer strength.\n",
        "    beta: Parameter for treatment equality regularizer strength.\n",
        "    gamma: Parameter for equal opportunity difference regularizer strength.\n",
        "    delta: Parameter for overall accuracy equality difference regularizer strength.\n",
        "\n",
        "    Returns:\n",
        "    -\n",
        "    \"\"\"\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data)\n",
        "\n",
        "        if fairness:\n",
        "            loss = fairness_aware_loss(out[data.train_mask], data, data.x[data.train_mask, -1],\n",
        "                                       weighted=weighted, alpha=alpha, beta=beta, gamma=gamma, delta=delta)\n",
        "        elif weighted:\n",
        "            loss = weighted_cross_entropy(out[data.train_mask], data)\n",
        "        else:\n",
        "            criterion = torch.nn.CrossEntropyLoss()\n",
        "            loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f'Epoch {epoch} | Loss: {loss.item()}')"
      ],
      "metadata": {
        "id": "bOHyBybux08x"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "def test(model, data):\n",
        "    \"\"\"\n",
        "    Helper function to test the trained GNN model.\n",
        "    Prints the Accuracy, as well as various fairness metrics values.\n",
        "    For fairness metrics used: Check the calculate_fairness method\n",
        "\n",
        "    Args:\n",
        "    model: Trained GNN model.\n",
        "    data: The torch_geometric data used to train the model.\n",
        "\n",
        "    Returns:\n",
        "    -\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      out = model(data)\n",
        "\n",
        "    _, pred = model(data).max(dim=1)\n",
        "    correct = int(pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
        "    accuracy = correct / int(data.test_mask.sum())\n",
        "    print(f'Accuracy: {accuracy}')\n",
        "\n",
        "    # Convert model outputs to binary predictions\n",
        "    predictions = out.argmax(dim=1)\n",
        "\n",
        "    # Fairness calculated for gender-classification task with bin_age as the sensitive attribute\n",
        "    fairness_metrics = calculate_fairness(label='gender', predictions=predictions, sens_attr='bin_age')\n",
        "\n",
        "    # Print the fairness metrics\n",
        "    for metric, value in fairness_metrics.items():\n",
        "        print(f\"{metric}: {value}\")\n",
        "\n",
        "    return accuracy, fairness_metrics"
      ],
      "metadata": {
        "id": "C-9-0Bbi2Q5l"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# AIF360 Pre-processing"
      ],
      "metadata": {
        "id": "yi1s71rrDQH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert data to a format suitable for AIF360\n",
        "dataset = BinaryLabelDataset(df=user_labels, label_names=['gender'], protected_attribute_names=['bin_age'])"
      ],
      "metadata": {
        "id": "xp4_oJAQDZGT"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Re-weighing the data w.r.t. a sensitive attribute"
      ],
      "metadata": {
        "id": "RR0zbI6ZBOYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the reweighing\n",
        "RW = Reweighing(unprivileged_groups=[{'bin_age': 0}], privileged_groups=[{'bin_age': 1}])\n",
        "dataset_transf = RW.fit_transform(dataset)\n",
        "\n",
        "# Create a copy of the dataset\n",
        "rw_data = clone(data)\n",
        "\n",
        "# Add weights to the PyTorch Geometric Data object\n",
        "rw_data.instance_weights = torch.tensor(dataset_transf.instance_weights, dtype=torch.float)"
      ],
      "metadata": {
        "id": "wmLTIYS1BTbw"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# GCN Models"
      ],
      "metadata": {
        "id": "LJJv17nM1xsH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GCN class that takes in the data as an input for dimensions of the convolutions\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, data):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GCNConv(data.num_node_features, 16)\n",
        "        self.conv2 = GCNConv(16, 2) # 2 output classes for gender\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "xcv3FZoGxoQn"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Base Model"
      ],
      "metadata": {
        "id": "p8DljQFY98sL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model, define loss function and optimizer\n",
        "gcn_model = GCN(data)\n",
        "gcn_optimizer = torch.optim.Adam(gcn_model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "nzu1iGeBxume"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the first model: GCN, standard data, cross-entropy loss\n",
        "training(model=gcn_model, data=data, optimizer=gcn_optimizer, epochs=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGXTHZpMdeiV",
        "outputId": "0c7eb32a-1958-45e8-e34e-ff68906412cc"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 | Loss: 1.192900538444519\n",
            "Epoch 10 | Loss: 0.5714607834815979\n",
            "Epoch 20 | Loss: 0.5072408318519592\n",
            "Epoch 30 | Loss: 0.46354928612709045\n",
            "Epoch 40 | Loss: 0.429584801197052\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the first model: GCN, standard data, cross-entropy loss\n",
        "print(\"Here are the values for the GCN model with the standard dataset and cross-entropy loss: \")\n",
        "print()\n",
        "test(gcn_model, data)\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "As_kuy6TdoNc",
        "outputId": "9508dea6-41f4-4716-a0a9-73f7f5240902"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are the values for the GCN model with the standard dataset and cross-entropy loss: \n",
            "\n",
            "Accuracy: 0.8521801629132726\n",
            "Statistical Parity Difference: 0.046637266874313354\n",
            "Equal Opportunity Difference: 0.06927275657653809\n",
            "Overall Accuracy Equality Difference: 0.08258169889450073\n",
            "Treatment Equality Difference: 1.336693286895752\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Second model: GCN, standard data, fairness-aware loss (alpha=0.01)"
      ],
      "metadata": {
        "id": "YPV13PBARHQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the second model, define loss function and optimizer\n",
        "gcn_model2 = GCN(data)\n",
        "gcn_optimizer2 = torch.optim.Adam(gcn_model2.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "qtUJdNJ0-Il8"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the second model: GCN, standard data, fairness-aware loss (alpha=0.01)\n",
        "training(model=gcn_model2, data=data, optimizer=gcn_optimizer2, epochs=50, fairness=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rimgm5h2PyAk",
        "outputId": "b823d397-c987-41e7-bd02-b2e2beac33b1"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 | Loss: 0.6220347881317139\n",
            "Epoch 10 | Loss: 0.5531500577926636\n",
            "Epoch 20 | Loss: 0.5012115240097046\n",
            "Epoch 30 | Loss: 0.4466976225376129\n",
            "Epoch 40 | Loss: 0.4097009301185608\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the second model: GCN, standard data, fairness-aware loss (alpha=0.01)\n",
        "print(\"Here are the values for the GCN model with the standard dataset and fairness-entropy loss(alpha=0.01): \")\n",
        "print()\n",
        "test(gcn_model2, data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vrC8bFFiO_3",
        "outputId": "b8a78898-b373-4897-9256-40e20acdc6fe"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are the values for the GCN model with the standard dataset and fairness-entropy loss(alpha=0.01): \n",
            "\n",
            "Accuracy: 0.8378653569717297\n",
            "Statistical Parity Difference: 0.030072450637817383\n",
            "Equal Opportunity Difference: 0.06199532747268677\n",
            "Overall Accuracy Equality Difference: 0.08194565773010254\n",
            "Treatment Equality Difference: 2.69785213470459\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.8378653569717297,\n",
              " {'Statistical Parity Difference': tensor(0.0301),\n",
              "  'Equal Opportunity Difference': tensor(0.0620),\n",
              "  'Overall Accuracy Equality Difference': tensor(0.0819),\n",
              "  'Treatment Equality Difference': tensor(2.6979)})"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ignore"
      ],
      "metadata": {
        "id": "hJ6_kBZqbwbC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test to check if stronger fairness-constraint produces a better model:"
      ],
      "metadata": {
        "id": "kl_IU4mwTTXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gcn_model3 = GCN(data)\n",
        "gcn_optimizer3 = torch.optim.Adam(gcn_model3.parameters(), lr=0.01)\n",
        "\n",
        "training(model=gcn_model3, data=data, optimizer=gcn_optimizer3, epochs=30, fairness=True, alpha=0.05)\n",
        "\n",
        "print(\"Here are the values for the GCN model with the standard dataset and fairness-entropy loss(alpha=0.05): \")\n",
        "print()\n",
        "test(gcn_model3, data)"
      ],
      "metadata": {
        "id": "EojDlWZYRACM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gcn_model4 = GCN(data)\n",
        "gcn_optimizer4 = torch.optim.Adam(gcn_model4.parameters(), lr=0.01)\n",
        "\n",
        "training(model=gcn_model4, data=data, optimizer=gcn_optimizer4, epochs=30, fairness=True, beta=0.005)\n",
        "# print(\"\\nHere are the values for the GCN model with the standard dataset and fairness-entropy loss(alpha=0.005): \")\n",
        "print()\n",
        "test(gcn_model4, data)\n",
        "print()"
      ],
      "metadata": {
        "id": "je4EZYbMXY7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nHere are the values for the GCN model with the standard dataset and fairness-entropy loss(alpha=0.005): \")\n",
        "print()\n",
        "test(gcn_model4, data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5k3Ee_qlZ26o",
        "outputId": "006e3512-6351-4550-aa35-37f690159a6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Here are the values for the GCN model with the standard dataset and fairness-entropy loss(alpha=0.005): \n",
            "\n",
            "Accuracy: 0.847747963584092\n",
            "Statistical Parity Difference: 0.047549135982990265\n",
            "Equal Opportunity Difference: 0.053668081760406494\n",
            "Overall Accuracy Equality Difference: 0.08105164766311646\n",
            "Treatment Equality Difference: 2.612381935119629\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Third model: GCN, re-weighed data, weighted-cross-entropy loss (alpha=0.01)"
      ],
      "metadata": {
        "id": "IviJbqMF_iEH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the third model, define loss function and optimizer\n",
        "rw_data_gcn_model = GCN(rw_data)\n",
        "rw_data_gcn_model_optimizer = torch.optim.Adam(rw_data_gcn_model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "Z2xjarJ4-FCA"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the third model: GCN, re-weighed data, weighted-cross entropy loss\n",
        "training(model=rw_data_gcn_model, data=rw_data, optimizer=rw_data_gcn_model_optimizer, epochs=50, weighted=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlzNiV0aPwQ_",
        "outputId": "17802556-58f1-4612-bae0-c8b3716521f1"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 | Loss: 0.6312071681022644\n",
            "Epoch 10 | Loss: 0.5407678484916687\n",
            "Epoch 20 | Loss: 0.4680361747741699\n",
            "Epoch 30 | Loss: 0.4228784739971161\n",
            "Epoch 40 | Loss: 0.3963108956813812\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the third model: GCN, re-weighed data, weighted-cross entropy loss\n",
        "print(\"Here are the values for the GCN model with the re-weighed dataset and weighted-cross-entropy loss: \")\n",
        "print()\n",
        "test(rw_data_gcn_model, data)\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIuaKSRpluun",
        "outputId": "558e8eb6-cc59-45aa-cd86-bee6f3ebff9a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are the values for the GCN model with the re-weighed dataset and weighted-cross-entropy loss: \n",
            "\n",
            "Accuracy: 0.8576305701964543\n",
            "Statistical Parity Difference: 0.034996867179870605\n",
            "Equal Opportunity Difference: 0.1110086739063263\n",
            "Overall Accuracy Equality Difference: 0.08673566579818726\n",
            "Treatment Equality Difference: 0.6095795631408691\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the third model: GCN, re-weighed data, weighted-cross entropy loss\n",
        "print(\"Here are the values for the GCN model with the re-weighed dataset and weighted-cross-entropy loss: \")\n",
        "print()\n",
        "test(rw_data_gcn_model, data)\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTRqpdjA-93u",
        "outputId": "c8c57ee5-7384-4974-8426-c17cd0337d66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are the values for the GCN model with the re-weighed dataset and weighted-cross-entropy loss: \n",
            "\n",
            "Accuracy: 0.8468495448011499\n",
            "Statistical Parity Difference: 0.030316658318042755\n",
            "Equal Opportunity Difference: 0.10094630718231201\n",
            "Overall Accuracy Equality Difference: 0.08911752700805664\n",
            "Treatment Equality Difference: 0.06582164764404297\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fourth model: GCN, re-weighed data, weighted- and fairness-aware cross entropy loss"
      ],
      "metadata": {
        "id": "3pAVsiTD_jOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the fourth model, define loss function and optimizer\n",
        "rw_data_gcn_model2 = GCN(rw_data)\n",
        "rw_data_gcn_model_optimizer2 = torch.optim.Adam(rw_data_gcn_model2.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "6EvCQZQ3-tGp"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the fourth model: GCN, re-weighed data, weighted- and fairness-aware cross entropy loss\n",
        "training(model=rw_data_gcn_model2, data=rw_data, optimizer=rw_data_gcn_model_optimizer2, epochs=50, weighted=True, fairness=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DA7shFyP4Ke",
        "outputId": "28608ac8-b618-42d6-fa03-c6e2ef8f4fcd"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 | Loss: 0.5843248963356018\n",
            "Epoch 10 | Loss: 0.5047746896743774\n",
            "Epoch 20 | Loss: 0.44967859983444214\n",
            "Epoch 30 | Loss: 0.4109334349632263\n",
            "Epoch 40 | Loss: 0.3880102038383484\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the fourth model: GCN, re-weighed data, weighted- and fairness-aware cross entropy loss\n",
        "print(\"Here are the values for the GCN model with the re-weighed dataset and weighted- and fairness-aware cross-entropy loss: \")\n",
        "print()\n",
        "test(rw_data_gcn_model2, data)\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7-ieFXrlzKH",
        "outputId": "3ba446fb-152f-48ee-b815-2942c8fbbeb2"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are the values for the GCN model with the re-weighed dataset and weighted- and fairness-aware cross-entropy loss: \n",
            "\n",
            "Accuracy: 0.8555342597029229\n",
            "Statistical Parity Difference: 0.034720584750175476\n",
            "Equal Opportunity Difference: 0.1112838089466095\n",
            "Overall Accuracy Equality Difference: 0.08701056241989136\n",
            "Treatment Equality Difference: 0.6815862655639648\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the fourth model: GCN, re-weighed data, weighted- and fairness-aware cross entropy loss\n",
        "training(model=rw_data_gcn_model2, data=rw_data, optimizer=rw_data_gcn_model_optimizer2, epochs=50, weighted=True,\n",
        "         fairness=True, alpha=0.01, beta=0.005, gamma=0.015, delta=0.012)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEz_RIiZvJ2A",
        "outputId": "fabb14bd-ab62-4298-8b74-9e80de05020c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 | Loss: 0.745595395565033\n",
            "Epoch 10 | Loss: 0.5941913723945618\n",
            "Epoch 20 | Loss: 0.5534152984619141\n",
            "Epoch 30 | Loss: 0.5050016641616821\n",
            "Epoch 40 | Loss: 0.4620010554790497\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the fourth model: GCN, re-weighed data, weighted- and fairness-aware cross entropy loss\n",
        "print(\"Here are the values for the GCN model with the re-weighed dataset and weighted- and fairness-aware cross-entropy loss: \")\n",
        "print()\n",
        "test(rw_data_gcn_model2, data)\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2rwS89lvZdT",
        "outputId": "60a1a3c2-fe89-4c69-80af-0465e0640db5"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are the values for the GCN model with the re-weighed dataset and weighted- and fairness-aware cross-entropy loss: \n",
            "\n",
            "Accuracy: 0.8442441303306181\n",
            "Statistical Parity Difference: 0.02938065677881241\n",
            "Equal Opportunity Difference: 0.09115374088287354\n",
            "Overall Accuracy Equality Difference: 0.08772581815719604\n",
            "Treatment Equality Difference: 1.3014488220214844\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the fourth model: GCN, re-weighed data, weighted- and fairness-aware cross entropy loss\n",
        "training(model=rw_data_gcn_model2, data=rw_data, optimizer=rw_data_gcn_model_optimizer2, epochs=50, weighted=True,\n",
        "         fairness=True, alpha=0.01, beta=0.012, gamma=0.015, delta=0.015)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWD3uICM0ACJ",
        "outputId": "f89506cd-5d19-4c56-cf22-37cbceee4acd"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 | Loss: 0.7213656306266785\n",
            "Epoch 10 | Loss: 0.5258520841598511\n",
            "Epoch 20 | Loss: 0.4605921804904938\n",
            "Epoch 30 | Loss: 0.424991250038147\n",
            "Epoch 40 | Loss: 0.40024101734161377\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the fourth model: GCN, re-weighed data, weighted- and fairness-aware cross entropy loss\n",
        "print(\"Here are the values for the GCN model with the re-weighed dataset and weighted- and fairness-aware cross-entropy loss: \")\n",
        "print()\n",
        "test(rw_data_gcn_model2, data)\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAADgkO40AlK",
        "outputId": "157d1064-a99a-4c34-f8b5-d03be9c079da"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are the values for the GCN model with the re-weighed dataset and weighted- and fairness-aware cross-entropy loss: \n",
            "\n",
            "Accuracy: 0.8580797795879253\n",
            "Statistical Parity Difference: 0.042138680815696716\n",
            "Equal Opportunity Difference: 0.0997222363948822\n",
            "Overall Accuracy Equality Difference: 0.0845213532447815\n",
            "Treatment Equality Difference: 0.3458895683288574\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the fourth model: GCN, re-weighed data, weighted- and fairness-aware cross entropy loss\n",
        "training(model=rw_data_gcn_model2, data=rw_data, optimizer=rw_data_gcn_model_optimizer2, epochs=50, weighted=True,\n",
        "         fairness=True, alpha=0.01, beta=0.012, gamma=0.02, delta=0.015)"
      ],
      "metadata": {
        "id": "0_SXFTgY9BT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the fourth model: GCN, re-weighed data, weighted- and fairness-aware cross entropy loss\n",
        "print(\"Here are the values for the GCN model with the re-weighed dataset and weighted- and fairness-aware cross-entropy loss: \")\n",
        "print()\n",
        "test(rw_data_gcn_model2, data)\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9xn8zjj5AmA",
        "outputId": "23600151-bf9b-4275-c2ab-06bf931cd839"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are the values for the GCN model with the re-weighed dataset and weighted- and fairness-aware cross-entropy loss: \n",
            "\n",
            "Accuracy: 0.8528689506468615\n",
            "Statistical Parity Difference: 0.04595881700515747\n",
            "Equal Opportunity Difference: 0.0724097192287445\n",
            "Overall Accuracy Equality Difference: 0.08293139934539795\n",
            "Treatment Equality Difference: 1.1784725189208984\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the fourth model: GCN, re-weighed data, weighted- and fairness-aware cross entropy loss\n",
        "training(model=rw_data_gcn_model2, data=rw_data, optimizer=rw_data_gcn_model_optimizer2, epochs=50, weighted=True,\n",
        "         fairness=True, alpha=0.01, beta=0.012, gamma=0.018, delta=0.018)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_Y5U_AT4t8y",
        "outputId": "49d26726-f7bf-43ba-9ee1-53346a08224f"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 | Loss: 0.6623578667640686\n",
            "Epoch 10 | Loss: 0.5442702770233154\n",
            "Epoch 20 | Loss: 0.47677215933799744\n",
            "Epoch 30 | Loss: 0.42851555347442627\n",
            "Epoch 40 | Loss: 0.39934295415878296\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the fourth model: GCN, re-weighed data, weighted- and fairness-aware cross entropy loss\n",
        "print(\"Here are the values for the GCN model with the re-weighed dataset and weighted- and fairness-aware cross-entropy loss: \")\n",
        "print()\n",
        "test(rw_data_gcn_model2, data)\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXttj2T-899t",
        "outputId": "033b1ad4-1c80-4c1a-d279-f28ed26c5719"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are the values for the GCN model with the re-weighed dataset and weighted- and fairness-aware cross-entropy loss: \n",
            "\n",
            "Accuracy: 0.8569717297556301\n",
            "Statistical Parity Difference: 0.03602193295955658\n",
            "Equal Opportunity Difference: 0.11044105887413025\n",
            "Overall Accuracy Equality Difference: 0.08629560470581055\n",
            "Treatment Equality Difference: 0.6496486663818359\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "# Extras"
      ],
      "metadata": {
        "id": "1zlTkMUwDiDP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### DI Remover"
      ],
      "metadata": {
        "id": "rvho1NeAdEFt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert your data to a format suitable for AIF360\n",
        "dataset = BinaryLabelDataset(df=user_labels, label_names=['gender'], protected_attribute_names=['bin_age'])\n",
        "\n",
        "# Apply the Disparate Impact Remover\n",
        "DIR = DisparateImpactRemover(repair_level=1.0)\n",
        "dataset_transf = DIR.fit_transform(dataset)\n",
        "\n",
        "# Extract the transformed features\n",
        "transformed_features = dataset_transf.features"
      ],
      "metadata": {
        "id": "WeTriESBdJcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rw_data.x = torch.tensor(transformed_features, dtype=torch.float)"
      ],
      "metadata": {
        "id": "p6x3GrdbdRIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DI GCN"
      ],
      "metadata": {
        "id": "pJ-BAyYv-J8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GCN2(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GCN2, self).__init__()\n",
        "        self.conv1 = GCNConv(rw_data.num_node_features, 16)\n",
        "        self.conv2 = GCNConv(16, 2)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "Pq2QOAU7eBLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model, define loss function and optimizer\n",
        "gcn_model = GCN(rw_data)\n",
        "gcn_optimizer = torch.optim.Adam(gcn_model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "8HIbrQ6fdnJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training(model=gcn_model, data=rw_data, optimizer=gcn_optimizer, epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "CGFVV1dCdsU0",
        "outputId": "b0ef8a70-25b8-46a1-d473-3b642745d17f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 | Loss: 2247.96337890625\n",
            "Epoch 10 | Loss: 1941.6514892578125\n",
            "Epoch 20 | Loss: 1676.3948974609375\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-d50c80bfdf74>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgcn_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrw_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgcn_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-9cd473488045>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(model, data, optimizer, epochs, variant, alpha)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test(gcn_model, rw_data)"
      ],
      "metadata": {
        "id": "_BRhQ_ZQeJyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss"
      ],
      "metadata": {
        "id": "PbQBmxjN1pqw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fairness_aware_loss(output, data, sensitive_attr, weighted=False, alpha=0.01):\n",
        "    \"\"\"\n",
        "    A custom loss function to calculate a fairness-aware loss.\n",
        "    The fairness-factor measures the disparity in predictions between +ve and -ve sensitive attribute group.\n",
        "\n",
        "    Args:\n",
        "    output: Outputs from the model.\n",
        "    data: The torch-geometric data object used for the model.\n",
        "    sensitive_attr: The sensitive attribute in the data (in our case: bin_age)\n",
        "    weighted: Boolean value indicating re-weighing done to the data or not.\n",
        "    alpha: Parameter to control the strength of the fairness regularizer.\n",
        "\n",
        "    Returns:\n",
        "    A fairness-aware combined loss.\n",
        "    \"\"\"\n",
        "    if weighted:\n",
        "        # Call the weighted-cross entropy loss\n",
        "        standard_loss = weighted_cross_entropy(output, data)\n",
        "    else:\n",
        "        # Call standard cross-entropy loss\n",
        "        target = data.y[data.train_mask]\n",
        "        standard_loss = F.cross_entropy(output, target)\n",
        "\n",
        "    pos_prob = torch.sigmoid(output[:, 1])\n",
        "\n",
        "    fairness_reg = torch.abs(pos_prob[sensitive_attr == 1].mean() - pos_prob[sensitive_attr == 0].mean())\n",
        "    combined_loss = standard_loss + alpha * fairness_reg\n",
        "\n",
        "    return combined_loss"
      ],
      "metadata": {
        "id": "Y16OLuPEZvKK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}